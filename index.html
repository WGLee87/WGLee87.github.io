<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Geony&#039;s Tech Blog</title><meta description="Data Analyst&amp;#39;s blog"><meta property="og:type" content="blog"><meta property="og:title" content="Geony&#039;s Tech Blog"><meta property="og:url" content="http://wglee87.github.io/"><meta property="og:site_name" content="Geony&#039;s Tech Blog"><meta property="og:description" content="Data Analyst&amp;#39;s blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://wglee87.github.io/img/og_image.png"><meta property="article:author" content="wglee87"><meta property="article:tag" content="data_analysis"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wglee87.github.io"},"headline":"Geony's Tech Blog","image":["http://wglee87.github.io/img/og_image.png"],"author":{"@type":"Person","name":"WGLee87"},"description":"Data Analyst&#39;s blog"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/" alt="Geony&#039;s Tech Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="/null">Download on GitHub</a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-10-13T13:12:04.000Z" title="2020-10-13T13:12:04.000Z">2020-10-13</time><span class="level-item">18 minutes read (About 2720 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/13/Introduce-to-Activation-Function/">Introduce to Activation Function</a></h1><div class="content"><p>Activation Function에 대해 알아봅시다!</p>
<p>딥러닝을 공부하다보면 계속해서 소개되고 사용되는 용어가 있습니다. 바로 활성함수입니다. 그러나 이 활성함수에 대한 설명을 생략하거나 간추려서 가볍게 설명한 후 넘어가는 것이 아쉬워서 이렇게 정리를 하게 되었습니다.</p>
<p>예를 들어</p>
<pre><code>* 활성화 함수 : 왜 sigmoid? 왜 relu?
* 옵티마이저 : RMSProp와 Adam의 차이는 뭐야?
* 손실함수 : 분류문제에서 categorical_crossentropy와 sparse_categorical_crossentropy의 차이는?</code></pre><p>이런 매개변수들을 디테일하게 알고 넘어가는 것이 딥러닝을 공부하는데 있어서 많은 도움이 될 듯 싶습니다.<br>앞으로 딥러닝에 관한 용어, 알고리즘, 이론 등을 하나하나씩 정리를 해보려 합니다. 오늘은 activation function입니다.</p>
<p>시작해보겠습니다.</p>
<h2 id="1-Activation-Function의-역할"><a href="#1-Activation-Function의-역할" class="headerlink" title="1. Activation Function의 역할"></a>1. Activation Function의 역할</h2><pre><code>활성화 함수 라고 번역되는 Activation Function은 인공신경망의 출력을 결정하는 식 입니다.</code></pre><img width="1018" alt="1" src="https://user-images.githubusercontent.com/59719711/95863854-a5598280-0d9f-11eb-8cb7-dfe14332298c.png">

<p>인공신경망에서는 뉴런(노드)에 연산 값을 계속 전달해주는 방식으로 가중치(weights)를 훈련하고, 예측(prediction)을 진행합니다.</p>
<p>각각의 함수는 네트워크의 각 뉴런에 연결되어 있으며, 각 뉴런의 입력이 모델의 예측과 관련되어 있는 지 여부에 따라 활성화 됩니다. 이런 활성화를 통해 인공신경망은 입력값에서 필요한 정보를 학습합니다.</p>
<p>활성화 함수는 훈련 과정에서 계산량이 많고, 역전파(back backpropagation)에서도 사용해야 하므로 연산에 대한 효율성은 중요합니다. 그렇다면 이런 활성화 함수의 종류를 살펴보겠습니다.</p>
<h2 id="2-Activation-3가지-분류"><a href="#2-Activation-3가지-분류" class="headerlink" title="2. Activation 3가지 분류"></a>2. Activation 3가지 분류</h2><h4 id="2-1-Binary-step-function"><a href="#2-1-Binary-step-function" class="headerlink" title="2.1 Binary step function"></a>2.1 Binary step function</h4><img width="829" alt="2" src="https://user-images.githubusercontent.com/59719711/95863960-bf936080-0d9f-11eb-8ed0-7431179eb54c.png">

<p>이미지 출처 : <a href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right">https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right</a></p>
<p>Binary step function 은 임계치 를 기준으로 출력을 해주는 함수입니다. 퍼셉트론(perceptron) 알고리즘에서 활성화 함수로 사용합니다.</p>
<img width="196" alt="3" src="https://user-images.githubusercontent.com/59719711/95864003-cc17b900-0d9f-11eb-8fc6-c98e648c8b74.png">

<p>이 함수의 경우, 다중 분류 문제(Multi-Classification)와 같은 문제에서 다중 출력을 할 수 없다는 단점이 있습니다.</p>
<h4 id="2-2-Linear-activation-function"><a href="#2-2-Linear-activation-function" class="headerlink" title="2.2 Linear activation function"></a>2.2 Linear activation function</h4><img width="791" alt="스크린샷 2020-10-13 오후 10 03 38" src="https://user-images.githubusercontent.com/59719711/95864144-f7020d00-0d9f-11eb-9273-540ceca8b090.png">

<p>이미지 출처 : <a href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right">https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right</a></p>
<p>Linear activation function 는 말그대로 선형 활성화 함수입니다.</p>
<img width="227" alt="5" src="https://user-images.githubusercontent.com/59719711/95864223-0bdea080-0da0-11eb-8a58-09e28e0ef96a.png">

<p>입력 값에 특정 상수 값을 곱한 값을 출력으로 가집니다. 다중 출력이 가능하다는 장점이 있지만, 다음과 같은 문제점을 가집니다.</p>
<ol>
<li>역전파(back-propagation) 알고리즘 사용이 불가능합니다.</li>
</ol>
<p>기본적으로 역전파는 활성화함수를 미분하여 이를 이용해 손실값을 줄이기 위한 과정입니다. 하지만 선형함수의 미분값은 상수이기때문에 입력값과 상관없는 결과를 얻습니다.</p>
<p>그렇기 때문에 예측과 가중치에 대해 관계에 대한 정보를 얻을 수 없습니다.</p>
<ol start="2">
<li>은닉층을 무시하고, 얻을 수 있는 정보를 제한합니다.</li>
</ol>
<p>흔히 딥러닝을 구겨진 공을 피는 과정 이라고 표현을 합니다. 이는 복잡한 입력을 신경망, 활성화 함수를 이용해 정보를 컴퓨터가 이해하기 쉽게 변환하는 딥러닝의 과정을 비유한 의미입니다.</p>
<img width="805" alt="스크린샷 2020-10-13 오후 10 05 07" src="https://user-images.githubusercontent.com/59719711/95864316-2dd82300-0da0-11eb-8149-90e5b89754b4.png">

<p>활성화 함수를 여러 층을 통해 얻고자 하는 것은 필요한 정보를 얻기 위함 입니다. 하지만 선형함수를 여러번 사용하는 것은 마지막에 선형함수를 한번 쓰는 것과 같습니다.</p>
<p>h(x)=cx 일때, h(h(h(x)))=c′x이기 때문입니다.</p>
<h4 id="2-3-Non-linear-activation-function"><a href="#2-3-Non-linear-activation-function" class="headerlink" title="2.3 Non-linear activation function"></a>2.3 Non-linear activation function</h4><p>이제 위의 두 종류의 활성화 함수의 단점때문에 활성화 함수는 비선형 함수를 주로 사용합니다.</p>
<p>최근 신경망 모델에서는 거의 대부분 비선형 함수를 사용합니다. 입력과 출력간의 복잡한 관계를 만들어 입력에서 필요한 정보를 얻습니다. 비정형적인 데이터에 특히 유용합니다. (이미지, 영상, 음성 등의 고차원 데이터)</p>
<p>비선형 함수가 좋은 이유는 선형 함수와 비교해 다음과 같습니다.</p>
<ol>
<li>입력과 관련있는 미분값을 얻으며 역전파를 가능하게 합니다.</li>
<li>심층 신경망을 통해 더 많은 핵심 정보를 얻을 수 있습니다.</li>
</ol>
<h2 id="3-Non-linear-Activation-종류"><a href="#3-Non-linear-Activation-종류" class="headerlink" title="3. Non-linear Activation 종류"></a>3. Non-linear Activation 종류</h2><p>케라스에서 제공하는 activation function을 위주로 만들어 보았습니다.</p>
<h4 id="3-1-Sigmoid"><a href="#3-1-Sigmoid" class="headerlink" title="3.1 Sigmoid"></a>3.1 Sigmoid</h4><p>로지스틱(logistic)으로도 불리는 sigmoid 함수는 s자 형태를 띄는 함수입니다.</p>
<img width="802" alt="7" src="https://user-images.githubusercontent.com/59719711/95864375-46483d80-0da0-11eb-9a63-233b4b3d585d.png">

<p>그래프는 살짝 아쉽지만 볼 수 있듯이 입력값이 커질수록 1로 수렴하고, 입력값이 작을수록 0에 수렴합니다.</p>
<p>함수의 식과 미분 값은 다음과 같습니다.</p>
<pre><code>Pros
유연한 미분값을 가집니다.
출력값의 범위가 (0, 1)로 제한됩니다. 정규화 관점에서 exploding gradient 문제를 방지합니다.
미분 식이 단순한 형태를 가집니다.

Cons
Vanishing Gradient 문제가 발생합니다. 미분 값의 범위는 (0, 1/4) 임을 알 수 있습니다. 입력이 아무리 커도 미분 값의 범위는 제한됩니다. 층이 쌓일수록 gradient 값이 0에 수렴할 것이고, 학습의 효율이 매우 떨어지는 것을 직관적으로 알 수 있습니다. 또한 극값으로 갈수록 값이 포화됩니다. 

출력의 중심이 0이 아닙니다.

이것이 단점인 이유를 직관적으로 알기는 어렵습니다. 또한 exp연산은 비용이 큽니다.
퍼셉트론 등 초기 신경망에 많이 사용했지만 여러 단점 때문에 현재는 많이 사용하지 않는 함수입니다.</code></pre><h4 id="3-2-Tanh"><a href="#3-2-Tanh" class="headerlink" title="3.2 Tanh"></a>3.2 Tanh</h4><p>tanh 또는 hyperbolic tangent 함수는 쌍곡선 함수입니다. 시그모이드 변형을 이용해 사용가능합니다.</p>
<img width="805" alt="8" src="https://user-images.githubusercontent.com/59719711/95864436-56f8b380-0da0-11eb-8490-857b771936ed.png">

<pre><code>Pros
Zero Centered 입니다.
이 부분에 대한 sigmoid의 단점을 해결할 수 있습니다.
다른 장점은 sigmoid와 같습니다.

Cons
center문제를 제외하고 sigmoid와 같습니다.
tanh도 시그모이드와 함께 잘 사용하지 않는 함수입니다.</code></pre><h4 id="3-3-ReLU"><a href="#3-3-ReLU" class="headerlink" title="3.3 ReLU"></a>3.3 ReLU</h4><p>Rectified Linear Unit 함수의 준말로 개선 선형 함수라고 생각할 수 있습니다. 그래프만 봐도 명칭을 이해할 수 있습니다. CNN에서 좋은 성능을 보였고, 현재 딥러닝에서 가장 많이 사용하는 활성화 함수 중 하나입니다.</p>
<p>실제 뇌와 같이 모든 정보에 반응하는 것이 아닌 일부 정보에 대해 무시와 수용을 통해 보다 효율적인 결과를 낸다고 생각할 수 있습니다.</p>
<img width="786" alt="9" src="https://user-images.githubusercontent.com/59719711/95864469-611ab200-0da0-11eb-8319-670ca0001cfb.png">

<pre><code>Pros
연산이 매우 빠릅니다.
함수의 원형을 통해 알 수 있듯, 연산은 비교연산 1회를 통해 함숫값을 구할 수 있습니다. 수렴속도 자체는 위의 두 함수보다 6배 이상 빠릅니다.
비선형 입니다.
모양 자체는 선형같지만, 이 함수는 비선형 함수입니다. 도함수를 가지며, backpropagtion을 허용합니다. 또한 위에서 언급한 바와 같이 정보를 효율적으로 받습니다.

Cons
Dying ReLU
입력값이 0또는 음수일때, gradient값은 0이 됩니다. 이 경우 학습을 하지 못합니다. 데이터의 희소성은 ReLU를 효과적으로 만들어줬고, 이것이 ReLU의 단점이기도 합니다.

이 문제를 해결하기 위해 다양한 유사함수가 만들어집니다. 유사함수는 아래에 소개되어 있습니다.</code></pre><h4 id="3-4-Leaky-ReLU"><a href="#3-4-Leaky-ReLU" class="headerlink" title="3.4 Leaky ReLU"></a>3.4 Leaky ReLU</h4><p>Leaky의 의미는 새는, 구멍이 난 입니다. ReLU에서 Dying ReLU 문제를 해결하기 위해 만든 함수입니다. 음수부에 매우 작은 상수를 곱한 ReLU입니다. 범위가 작아 그래프는 거의 유사하게 그려졌습니다.</p>
<img width="803" alt="10" src="https://user-images.githubusercontent.com/59719711/95864498-6c6ddd80-0da0-11eb-8498-8c7261b17246.png">

<pre><code>Pros
Dying ReLU문제를 방지합니다.
연산이 (여전히) 빠릅니다.
ReLU보다 균형적인 값을 반환하고, 이로 인해 학습이 조금 더 빨라집니다.

Cons
ReLU보다 항상 나은 성능을 내는 것은 아니며, 하나의 대안책으로 추천합니다.</code></pre><h4 id="3-5-ELU"><a href="#3-5-ELU" class="headerlink" title="3.5 ELU"></a>3.5 ELU</h4><p>ELU 는 Exponential Linear Unit을 의미합니다. 음수일 때 exp를 활용하여 표현합니다.</p>
<img width="897" alt="11" src="https://user-images.githubusercontent.com/59719711/95864585-8ad3d900-0da0-11eb-98ca-8f9aa578901d.png">

<pre><code>Pros
ReLU의 모든 장점을 포함합니다.
Dying ReLU 문제를 해결했습니다.

Cons
exp 함수를 사용하여 연산 비용이 추가적으로 발생합니다.
큰 음수값에 대해 쉽게 포화됩니다.</code></pre><h4 id="3-6-softmax"><a href="#3-6-softmax" class="headerlink" title="3.6 softmax"></a>3.6 softmax</h4><p>MNIST 등의 기본적인 다중 분류 문제를 해결하신 분들에게는 익숙한 함수입니다.</p>
<p>softmax함수는 입력받은 값을 0에서 1사이의 값으로 모두 정규화하며, 출력 값이 여러개입니다. 출력 값의 총합은 항상 1이 되는 특징을 가집니다.</p>
<pre><code>Pros
다중 클래스 문제에 적용 가능합니다.
정규화 기능을 가집니다.

Cons
지수함수를 사용하여 오버플로 발생이 가능합니다. (분모분자에 C를 곱해 이를 방지)</code></pre><h4 id="3-7-Maxout"><a href="#3-7-Maxout" class="headerlink" title="3.7 Maxout"></a>3.7 Maxout</h4><p>softmax와 마찬가지로 출력이 여러개로 이루어진 활성화 함수입니다. 효과가 매우 좋은 활성화 함수라고 합니다.</p>
<p>이 함수에 대한 소개 글 중에 매우 좋은 자료가 있어 링크를 올립니다.</p>
<p>[라온피플 : Machine Learning Academy_Part VI. CNN 핵심 요소 기술] 4.Maxout</p>
<pre><code>Pros
ReLU의 장점을 가집니다.
성능이 매우 좋습니다.
Dropout과 함께 사용하기 좋은 활성화 함수입니다.

Cons
계산량이 많고 복잡합니다.</code></pre><p>그 외 Swish, softplus, softsign, Thresholded ReLU, SoftExponential 등 다양한 활성화 함수가 존재하며, 어떤 함수가 제일 좋다고는 할 수 없지만, 자주 사용하는 활성화 함수는 이미 일부 정해져있습니다. 하지만 모든 문제에 최적화된 함수는 없다 는 것이 포인트입니다.</p>
<p>어떤 문제에 있어서는 새로운 활성화 함수가 유용한 케이스가 존재할 것이고, 간단한 아이디어만으로 성능을 향상 시킬 수 있다고 생각합니다. 그런만큼 딥러닝에서는 직관을 키우는 것이 매우 중요하다고 생각합니다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-26T10:15:22.000Z" title="2020-08-26T10:15:22.000Z">2020-08-26</time><span class="level-item">26 minutes read (About 3919 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/26/Hyper-Parameter-Tuner/">Hyper-Parameter(Tuner)</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> IPython</span><br></pre></td></tr></table></figure>

<p>Keras Tuner를 설치하고 가져옵니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install -q -U keras-tuner</span><br><span class="line"><span class="keyword">import</span> kerastuner <span class="keyword">as</span> kt</span><br></pre></td></tr></table></figure>

<p>데이터 세트 다운로드 및 준비<br>    - 이 자습서에서는 Keras Tuner를 사용하여 Fashion MNIST 데이터 세트 에서 의류 이미지를 분류하는 기계 학습 모델에 가장 적합한 하이퍼 파라미터를 찾습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 로드</span></span><br><span class="line">(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize pixel values between 0 and 1</span></span><br><span class="line">img_train = img_train.astype(<span class="string">'float32'</span>) / <span class="number">255.0</span></span><br><span class="line">img_test = img_test.astype(<span class="string">'float32'</span>) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_train.shape, label_train.shape</span><br></pre></td></tr></table></figure>




<pre><code>((60000, 28, 28), (60000,))</code></pre><p>모델 정의</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_builder</span><span class="params">(hp)</span>:</span></span><br><span class="line">    model = keras.Sequential()</span><br><span class="line">    model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Tune the number of units in the first Dense layer</span></span><br><span class="line">    <span class="comment"># Choose an optimal value between 32-512</span></span><br><span class="line">    hp_units = hp.Int(<span class="string">'units'</span>, min_value = <span class="number">32</span>, max_value = <span class="number">512</span>, step = <span class="number">32</span>)</span><br><span class="line">    model.add(keras.layers.Dense(units = hp_units, activation = <span class="string">'relu'</span>))</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Tune the learning rate for the optimizer </span></span><br><span class="line">    <span class="comment"># Choose an optimal value from 0.01, 0.001, or 0.0001</span></span><br><span class="line">    hp_learning_rate = hp.Choice(<span class="string">'learning_rate'</span>, values = [<span class="number">1e-2</span>, <span class="number">1e-3</span>, <span class="number">1e-4</span>]) </span><br><span class="line">    </span><br><span class="line">    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),</span><br><span class="line">                  loss = keras.losses.SparseCategoricalCrossentropy(from_logits = <span class="literal">True</span>), </span><br><span class="line">                  metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<pre><code>튜너를 인스턴스화하고 하이퍼 튜닝을 수행합니다

튜너를 인스턴스화하여 하이퍼 튜닝을 수행합니다. Hyperband Tuner에는 RandomSearch , Hyperband , BayesianOptimization 및 Sklearn 네 가지 튜너가 있습니다. 이 자습서에서는 하이퍼 밴드 튜너를 사용합니다.

하이퍼 밴드 튜너를 인스턴스화하려면 하이퍼 모델, 최적화 할 objective 및 훈련 할 최대 max_epochs ( max_epochs )를 지정해야합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tuner = kt.Hyperband(model_builder,</span><br><span class="line">                     objective = <span class="string">'val_accuracy'</span>, </span><br><span class="line">                     max_epochs = <span class="number">10</span>,</span><br><span class="line">                     factor = <span class="number">3</span>,</span><br><span class="line">                     directory = <span class="string">'my_dir'</span>,</span><br><span class="line">                     project_name = <span class="string">'intro_to_kt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>INFO:tensorflow:Reloading Oracle from existing project my_dir/intro_to_kt/oracle.json
INFO:tensorflow:Reloading Tuner from my_dir/intro_to_kt/tuner0.json</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClearTrainingOutput</span><span class="params">(tf.keras.callbacks.Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_end</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        IPython.display.clear_output(wait = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tuner.search(img_train, label_train, epochs = <span class="number">10</span>, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the optimal hyperparameters</span></span><br><span class="line">best_hps = tuner.get_best_hyperparameters(num_trials = <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"""</span></span><br><span class="line"><span class="string">The hyperparameter search is complete. The optimal number of units in the first densely-connected</span></span><br><span class="line"><span class="string">layer is <span class="subst">&#123;best_hps.get(<span class="string">'units'</span>)&#125;</span> and the optimal learning rate for the optimizer</span></span><br><span class="line"><span class="string">is <span class="subst">&#123;best_hps.get(<span class="string">'learning_rate'</span>)&#125;</span>.</span></span><br><span class="line"><span class="string">"""</span>)</span><br></pre></td></tr></table></figure>


<p><span style="color:#4527A0"><h1 style="font-size:18px">Trial complete</h1></span></p>
<p><span style="color:#4527A0"><h1 style="font-size:18px">Trial summary</h1></span></p>
<p><span style="color:cyan"> |-Trial ID: a07676c4549fc425444c7c101819cb0a</span></p>
<p><span style="color:cyan"> |-Score: 0.8574000000953674</span></p>
<p><span style="color:cyan"> |-Best step: 0</span></p>
<p><span style="color:#7E57C2"><h2 style="font-size:16px">Hyperparameters:</h2></span></p>
<p><span style="color:cyan"> |-learning_rate: 0.0001</span></p>
<p><span style="color:blue"> |-tuner/bracket: 0</span></p>
<p><span style="color:cyan"> |-tuner/epochs: 10</span></p>
<p><span style="color:blue"> |-tuner/initial_epoch: 0</span></p>
<p><span style="color:cyan"> |-tuner/round: 0</span></p>
<p><span style="color:blue"> |-units: 64</span></p>
<pre><code>INFO:tensorflow:Oracle triggered exit

The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is 384 and the optimal learning rate for the optimizer
is 0.001.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the model with the optimal hyperparameters and train it on the data</span></span><br><span class="line">model = tuner.hypermodel.build(best_hps)</span><br><span class="line">model.fit(img_train, label_train, epochs = <span class="number">10</span>, validation_data = (img_test, label_test))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
1875/1875 [==============================] - ETA: 8:42 - loss: 2.1883 - accuracy: 0.18 - ETA: 20s - loss: 1.5613 - accuracy: 0.4721 - ETA: 11s - loss: 1.3144 - accuracy: 0.550 - ETA: 9s - loss: 1.1853 - accuracy: 0.591 - ETA: 7s - loss: 1.1046 - accuracy: 0.61 - ETA: 6s - loss: 1.0539 - accuracy: 0.63 - ETA: 6s - loss: 1.0195 - accuracy: 0.64 - ETA: 6s - loss: 0.9893 - accuracy: 0.65 - ETA: 5s - loss: 0.9689 - accuracy: 0.66 - ETA: 5s - loss: 0.9544 - accuracy: 0.67 - ETA: 6s - loss: 0.9429 - accuracy: 0.67 - ETA: 5s - loss: 0.9283 - accuracy: 0.68 - ETA: 5s - loss: 0.9130 - accuracy: 0.68 - ETA: 5s - loss: 0.9015 - accuracy: 0.68 - ETA: 5s - loss: 0.8923 - accuracy: 0.69 - ETA: 5s - loss: 0.8822 - accuracy: 0.69 - ETA: 5s - loss: 0.8716 - accuracy: 0.69 - ETA: 5s - loss: 0.8618 - accuracy: 0.70 - ETA: 5s - loss: 0.8483 - accuracy: 0.70 - ETA: 5s - loss: 0.8360 - accuracy: 0.71 - ETA: 4s - loss: 0.8242 - accuracy: 0.71 - ETA: 4s - loss: 0.8147 - accuracy: 0.71 - ETA: 4s - loss: 0.8061 - accuracy: 0.72 - ETA: 4s - loss: 0.8001 - accuracy: 0.72 - ETA: 4s - loss: 0.7961 - accuracy: 0.72 - ETA: 4s - loss: 0.7900 - accuracy: 0.72 - ETA: 4s - loss: 0.7842 - accuracy: 0.72 - ETA: 4s - loss: 0.7790 - accuracy: 0.73 - ETA: 4s - loss: 0.7746 - accuracy: 0.73 - ETA: 4s - loss: 0.7703 - accuracy: 0.73 - ETA: 4s - loss: 0.7662 - accuracy: 0.73 - ETA: 4s - loss: 0.7617 - accuracy: 0.73 - ETA: 4s - loss: 0.7573 - accuracy: 0.73 - ETA: 4s - loss: 0.7536 - accuracy: 0.73 - ETA: 4s - loss: 0.7493 - accuracy: 0.73 - ETA: 4s - loss: 0.7453 - accuracy: 0.74 - ETA: 3s - loss: 0.7407 - accuracy: 0.74 - ETA: 3s - loss: 0.7365 - accuracy: 0.74 - ETA: 3s - loss: 0.7324 - accuracy: 0.74 - ETA: 3s - loss: 0.7288 - accuracy: 0.74 - ETA: 3s - loss: 0.7252 - accuracy: 0.74 - ETA: 3s - loss: 0.7212 - accuracy: 0.74 - ETA: 3s - loss: 0.7171 - accuracy: 0.75 - ETA: 3s - loss: 0.7132 - accuracy: 0.75 - ETA: 3s - loss: 0.7094 - accuracy: 0.75 - ETA: 3s - loss: 0.7059 - accuracy: 0.75 - ETA: 3s - loss: 0.7021 - accuracy: 0.75 - ETA: 3s - loss: 0.6986 - accuracy: 0.75 - ETA: 3s - loss: 0.6962 - accuracy: 0.75 - ETA: 3s - loss: 0.6929 - accuracy: 0.75 - ETA: 2s - loss: 0.6897 - accuracy: 0.75 - ETA: 2s - loss: 0.6866 - accuracy: 0.76 - ETA: 2s - loss: 0.6839 - accuracy: 0.76 - ETA: 2s - loss: 0.6810 - accuracy: 0.76 - ETA: 2s - loss: 0.6781 - accuracy: 0.76 - ETA: 2s - loss: 0.6753 - accuracy: 0.76 - ETA: 2s - loss: 0.6728 - accuracy: 0.76 - ETA: 2s - loss: 0.6704 - accuracy: 0.76 - ETA: 2s - loss: 0.6682 - accuracy: 0.76 - ETA: 2s - loss: 0.6657 - accuracy: 0.76 - ETA: 2s - loss: 0.6633 - accuracy: 0.76 - ETA: 2s - loss: 0.6607 - accuracy: 0.76 - ETA: 2s - loss: 0.6584 - accuracy: 0.76 - ETA: 2s - loss: 0.6561 - accuracy: 0.77 - ETA: 2s - loss: 0.6541 - accuracy: 0.77 - ETA: 2s - loss: 0.6521 - accuracy: 0.77 - ETA: 1s - loss: 0.6500 - accuracy: 0.77 - ETA: 1s - loss: 0.6479 - accuracy: 0.77 - ETA: 1s - loss: 0.6457 - accuracy: 0.77 - ETA: 1s - loss: 0.6433 - accuracy: 0.77 - ETA: 1s - loss: 0.6410 - accuracy: 0.77 - ETA: 1s - loss: 0.6388 - accuracy: 0.77 - ETA: 1s - loss: 0.6366 - accuracy: 0.77 - ETA: 1s - loss: 0.6347 - accuracy: 0.77 - ETA: 1s - loss: 0.6324 - accuracy: 0.77 - ETA: 1s - loss: 0.6302 - accuracy: 0.77 - ETA: 1s - loss: 0.6281 - accuracy: 0.77 - ETA: 1s - loss: 0.6260 - accuracy: 0.78 - ETA: 1s - loss: 0.6240 - accuracy: 0.78 - ETA: 1s - loss: 0.6221 - accuracy: 0.78 - ETA: 1s - loss: 0.6202 - accuracy: 0.78 - ETA: 0s - loss: 0.6182 - accuracy: 0.78 - ETA: 0s - loss: 0.6163 - accuracy: 0.78 - ETA: 0s - loss: 0.6144 - accuracy: 0.78 - ETA: 0s - loss: 0.6126 - accuracy: 0.78 - ETA: 0s - loss: 0.6108 - accuracy: 0.78 - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - ETA: 0s - loss: 0.6077 - accuracy: 0.78 - ETA: 0s - loss: 0.6061 - accuracy: 0.78 - ETA: 0s - loss: 0.6043 - accuracy: 0.78 - ETA: 0s - loss: 0.6026 - accuracy: 0.78 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.5995 - accuracy: 0.78 - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.5965 - accuracy: 0.79 - 6s 3ms/step - loss: 0.5951 - accuracy: 0.7907 - val_loss: 0.4127 - val_accuracy: 0.8468
Epoch 2/10
1875/1875 [==============================] - ETA: 8s - loss: 0.4197 - accuracy: 0.84 - ETA: 3s - loss: 0.4062 - accuracy: 0.85 - ETA: 3s - loss: 0.3977 - accuracy: 0.85 - ETA: 3s - loss: 0.3848 - accuracy: 0.86 - ETA: 3s - loss: 0.3777 - accuracy: 0.86 - ETA: 3s - loss: 0.3730 - accuracy: 0.86 - ETA: 3s - loss: 0.3704 - accuracy: 0.86 - ETA: 3s - loss: 0.3683 - accuracy: 0.86 - ETA: 3s - loss: 0.3674 - accuracy: 0.86 - ETA: 3s - loss: 0.3670 - accuracy: 0.86 - ETA: 3s - loss: 0.3664 - accuracy: 0.86 - ETA: 3s - loss: 0.3659 - accuracy: 0.86 - ETA: 3s - loss: 0.3653 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3651 - accuracy: 0.86 - ETA: 3s - loss: 0.3652 - accuracy: 0.86 - ETA: 3s - loss: 0.3654 - accuracy: 0.86 - ETA: 3s - loss: 0.3655 - accuracy: 0.86 - ETA: 2s - loss: 0.3657 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3651 - accuracy: 0.86 - 4s 2ms/step - loss: 0.3651 - accuracy: 0.8674 - val_loss: 0.3735 - val_accuracy: 0.8663
Epoch 3/10
1875/1875 [==============================] - ETA: 7s - loss: 0.2668 - accuracy: 0.87 - ETA: 4s - loss: 0.2867 - accuracy: 0.88 - ETA: 4s - loss: 0.2901 - accuracy: 0.89 - ETA: 4s - loss: 0.2983 - accuracy: 0.89 - ETA: 3s - loss: 0.3031 - accuracy: 0.89 - ETA: 3s - loss: 0.3054 - accuracy: 0.89 - ETA: 3s - loss: 0.3067 - accuracy: 0.89 - ETA: 3s - loss: 0.3087 - accuracy: 0.88 - ETA: 3s - loss: 0.3110 - accuracy: 0.88 - ETA: 3s - loss: 0.3125 - accuracy: 0.88 - ETA: 3s - loss: 0.3140 - accuracy: 0.88 - ETA: 3s - loss: 0.3150 - accuracy: 0.88 - ETA: 3s - loss: 0.3159 - accuracy: 0.88 - ETA: 3s - loss: 0.3167 - accuracy: 0.88 - ETA: 3s - loss: 0.3174 - accuracy: 0.88 - ETA: 3s - loss: 0.3180 - accuracy: 0.88 - ETA: 3s - loss: 0.3185 - accuracy: 0.88 - ETA: 3s - loss: 0.3189 - accuracy: 0.88 - ETA: 3s - loss: 0.3194 - accuracy: 0.88 - ETA: 3s - loss: 0.3199 - accuracy: 0.88 - ETA: 3s - loss: 0.3202 - accuracy: 0.88 - ETA: 3s - loss: 0.3205 - accuracy: 0.88 - ETA: 3s - loss: 0.3208 - accuracy: 0.88 - ETA: 3s - loss: 0.3212 - accuracy: 0.88 - ETA: 3s - loss: 0.3214 - accuracy: 0.88 - ETA: 3s - loss: 0.3215 - accuracy: 0.88 - ETA: 2s - loss: 0.3217 - accuracy: 0.88 - ETA: 2s - loss: 0.3219 - accuracy: 0.88 - ETA: 2s - loss: 0.3220 - accuracy: 0.88 - ETA: 2s - loss: 0.3221 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3224 - accuracy: 0.88 - ETA: 2s - loss: 0.3225 - accuracy: 0.88 - ETA: 2s - loss: 0.3226 - accuracy: 0.88 - ETA: 2s - loss: 0.3227 - accuracy: 0.88 - ETA: 2s - loss: 0.3229 - accuracy: 0.88 - ETA: 2s - loss: 0.3230 - accuracy: 0.88 - ETA: 2s - loss: 0.3232 - accuracy: 0.88 - ETA: 2s - loss: 0.3233 - accuracy: 0.88 - ETA: 1s - loss: 0.3234 - accuracy: 0.88 - ETA: 1s - loss: 0.3235 - accuracy: 0.88 - ETA: 1s - loss: 0.3236 - accuracy: 0.88 - ETA: 1s - loss: 0.3237 - accuracy: 0.88 - ETA: 1s - loss: 0.3239 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3241 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - 4s 2ms/step - loss: 0.3238 - accuracy: 0.8810 - val_loss: 0.3695 - val_accuracy: 0.8646
Epoch 4/10
1875/1875 [==============================] - ETA: 7s - loss: 0.2077 - accuracy: 0.87 - ETA: 4s - loss: 0.3105 - accuracy: 0.85 - ETA: 4s - loss: 0.3010 - accuracy: 0.86 - ETA: 3s - loss: 0.2976 - accuracy: 0.87 - ETA: 3s - loss: 0.2957 - accuracy: 0.87 - ETA: 3s - loss: 0.2952 - accuracy: 0.88 - ETA: 3s - loss: 0.2951 - accuracy: 0.88 - ETA: 3s - loss: 0.2948 - accuracy: 0.88 - ETA: 3s - loss: 0.2947 - accuracy: 0.88 - ETA: 3s - loss: 0.2943 - accuracy: 0.88 - ETA: 3s - loss: 0.2940 - accuracy: 0.88 - ETA: 3s - loss: 0.2937 - accuracy: 0.88 - ETA: 3s - loss: 0.2934 - accuracy: 0.88 - ETA: 3s - loss: 0.2931 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 3s - loss: 0.2925 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2925 - accuracy: 0.88 - ETA: 2s - loss: 0.2926 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2929 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2931 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2937 - accuracy: 0.88 - ETA: 1s - loss: 0.2938 - accuracy: 0.88 - ETA: 1s - loss: 0.2939 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2941 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2943 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2949 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2951 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2956 - accuracy: 0.88 - 4s 2ms/step - loss: 0.2956 - accuracy: 0.8898 - val_loss: 0.3532 - val_accuracy: 0.8745
Epoch 5/10
1875/1875 [==============================] - ETA: 7s - loss: 0.3435 - accuracy: 0.87 - ETA: 4s - loss: 0.3009 - accuracy: 0.89 - ETA: 3s - loss: 0.2893 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2894 - accuracy: 0.89 - ETA: 3s - loss: 0.2882 - accuracy: 0.89 - ETA: 3s - loss: 0.2872 - accuracy: 0.89 - ETA: 3s - loss: 0.2868 - accuracy: 0.89 - ETA: 3s - loss: 0.2870 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2888 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2887 - accuracy: 0.89 - ETA: 3s - loss: 0.2883 - accuracy: 0.89 - ETA: 3s - loss: 0.2881 - accuracy: 0.89 - ETA: 3s - loss: 0.2880 - accuracy: 0.89 - ETA: 3s - loss: 0.2879 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 2s - loss: 0.2875 - accuracy: 0.89 - ETA: 2s - loss: 0.2872 - accuracy: 0.89 - ETA: 2s - loss: 0.2870 - accuracy: 0.89 - ETA: 2s - loss: 0.2867 - accuracy: 0.89 - ETA: 2s - loss: 0.2865 - accuracy: 0.89 - ETA: 2s - loss: 0.2863 - accuracy: 0.89 - ETA: 2s - loss: 0.2861 - accuracy: 0.89 - ETA: 2s - loss: 0.2859 - accuracy: 0.89 - ETA: 2s - loss: 0.2858 - accuracy: 0.89 - ETA: 2s - loss: 0.2856 - accuracy: 0.89 - ETA: 2s - loss: 0.2854 - accuracy: 0.89 - ETA: 2s - loss: 0.2852 - accuracy: 0.89 - ETA: 2s - loss: 0.2851 - accuracy: 0.89 - ETA: 2s - loss: 0.2850 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2839 - accuracy: 0.89 - 5s 2ms/step - loss: 0.2839 - accuracy: 0.8954 - val_loss: 0.3473 - val_accuracy: 0.8731
Epoch 6/10
1875/1875 [==============================] - ETA: 8s - loss: 0.1747 - accuracy: 0.93 - ETA: 4s - loss: 0.2120 - accuracy: 0.92 - ETA: 4s - loss: 0.2342 - accuracy: 0.91 - ETA: 4s - loss: 0.2445 - accuracy: 0.91 - ETA: 4s - loss: 0.2483 - accuracy: 0.91 - ETA: 4s - loss: 0.2509 - accuracy: 0.91 - ETA: 3s - loss: 0.2532 - accuracy: 0.90 - ETA: 3s - loss: 0.2548 - accuracy: 0.90 - ETA: 3s - loss: 0.2559 - accuracy: 0.90 - ETA: 3s - loss: 0.2568 - accuracy: 0.90 - ETA: 3s - loss: 0.2574 - accuracy: 0.90 - ETA: 3s - loss: 0.2578 - accuracy: 0.90 - ETA: 3s - loss: 0.2582 - accuracy: 0.90 - ETA: 3s - loss: 0.2586 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 2s - loss: 0.2589 - accuracy: 0.90 - ETA: 2s - loss: 0.2590 - accuracy: 0.90 - ETA: 2s - loss: 0.2591 - accuracy: 0.90 - ETA: 2s - loss: 0.2592 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2594 - accuracy: 0.90 - ETA: 2s - loss: 0.2595 - accuracy: 0.90 - ETA: 2s - loss: 0.2596 - accuracy: 0.90 - ETA: 2s - loss: 0.2597 - accuracy: 0.90 - ETA: 2s - loss: 0.2599 - accuracy: 0.90 - ETA: 2s - loss: 0.2601 - accuracy: 0.90 - ETA: 2s - loss: 0.2602 - accuracy: 0.90 - ETA: 2s - loss: 0.2604 - accuracy: 0.90 - ETA: 2s - loss: 0.2605 - accuracy: 0.90 - ETA: 2s - loss: 0.2606 - accuracy: 0.90 - ETA: 2s - loss: 0.2608 - accuracy: 0.90 - ETA: 2s - loss: 0.2609 - accuracy: 0.90 - ETA: 1s - loss: 0.2610 - accuracy: 0.90 - ETA: 1s - loss: 0.2611 - accuracy: 0.90 - ETA: 1s - loss: 0.2612 - accuracy: 0.90 - ETA: 1s - loss: 0.2613 - accuracy: 0.90 - ETA: 1s - loss: 0.2614 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2616 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2618 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2621 - accuracy: 0.90 - ETA: 1s - loss: 0.2622 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 0s - loss: 0.2624 - accuracy: 0.90 - ETA: 0s - loss: 0.2625 - accuracy: 0.90 - ETA: 0s - loss: 0.2626 - accuracy: 0.90 - ETA: 0s - loss: 0.2627 - accuracy: 0.90 - ETA: 0s - loss: 0.2628 - accuracy: 0.90 - ETA: 0s - loss: 0.2629 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2631 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2635 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2635 - accuracy: 0.9027 - val_loss: 0.3392 - val_accuracy: 0.8760
Epoch 7/10
1875/1875 [==============================] - ETA: 8s - loss: 0.2374 - accuracy: 0.90 - ETA: 3s - loss: 0.2432 - accuracy: 0.91 - ETA: 3s - loss: 0.2501 - accuracy: 0.91 - ETA: 3s - loss: 0.2520 - accuracy: 0.91 - ETA: 3s - loss: 0.2517 - accuracy: 0.91 - ETA: 3s - loss: 0.2506 - accuracy: 0.91 - ETA: 3s - loss: 0.2494 - accuracy: 0.91 - ETA: 3s - loss: 0.2490 - accuracy: 0.91 - ETA: 3s - loss: 0.2492 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2491 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2488 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2485 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2490 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2501 - accuracy: 0.9087 - val_loss: 0.3413 - val_accuracy: 0.8810
Epoch 8/10
1875/1875 [==============================] - ETA: 11s - loss: 0.1224 - accuracy: 0.937 - ETA: 5s - loss: 0.2162 - accuracy: 0.927 - ETA: 5s - loss: 0.2236 - accuracy: 0.92 - ETA: 5s - loss: 0.2277 - accuracy: 0.91 - ETA: 4s - loss: 0.2284 - accuracy: 0.91 - ETA: 4s - loss: 0.2283 - accuracy: 0.91 - ETA: 3s - loss: 0.2286 - accuracy: 0.91 - ETA: 3s - loss: 0.2295 - accuracy: 0.91 - ETA: 3s - loss: 0.2307 - accuracy: 0.91 - ETA: 3s - loss: 0.2317 - accuracy: 0.91 - ETA: 3s - loss: 0.2323 - accuracy: 0.91 - ETA: 3s - loss: 0.2324 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 2s - loss: 0.2327 - accuracy: 0.91 - ETA: 2s - loss: 0.2329 - accuracy: 0.91 - ETA: 2s - loss: 0.2332 - accuracy: 0.91 - ETA: 2s - loss: 0.2335 - accuracy: 0.91 - ETA: 2s - loss: 0.2338 - accuracy: 0.91 - ETA: 2s - loss: 0.2340 - accuracy: 0.91 - ETA: 2s - loss: 0.2342 - accuracy: 0.91 - ETA: 2s - loss: 0.2343 - accuracy: 0.91 - ETA: 2s - loss: 0.2345 - accuracy: 0.91 - ETA: 2s - loss: 0.2347 - accuracy: 0.91 - ETA: 2s - loss: 0.2349 - accuracy: 0.91 - ETA: 2s - loss: 0.2351 - accuracy: 0.91 - ETA: 2s - loss: 0.2353 - accuracy: 0.91 - ETA: 2s - loss: 0.2355 - accuracy: 0.91 - ETA: 2s - loss: 0.2356 - accuracy: 0.91 - ETA: 2s - loss: 0.2357 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2365 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2367 - accuracy: 0.91 - ETA: 1s - loss: 0.2369 - accuracy: 0.91 - ETA: 1s - loss: 0.2370 - accuracy: 0.91 - ETA: 1s - loss: 0.2371 - accuracy: 0.91 - ETA: 1s - loss: 0.2372 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2374 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2376 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2379 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2381 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - 5s 2ms/step - loss: 0.2386 - accuracy: 0.9109 - val_loss: 0.3364 - val_accuracy: 0.8836
Epoch 9/10
1875/1875 [==============================] - ETA: 11s - loss: 0.3092 - accuracy: 0.843 - ETA: 4s - loss: 0.2166 - accuracy: 0.911 - ETA: 4s - loss: 0.2219 - accuracy: 0.91 - ETA: 4s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2254 - accuracy: 0.91 - ETA: 3s - loss: 0.2260 - accuracy: 0.91 - ETA: 3s - loss: 0.2264 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2265 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 2s - loss: 0.2271 - accuracy: 0.91 - ETA: 2s - loss: 0.2273 - accuracy: 0.91 - ETA: 2s - loss: 0.2275 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2277 - accuracy: 0.91 - ETA: 2s - loss: 0.2278 - accuracy: 0.91 - ETA: 2s - loss: 0.2279 - accuracy: 0.91 - ETA: 2s - loss: 0.2280 - accuracy: 0.91 - ETA: 2s - loss: 0.2282 - accuracy: 0.91 - ETA: 2s - loss: 0.2283 - accuracy: 0.91 - ETA: 2s - loss: 0.2284 - accuracy: 0.91 - ETA: 2s - loss: 0.2286 - accuracy: 0.91 - ETA: 2s - loss: 0.2287 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2289 - accuracy: 0.91 - ETA: 1s - loss: 0.2290 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2299 - accuracy: 0.9138 - val_loss: 0.3266 - val_accuracy: 0.8846
Epoch 10/10
1875/1875 [==============================] - ETA: 6s - loss: 0.1648 - accuracy: 0.93 - ETA: 3s - loss: 0.1805 - accuracy: 0.93 - ETA: 3s - loss: 0.1840 - accuracy: 0.93 - ETA: 3s - loss: 0.1904 - accuracy: 0.92 - ETA: 3s - loss: 0.1970 - accuracy: 0.92 - ETA: 3s - loss: 0.2009 - accuracy: 0.92 - ETA: 3s - loss: 0.2040 - accuracy: 0.92 - ETA: 3s - loss: 0.2060 - accuracy: 0.92 - ETA: 3s - loss: 0.2075 - accuracy: 0.92 - ETA: 3s - loss: 0.2087 - accuracy: 0.92 - ETA: 3s - loss: 0.2099 - accuracy: 0.92 - ETA: 3s - loss: 0.2112 - accuracy: 0.92 - ETA: 3s - loss: 0.2121 - accuracy: 0.92 - ETA: 3s - loss: 0.2127 - accuracy: 0.92 - ETA: 3s - loss: 0.2132 - accuracy: 0.92 - ETA: 3s - loss: 0.2135 - accuracy: 0.92 - ETA: 3s - loss: 0.2138 - accuracy: 0.92 - ETA: 3s - loss: 0.2141 - accuracy: 0.92 - ETA: 3s - loss: 0.2144 - accuracy: 0.92 - ETA: 3s - loss: 0.2146 - accuracy: 0.91 - ETA: 3s - loss: 0.2148 - accuracy: 0.91 - ETA: 3s - loss: 0.2151 - accuracy: 0.91 - ETA: 3s - loss: 0.2154 - accuracy: 0.91 - ETA: 3s - loss: 0.2157 - accuracy: 0.91 - ETA: 3s - loss: 0.2159 - accuracy: 0.91 - ETA: 2s - loss: 0.2161 - accuracy: 0.91 - ETA: 2s - loss: 0.2163 - accuracy: 0.91 - ETA: 2s - loss: 0.2165 - accuracy: 0.91 - ETA: 2s - loss: 0.2167 - accuracy: 0.91 - ETA: 2s - loss: 0.2168 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2171 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2177 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2181 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2183 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2185 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2187 - accuracy: 0.91 - ETA: 1s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2190 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2193 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2195 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2198 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2198 - accuracy: 0.9179 - val_loss: 0.3605 - val_accuracy: 0.8808





&lt;tensorflow.python.keras.callbacks.History at 0x7fdcd087f710&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-26T10:14:02.000Z" title="2020-08-26T10:14:02.000Z">2020-08-26</time><span class="level-item">17 minutes read (About 2590 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/26/Model-sava-and-load-in-tensorflow/">Model sava and load in tensorflow</a></h1><div class="content"><h6 id="설정"><a href="#설정" class="headerlink" title="설정"></a>설정</h6><pre><code>필요한 라이브러리를 설치하고 텐서플로를 임포트(import)합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -q pyyaml h5py  <span class="comment"># HDF5 포맷으로 모델을 저장하기 위해서 필요합니다</span></span><br></pre></td></tr></table></figure>

<pre><code>Note: you may need to restart the kernel to use updated packages.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>2.4.0-dev20200724


예제 데이터셋 받기</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_labels = train_labels[:<span class="number">1000</span>]</span><br><span class="line">test_labels = test_labels[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">train_images = train_images[:<span class="number">1000</span>].reshape(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>) / <span class="number">255.0</span></span><br><span class="line">test_images = test_images[:<span class="number">1000</span>].reshape(<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>

<pre><code>모델링 작업</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential 모델 정의</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = tf.keras.models.Sequential([</span><br><span class="line">        keras.layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">784</span>,)),</span><br><span class="line">        keras.layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 객체 생성</span></span><br><span class="line">model = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 출력</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_8&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 512)               401920    
_________________________________________________________________
dropout_8 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_17 (Dense)             (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________


훈련하는 동안 체크포인트 저장하기
훈련 중간과 훈련 마지막에 체크포인트(checkpoint)를 자동으로 저장하도록 하는 것이 많이 사용하는 방법입니다. 다시 훈련하지 않고 모델을 재사용하거나 훈련 과정이 중지된 경우 이어서 훈련을 진행할 수 있습니다. tf.keras.callbacks.ModelCheckpoint은 이런 작업을 수행하는 콜백(callback)입니다. 이 콜백은 체크포인트 작업을 조정할 수 있도록 여러가지 매개변수를 제공합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_path = <span class="string">"training_1/cp.ckpt"</span></span><br><span class="line">checkpoint_dir = os.path.dirname(checkpoint_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델의 가중치를 저장하는 콜백 만들기</span></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,</span><br><span class="line">                                                 save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                                                 verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 새로운 콜백으로 모델 훈련하기</span></span><br><span class="line">model.fit(train_images, </span><br><span class="line">          train_labels,  </span><br><span class="line">          epochs=<span class="number">10</span>,</span><br><span class="line">          validation_data=(test_images,test_labels),</span><br><span class="line">          callbacks=[cp_callback])  <span class="comment"># 콜백을 훈련에 전달합니다</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 옵티마이저의 상태를 저장하는 것과 관련되어 경고가 발생할 수 있습니다.</span></span><br><span class="line"><span class="comment"># 이 경고는 (그리고 이 노트북의 다른 비슷한 경고는) 이전 사용 방식을 권장하지 않기 위함이며 무시해도 좋습니다.</span></span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback.
Epoch 1/10
16/32 [==============&gt;...............] - ETA: 0s - loss: 1.8756 - accuracy: 0.3736 
Epoch 00001: saving model to training_1/cp.ckpt
32/32 [==============================] - 1s 30ms/step - loss: 1.5677 - accuracy: 0.5056 - val_loss: 0.6899 - val_accuracy: 0.7870
Epoch 2/10
31/32 [============================&gt;.] - ETA: 0s - loss: 0.4283 - accuracy: 0.8845
Epoch 00002: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 8ms/step - loss: 0.4276 - accuracy: 0.8844 - val_loss: 0.5193 - val_accuracy: 0.8380
Epoch 3/10
20/32 [=================&gt;............] - ETA: 0s - loss: 0.2892 - accuracy: 0.9208
Epoch 00003: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 6ms/step - loss: 0.2828 - accuracy: 0.9232 - val_loss: 0.4733 - val_accuracy: 0.8510
Epoch 4/10
19/32 [================&gt;.............] - ETA: 0s - loss: 0.1721 - accuracy: 0.9687
Epoch 00004: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9622 - val_loss: 0.4489 - val_accuracy: 0.8490
Epoch 5/10
17/32 [==============&gt;...............] - ETA: 0s - loss: 0.1666 - accuracy: 0.9582
Epoch 00005: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.9605 - val_loss: 0.4112 - val_accuracy: 0.8580
Epoch 6/10
30/32 [===========================&gt;..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9851
Epoch 00006: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 0.9846 - val_loss: 0.4088 - val_accuracy: 0.8650
Epoch 7/10
17/32 [==============&gt;...............] - ETA: 0s - loss: 0.0798 - accuracy: 0.9883
Epoch 00007: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9870 - val_loss: 0.4074 - val_accuracy: 0.8680
Epoch 8/10
32/32 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9899
Epoch 00008: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9900 - val_loss: 0.4204 - val_accuracy: 0.8590
Epoch 9/10
28/32 [=========================&gt;....] - ETA: 0s - loss: 0.0588 - accuracy: 0.9915
Epoch 00009: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 7ms/step - loss: 0.0574 - accuracy: 0.9920 - val_loss: 0.4110 - val_accuracy: 0.8640
Epoch 10/10
31/32 [============================&gt;.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9978
Epoch 00010: saving model to training_1/cp.ckpt
32/32 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 0.9978 - val_loss: 0.3962 - val_accuracy: 0.8660





&lt;tensorflow.python.keras.callbacks.History at 0x7fd0f59b3f10&gt;



이 코드는 tensorflow 체크포인트 파일을 만들고 에포크가 종료될 때마다 업데이트합니다:</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls &#123;checkpoint_dir&#125;</span><br></pre></td></tr></table></figure>

<pre><code>checkpoint                   cp.ckpt.index
cp.ckpt.data-00000-of-00001</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 새로운 모델 생성</span></span><br><span class="line">model = create_model()</span><br><span class="line"></span><br><span class="line">loss, acc = model.evaluate(test_images, test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"훈련되지 않은 모델의 정확도: &#123;:5.2f&#125;%"</span>.format(<span class="number">100</span>*acc))</span><br></pre></td></tr></table></figure>

<pre><code>32/32 - 0s - loss: 2.3409 - accuracy: 0.1250
훈련되지 않은 모델의 정확도: 12.50%


저장했던 모델을 로드하고 다시 평가해보도록 하겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가중치 로드</span></span><br><span class="line">model.load_weights(checkpoint_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 재평가</span></span><br><span class="line">loss, acc = model.evaluate(test_images, test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"복원된 모델의 정확도: &#123;:5.2f&#125;%"</span>.format(<span class="number">100</span>*acc))</span><br></pre></td></tr></table></figure>

<pre><code>32/32 - 0s - loss: 0.3962 - accuracy: 0.8660
복원된 모델의 정확도: 86.60%


체크포인트 콜백 매개변수</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)</span></span><br><span class="line">checkpoint_path = <span class="string">"training_2/cp-&#123;epoch:04d&#125;.ckpt"</span></span><br><span class="line">checkpoint_dir = os.path.dirname(checkpoint_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 다섯 번째 에포크마다 가중치를 저장하기 위한 콜백을 만듭니다</span></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(</span><br><span class="line">    filepath=checkpoint_path, </span><br><span class="line">    verbose=<span class="number">1</span>, </span><br><span class="line">    save_weights_only=<span class="literal">True</span>,</span><br><span class="line">    period=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 새로운 모델 객체를 만듭니다</span></span><br><span class="line">model = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># `checkpoint_path` 포맷을 사용하는 가중치를 저장합니다</span></span><br><span class="line">model.save_weights(checkpoint_path.format(epoch=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 새로운 콜백을 사용하여 모델을 훈련합니다</span></span><br><span class="line">model.fit(train_images, </span><br><span class="line">          train_labels,</span><br><span class="line">          epochs=<span class="number">50</span>, </span><br><span class="line">          callbacks=[cp_callback],</span><br><span class="line">          validation_data=(test_images,test_labels),</span><br><span class="line">          verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.
WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback.

Epoch 00005: saving model to training_2/cp-0005.ckpt

Epoch 00010: saving model to training_2/cp-0010.ckpt

Epoch 00015: saving model to training_2/cp-0015.ckpt

Epoch 00020: saving model to training_2/cp-0020.ckpt

Epoch 00025: saving model to training_2/cp-0025.ckpt

Epoch 00030: saving model to training_2/cp-0030.ckpt

Epoch 00035: saving model to training_2/cp-0035.ckpt

Epoch 00040: saving model to training_2/cp-0040.ckpt

Epoch 00045: saving model to training_2/cp-0045.ckpt

Epoch 00050: saving model to training_2/cp-0050.ckpt





&lt;tensorflow.python.keras.callbacks.History at 0x7fd0f1d481d0&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls &#123;checkpoint_dir&#125;</span><br></pre></td></tr></table></figure>

<pre><code>checkpoint                        cp-0025.ckpt.index
cp-0000.ckpt.data-00000-of-00001  cp-0030.ckpt.data-00000-of-00001
cp-0000.ckpt.index                cp-0030.ckpt.index
cp-0005.ckpt.data-00000-of-00001  cp-0035.ckpt.data-00000-of-00001
cp-0005.ckpt.index                cp-0035.ckpt.index
cp-0010.ckpt.data-00000-of-00001  cp-0040.ckpt.data-00000-of-00001
cp-0010.ckpt.index                cp-0040.ckpt.index
cp-0015.ckpt.data-00000-of-00001  cp-0045.ckpt.data-00000-of-00001
cp-0015.ckpt.index                cp-0045.ckpt.index
cp-0020.ckpt.data-00000-of-00001  cp-0050.ckpt.data-00000-of-00001
cp-0020.ckpt.index                cp-0050.ckpt.index
cp-0025.ckpt.data-00000-of-00001</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">latest = tf.train.latest_checkpoint(checkpoint_dir)</span><br><span class="line">latest</span><br></pre></td></tr></table></figure>




<pre><code>&apos;training_2/cp-0050.ckpt&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 초기화 및 생성</span></span><br><span class="line">model = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 로드</span></span><br><span class="line">model.load_weights(latest)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 복원, 평가</span></span><br><span class="line">loss, acc = model.evaluate(test_images, test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"복원된 모델의 정확도: &#123;:5.2f&#125;%"</span>.format(<span class="number">100</span>*acc))</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
32/32 - 0s - loss: 0.4795 - accuracy: 0.8720
복원된 모델의 정확도: 87.20%


수동으로 가중치 저장하기</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가중치를 저장합니다</span></span><br><span class="line">model.save_weights(<span class="string">'./checkpoints/my_checkpoint'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 새로운 모델 객체를 만듭니다</span></span><br><span class="line">model = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 가중치를 복원합니다</span></span><br><span class="line">model.load_weights(<span class="string">'./checkpoints/my_checkpoint'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델을 평가합니다</span></span><br><span class="line">loss,acc = model.evaluate(test_images,  test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"복원된 모델의 정확도: &#123;:5.2f&#125;%"</span>.format(<span class="number">100</span>*acc))</span><br></pre></td></tr></table></figure>

<pre><code>32/32 - 0s - loss: 0.4795 - accuracy: 0.8720
복원된 모델의 정확도: 87.20%</code></pre><h6 id="전체-모델-저장하기"><a href="#전체-모델-저장하기" class="headerlink" title="전체 모델 저장하기"></a>전체 모델 저장하기</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 새로운 모델 객체를 만들고 훈련합니다</span></span><br><span class="line">model = create_model()</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SavedModel로 전체 모델을 저장합니다</span></span><br><span class="line">!mkdir -p saved_model</span><br><span class="line">model.save(<span class="string">'saved_model/my_model'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
32/32 [==============================] - 0s 15ms/step - loss: 1.6664 - accuracy: 0.4644
Epoch 2/10
32/32 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8490
Epoch 3/10
32/32 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.9225
Epoch 4/10
32/32 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9644
Epoch 5/10
32/32 [==============================] - 0s 4ms/step - loss: 0.1473 - accuracy: 0.9746
Epoch 6/10
32/32 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9736
Epoch 7/10
32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9785
Epoch 8/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9967
Epoch 9/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9974
Epoch 10/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9988
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
INFO:tensorflow:Assets written to: saved_model/my_model/assets</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my_model 디렉토리</span></span><br><span class="line">!ls saved_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># assests 폴더, saved_model.pb, variables 폴더</span></span><br><span class="line">!ls saved_model/my_model</span><br></pre></td></tr></table></figure>

<pre><code>[34mmy_model[m[m
[34massets[m[m         saved_model.pb [34mvariables[m[m</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_model = tf.keras.models.load_model(<span class="string">'saved_model/my_model'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 구조를 확인합니다</span></span><br><span class="line">new_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_23&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_46 (Dense)             (None, 512)               401920    
_________________________________________________________________
dropout_23 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_47 (Dense)             (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 복원된 모델을 평가합니다</span></span><br><span class="line">loss, acc = new_model.evaluate(test_images,  test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'복원된 모델의 정확도: &#123;:5.2f&#125;%'</span>.format(<span class="number">100</span>*acc))</span><br><span class="line"></span><br><span class="line">print(new_model.predict(test_images).shape)</span><br></pre></td></tr></table></figure>

<pre><code>32/32 - 0s - loss: 0.4205 - accuracy: 0.0880
복원된 모델의 정확도:  8.80%
(1000, 10)</code></pre><h6 id="HDF5-파일로-저장하기"><a href="#HDF5-파일로-저장하기" class="headerlink" title="HDF5 파일로 저장하기"></a>HDF5 파일로 저장하기</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 새로운 모델 객체를 만들고 훈련합니다</span></span><br><span class="line">model = create_model()</span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 전체 모델을 HDF5 파일로 저장합니다</span></span><br><span class="line"><span class="comment"># '.h5' 확장자는 이 모델이 HDF5로 저장되었다는 것을 나타냅니다</span></span><br><span class="line">model.save(<span class="string">'my_model.h5'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
32/32 [==============================] - 0s 14ms/step - loss: 1.6326 - accuracy: 0.5135
Epoch 2/10
32/32 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8959
Epoch 3/10
32/32 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.9177
Epoch 4/10
32/32 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9320
Epoch 5/10
32/32 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9757
Epoch 6/10
32/32 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9879
Epoch 7/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9864
Epoch 8/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9946
Epoch 9/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9925
Epoch 10/10
32/32 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.9994</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가중치와 옵티마이저를 포함하여 정확히 동일한 모델을 다시 생성합니다</span></span><br><span class="line">new_model = tf.keras.models.load_model(<span class="string">'my_model.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 구조를 출력합니다</span></span><br><span class="line">new_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_25&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_50 (Dense)             (None, 512)               401920    
_________________________________________________________________
dropout_25 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_51 (Dense)             (None, 10)                5130      
=================================================================
Total params: 407,050
Trainable params: 407,050
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, acc = new_model.evaluate(test_images,  test_labels, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'복원된 모델의 정확도: &#123;:5.2f&#125;%'</span>.format(<span class="number">100</span>*acc))</span><br></pre></td></tr></table></figure>

<pre><code>32/32 - 0s - loss: 0.4255 - accuracy: 0.0890
복원된 모델의 정확도:  8.90%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-25T11:21:43.000Z" title="2020-08-25T11:21:43.000Z">2020-08-25</time><span class="level-item">19 minutes read (About 2880 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/25/%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9-Overfitting-%EA%B3%BC-%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9-Underfitting/">과대적합(Overfitting)과 과소적합(Underfitting)</a></h1><div class="content"><h6 id="과대적합-Overfitting-과-과소적합-Underfitting"><a href="#과대적합-Overfitting-과-과소적합-Underfitting" class="headerlink" title="과대적합(Overfitting)과 과소적합(Underfitting)"></a>과대적합(Overfitting)과 과소적합(Underfitting)</h6><pre><code>일정 에포크 동안 훈련을 시키면 검증세트에서 모델 성능이 최고점에 도달한 다음 감소하기 시작한 것을 알 수 있습니다.
훈련 세트에서 높은 성능을 얻을 수 있지만 진짜 원하는 것은 테스트 세트(또는 이전에 본 적 없는 데이터)에 잘 일반화되는 모델입니다.

과소적합이란 테스트 세트의 성능이 향상될 여지가 아직 있을 때 일어납니다. 발생하는 원인은 여러가지입니다. 모델이 너무 단순하거나, 규제가 너무 많거나, 그냥 단순히 충분히 오래 훈련하지 않는 경우입니다. 즉 네트워크가 훈련 세트에서 적절한 패턴을 학습하지 못했다는 뜻입니다.

모델을 너무 오래 훈련하면 과대적합되기 시작하고 테스트 세트에서 일반화되지 못하는 패턴을 훈련 세트에서 학습합니다. 과대적합과 과소적합 사이에서 균형을 잡아야 합니다.

균형을 잘 잡고 과대적합을 방지하기 위한 2가지 규제방법을 알아보도록 하겠습니다</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>2.4.0-dev20200724


데이터셋 다운로드를 받고 원핫 인코딩으로 변환하자!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">NUM_WORDS = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_hot_sequences</span><span class="params">(sequences, dimension)</span>:</span></span><br><span class="line">    <span class="comment"># 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, word_indices <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, word_indices] = <span class="number">1.0</span>  <span class="comment"># results[i]의 특정 인덱스만 1로 설정합니다</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)</span><br><span class="line">test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(train_data[<span class="number">0</span>])</span><br><span class="line">plt.grid(<span class="literal">False</span>)</span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="374" alt="output_4_0" src="https://user-images.githubusercontent.com/59719711/91168503-b4fa0a80-e710-11ea-9844-30bd30bac3a6.png">


<pre><code>기준 모델을 만들어 기준보다 유닛의 수가 크거나 작은 모델과 비교를 해보겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">base_model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(NUM_WORDS,)),</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">base_model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                   loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                   metrics=[<span class="string">'accuracy'</span>, <span class="string">'binary_crossentropy'</span>])</span><br><span class="line"></span><br><span class="line">base_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 16)                16016     
_________________________________________________________________
dense_12 (Dense)             (None, 16)                272       
_________________________________________________________________
dense_13 (Dense)             (None, 1)                 17        
=================================================================
Total params: 16,305
Trainable params: 16,305
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">base_history = base_model.fit(train_data, train_labels, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>,</span><br><span class="line">                             validation_data=(test_data, test_labels), verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/20
49/49 - 0s - loss: 0.2555 - accuracy: 0.8971 - binary_crossentropy: 0.2555 - val_loss: 0.3410 - val_accuracy: 0.8558 - val_binary_crossentropy: 0.3410
Epoch 2/20
49/49 - 0s - loss: 0.2436 - accuracy: 0.9030 - binary_crossentropy: 0.2436 - val_loss: 0.3454 - val_accuracy: 0.8540 - val_binary_crossentropy: 0.3454
Epoch 3/20
49/49 - 0s - loss: 0.2356 - accuracy: 0.9068 - binary_crossentropy: 0.2356 - val_loss: 0.3525 - val_accuracy: 0.8508 - val_binary_crossentropy: 0.3525
Epoch 4/20
49/49 - 0s - loss: 0.2259 - accuracy: 0.9102 - binary_crossentropy: 0.2259 - val_loss: 0.3638 - val_accuracy: 0.8482 - val_binary_crossentropy: 0.3638
Epoch 5/20
49/49 - 0s - loss: 0.2178 - accuracy: 0.9142 - binary_crossentropy: 0.2178 - val_loss: 0.3701 - val_accuracy: 0.8487 - val_binary_crossentropy: 0.3701
Epoch 6/20
49/49 - 0s - loss: 0.2093 - accuracy: 0.9188 - binary_crossentropy: 0.2093 - val_loss: 0.3809 - val_accuracy: 0.8469 - val_binary_crossentropy: 0.3809
Epoch 7/20
49/49 - 0s - loss: 0.2026 - accuracy: 0.9208 - binary_crossentropy: 0.2026 - val_loss: 0.3854 - val_accuracy: 0.8465 - val_binary_crossentropy: 0.3854
Epoch 8/20
49/49 - 0s - loss: 0.1963 - accuracy: 0.9240 - binary_crossentropy: 0.1963 - val_loss: 0.3996 - val_accuracy: 0.8430 - val_binary_crossentropy: 0.3996
Epoch 9/20
49/49 - 0s - loss: 0.1905 - accuracy: 0.9254 - binary_crossentropy: 0.1905 - val_loss: 0.4014 - val_accuracy: 0.8421 - val_binary_crossentropy: 0.4014
Epoch 10/20
49/49 - 0s - loss: 0.1846 - accuracy: 0.9307 - binary_crossentropy: 0.1846 - val_loss: 0.4143 - val_accuracy: 0.8418 - val_binary_crossentropy: 0.4143
Epoch 11/20
49/49 - 0s - loss: 0.1787 - accuracy: 0.9322 - binary_crossentropy: 0.1787 - val_loss: 0.4300 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.4300
Epoch 12/20
49/49 - 0s - loss: 0.1739 - accuracy: 0.9329 - binary_crossentropy: 0.1739 - val_loss: 0.4402 - val_accuracy: 0.8372 - val_binary_crossentropy: 0.4402
Epoch 13/20
49/49 - 0s - loss: 0.1663 - accuracy: 0.9373 - binary_crossentropy: 0.1663 - val_loss: 0.4508 - val_accuracy: 0.8358 - val_binary_crossentropy: 0.4508
Epoch 14/20
49/49 - 0s - loss: 0.1613 - accuracy: 0.9396 - binary_crossentropy: 0.1613 - val_loss: 0.4584 - val_accuracy: 0.8364 - val_binary_crossentropy: 0.4584
Epoch 15/20
49/49 - 0s - loss: 0.1581 - accuracy: 0.9400 - binary_crossentropy: 0.1581 - val_loss: 0.4805 - val_accuracy: 0.8356 - val_binary_crossentropy: 0.4805
Epoch 16/20
49/49 - 0s - loss: 0.1534 - accuracy: 0.9419 - binary_crossentropy: 0.1534 - val_loss: 0.4836 - val_accuracy: 0.8343 - val_binary_crossentropy: 0.4836
Epoch 17/20
49/49 - 0s - loss: 0.1477 - accuracy: 0.9454 - binary_crossentropy: 0.1477 - val_loss: 0.5082 - val_accuracy: 0.8330 - val_binary_crossentropy: 0.5082
Epoch 18/20
49/49 - 0s - loss: 0.1440 - accuracy: 0.9458 - binary_crossentropy: 0.1440 - val_loss: 0.5069 - val_accuracy: 0.8342 - val_binary_crossentropy: 0.5069
Epoch 19/20
49/49 - 0s - loss: 0.1382 - accuracy: 0.9489 - binary_crossentropy: 0.1382 - val_loss: 0.5187 - val_accuracy: 0.8323 - val_binary_crossentropy: 0.5187
Epoch 20/20
49/49 - 0s - loss: 0.1339 - accuracy: 0.9520 - binary_crossentropy: 0.1339 - val_loss: 0.5385 - val_accuracy: 0.8310 - val_binary_crossentropy: 0.5385


작은 모델을 만들어보자</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">small_model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">6</span>, activation=<span class="string">'relu'</span>, input_shape=(NUM_WORDS,)),</span><br><span class="line">    keras.layers.Dense(<span class="number">6</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">small_model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                   loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                   metrics=[<span class="string">'accuracy'</span>, <span class="string">'binary_crossentropy'</span>])</span><br><span class="line"></span><br><span class="line">small_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_18 (Dense)             (None, 6)                 6006      
_________________________________________________________________
dense_19 (Dense)             (None, 6)                 42        
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 7         
=================================================================
Total params: 6,055
Trainable params: 6,055
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">small_history = small_model.fit(train_data, train_labels, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>,</span><br><span class="line">                             validation_data=(test_data, test_labels), verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/20
49/49 - 0s - loss: 0.2994 - accuracy: 0.8785 - binary_crossentropy: 0.2994 - val_loss: 0.3305 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3305
Epoch 2/20
49/49 - 0s - loss: 0.2972 - accuracy: 0.8790 - binary_crossentropy: 0.2972 - val_loss: 0.3306 - val_accuracy: 0.8599 - val_binary_crossentropy: 0.3306
Epoch 3/20
49/49 - 0s - loss: 0.2970 - accuracy: 0.8782 - binary_crossentropy: 0.2970 - val_loss: 0.3343 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3343
Epoch 4/20
49/49 - 0s - loss: 0.2965 - accuracy: 0.8777 - binary_crossentropy: 0.2965 - val_loss: 0.3312 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3312
Epoch 5/20
49/49 - 0s - loss: 0.2960 - accuracy: 0.8794 - binary_crossentropy: 0.2960 - val_loss: 0.3314 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3314
Epoch 6/20
49/49 - 0s - loss: 0.2957 - accuracy: 0.8783 - binary_crossentropy: 0.2957 - val_loss: 0.3320 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3320
Epoch 7/20
49/49 - 0s - loss: 0.2968 - accuracy: 0.8768 - binary_crossentropy: 0.2968 - val_loss: 0.3321 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3321
Epoch 8/20
49/49 - 0s - loss: 0.2960 - accuracy: 0.8790 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8594 - val_binary_crossentropy: 0.3323
Epoch 9/20
49/49 - 0s - loss: 0.2960 - accuracy: 0.8787 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8582 - val_binary_crossentropy: 0.3323
Epoch 10/20
49/49 - 0s - loss: 0.2959 - accuracy: 0.8784 - binary_crossentropy: 0.2959 - val_loss: 0.3327 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3327
Epoch 11/20
49/49 - 0s - loss: 0.2953 - accuracy: 0.8789 - binary_crossentropy: 0.2953 - val_loss: 0.3334 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3334
Epoch 12/20
49/49 - 0s - loss: 0.2970 - accuracy: 0.8775 - binary_crossentropy: 0.2970 - val_loss: 0.3334 - val_accuracy: 0.8578 - val_binary_crossentropy: 0.3334
Epoch 13/20
49/49 - 0s - loss: 0.2951 - accuracy: 0.8798 - binary_crossentropy: 0.2951 - val_loss: 0.3341 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3341
Epoch 14/20
49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3323 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3323
Epoch 15/20
49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3324 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3324
Epoch 16/20
49/49 - 0s - loss: 0.2949 - accuracy: 0.8790 - binary_crossentropy: 0.2949 - val_loss: 0.3330 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3330
Epoch 17/20
49/49 - 0s - loss: 0.2946 - accuracy: 0.8784 - binary_crossentropy: 0.2946 - val_loss: 0.3324 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3324
Epoch 18/20
49/49 - 0s - loss: 0.2952 - accuracy: 0.8784 - binary_crossentropy: 0.2952 - val_loss: 0.3329 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3329
Epoch 19/20
49/49 - 0s - loss: 0.2943 - accuracy: 0.8794 - binary_crossentropy: 0.2943 - val_loss: 0.3330 - val_accuracy: 0.8588 - val_binary_crossentropy: 0.3330
Epoch 20/20
49/49 - 0s - loss: 0.2949 - accuracy: 0.8789 - binary_crossentropy: 0.2949 - val_loss: 0.3329 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3329


큰 모델 만들기</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">big_model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>, input_shape=(NUM_WORDS,)),</span><br><span class="line">    keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">big_model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                   loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                   metrics=[<span class="string">'accuracy'</span>, <span class="string">'binary_crossentropy'</span>])</span><br><span class="line"></span><br><span class="line">big_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_21 (Dense)             (None, 128)               128128    
_________________________________________________________________
dense_22 (Dense)             (None, 128)               16512     
_________________________________________________________________
dense_23 (Dense)             (None, 1)                 129       
=================================================================
Total params: 144,769
Trainable params: 144,769
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">big_history = big_model.fit(train_data, train_labels, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>,</span><br><span class="line">                             validation_data=(test_data, test_labels), verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/20
49/49 - 0s - loss: 0.0047 - accuracy: 0.9999 - binary_crossentropy: 0.0047 - val_loss: 0.6867 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.6867
Epoch 2/20
49/49 - 0s - loss: 0.0029 - accuracy: 1.0000 - binary_crossentropy: 0.0029 - val_loss: 0.7205 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.7205
Epoch 3/20
49/49 - 0s - loss: 0.0019 - accuracy: 1.0000 - binary_crossentropy: 0.0019 - val_loss: 0.7533 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.7533
Epoch 4/20
49/49 - 0s - loss: 0.0014 - accuracy: 1.0000 - binary_crossentropy: 0.0014 - val_loss: 0.7802 - val_accuracy: 0.8383 - val_binary_crossentropy: 0.7802
Epoch 5/20
49/49 - 0s - loss: 0.0010 - accuracy: 1.0000 - binary_crossentropy: 0.0010 - val_loss: 0.8079 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8079
Epoch 6/20
49/49 - 0s - loss: 8.0437e-04 - accuracy: 1.0000 - binary_crossentropy: 8.0437e-04 - val_loss: 0.8324 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8324
Epoch 7/20
49/49 - 0s - loss: 6.4169e-04 - accuracy: 1.0000 - binary_crossentropy: 6.4169e-04 - val_loss: 0.8510 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8510
Epoch 8/20
49/49 - 0s - loss: 5.2259e-04 - accuracy: 1.0000 - binary_crossentropy: 5.2259e-04 - val_loss: 0.8707 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8707
Epoch 9/20
49/49 - 0s - loss: 4.3499e-04 - accuracy: 1.0000 - binary_crossentropy: 4.3499e-04 - val_loss: 0.8885 - val_accuracy: 0.8395 - val_binary_crossentropy: 0.8885
Epoch 10/20
49/49 - 0s - loss: 3.6612e-04 - accuracy: 1.0000 - binary_crossentropy: 3.6612e-04 - val_loss: 0.9055 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.9055
Epoch 11/20
49/49 - 0s - loss: 3.1179e-04 - accuracy: 1.0000 - binary_crossentropy: 3.1179e-04 - val_loss: 0.9202 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9202
Epoch 12/20
49/49 - 0s - loss: 2.6851e-04 - accuracy: 1.0000 - binary_crossentropy: 2.6851e-04 - val_loss: 0.9358 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9358
Epoch 13/20
49/49 - 0s - loss: 2.3418e-04 - accuracy: 1.0000 - binary_crossentropy: 2.3418e-04 - val_loss: 0.9482 - val_accuracy: 0.8399 - val_binary_crossentropy: 0.9482
Epoch 14/20
49/49 - 0s - loss: 2.0480e-04 - accuracy: 1.0000 - binary_crossentropy: 2.0480e-04 - val_loss: 0.9615 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9615
Epoch 15/20
49/49 - 0s - loss: 1.8099e-04 - accuracy: 1.0000 - binary_crossentropy: 1.8099e-04 - val_loss: 0.9732 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9732
Epoch 16/20
49/49 - 0s - loss: 1.6065e-04 - accuracy: 1.0000 - binary_crossentropy: 1.6065e-04 - val_loss: 0.9851 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9851
Epoch 17/20
49/49 - 0s - loss: 1.4336e-04 - accuracy: 1.0000 - binary_crossentropy: 1.4336e-04 - val_loss: 0.9966 - val_accuracy: 0.8401 - val_binary_crossentropy: 0.9966
Epoch 18/20
49/49 - 0s - loss: 1.2880e-04 - accuracy: 1.0000 - binary_crossentropy: 1.2880e-04 - val_loss: 1.0070 - val_accuracy: 0.8399 - val_binary_crossentropy: 1.0070
Epoch 19/20
49/49 - 0s - loss: 1.1636e-04 - accuracy: 1.0000 - binary_crossentropy: 1.1636e-04 - val_loss: 1.0171 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0171
Epoch 20/20
49/49 - 0s - loss: 1.0553e-04 - accuracy: 1.0000 - binary_crossentropy: 1.0553e-04 - val_loss: 1.0270 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0270</code></pre><p>training dataset의 loss(손실)값과 test dataset의 loss(손실)값 시각화</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_history</span><span class="params">(histories, key=<span class="string">'binary_crossentropy'</span>)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> name, history <span class="keyword">in</span> histories:</span><br><span class="line">        val = plt.plot(history.epoch, history.history[<span class="string">'val_'</span> + key],</span><br><span class="line">                      <span class="string">'--'</span>, label=name.title()+<span class="string">' Val'</span>)</span><br><span class="line">        plt.plot(history.epoch, history.history[key], color=val[<span class="number">0</span>].get_color(),</span><br><span class="line">                label=name.title()+<span class="string">'Train'</span>)</span><br><span class="line">                       </span><br><span class="line">    plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">    plt.ylabel(key.replace(<span class="string">'-'</span>, <span class="string">' '</span>).title())</span><br><span class="line">    plt.legend()</span><br><span class="line">    </span><br><span class="line">    plt.xlim([<span class="number">0</span>, max(history.epoch)])</span><br><span class="line">plot_history([(<span class="string">'base'</span>, base_history),</span><br><span class="line">              (<span class="string">'smaller'</span>, small_history),</span><br><span class="line">              (<span class="string">'bigger'</span>, big_history)])</span><br></pre></td></tr></table></figure>



<img width="947" alt="output_15_0" src="https://user-images.githubusercontent.com/59719711/91168509-b6c3ce00-e710-11ea-9ccf-2e3a684d81c9.png">



<p>big model의 경우 에포크가 시작하자마자 과대적합(Overfitting)이 일어나는 것을 알 수 있고 생각보다 심하게 이뤄집니다. 모델 네트워크의 용량이 많을수록 과대적합이 될 확률이 커집니다.(훈련 loss값과 검증 loss값 사이에 큰 차이가 발생)</p>
<p>과대적합(Overfitting)을 방지하기 위한 전략</p>
<pre><code>- 가중치 규제하기
    1. 훈련 데이터와 네트워크 구조가 주어졌을 때, 데이터를 설명할 수 있는 가중치의 조합을 간단하게!
    2. 모델 파라미터의 분포를 봤을 때 엔트로피가 작은 모델(적은 파라미터를 가지는 모델), 즉 과대적합을 완화시키는 일반적인 방법은 가중치가 작은 값을 가지도록 네트워크의 복잡도에 제약을 가하는 것이라고 할 수 있습니다. &apos;가중치 규제(Weight regularization)
        * L1 규제는 가중치의 절댓값에 비례하는 비용이 추가
        * L2 규제는 가중치의 제곱에 비례하는 비용이 추가, 신경망에서는 L2규제를 가중치 감쇠(weight decay)라고도 합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">l2_model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>),</span><br><span class="line">                       activation=<span class="string">'relu'</span>, input_shape=(NUM_WORDS,)),</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, kernel_regularizer=keras.regularizers.l2(<span class="number">0.001</span>),</span><br><span class="line">                       activation=<span class="string">'relu'</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">l2_model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                   loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                   metrics=[<span class="string">'accuracy'</span>, <span class="string">'binary_crossentropy'</span>])</span><br><span class="line"></span><br><span class="line">l2_history = l2_model.fit(train_data, train_labels, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>,</span><br><span class="line">                             validation_data=(test_data, test_labels), verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/20
49/49 - 1s - loss: 0.6362 - accuracy: 0.6929 - binary_crossentropy: 0.5927 - val_loss: 0.4927 - val_accuracy: 0.8113 - val_binary_crossentropy: 0.4513
Epoch 2/20
49/49 - 0s - loss: 0.4164 - accuracy: 0.8462 - binary_crossentropy: 0.3749 - val_loss: 0.3873 - val_accuracy: 0.8545 - val_binary_crossentropy: 0.3460
Epoch 3/20
49/49 - 0s - loss: 0.3636 - accuracy: 0.8669 - binary_crossentropy: 0.3230 - val_loss: 0.3708 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3312
Epoch 4/20
49/49 - 0s - loss: 0.3498 - accuracy: 0.8721 - binary_crossentropy: 0.3113 - val_loss: 0.3687 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3312
Epoch 5/20
49/49 - 0s - loss: 0.3440 - accuracy: 0.8726 - binary_crossentropy: 0.3073 - val_loss: 0.3640 - val_accuracy: 0.8602 - val_binary_crossentropy: 0.3283
Epoch 6/20
49/49 - 0s - loss: 0.3393 - accuracy: 0.8760 - binary_crossentropy: 0.3044 - val_loss: 0.3622 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3281
Epoch 7/20
49/49 - 0s - loss: 0.3369 - accuracy: 0.8749 - binary_crossentropy: 0.3034 - val_loss: 0.3604 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3276
Epoch 8/20
49/49 - 0s - loss: 0.3349 - accuracy: 0.8754 - binary_crossentropy: 0.3027 - val_loss: 0.3595 - val_accuracy: 0.8595 - val_binary_crossentropy: 0.3281
Epoch 9/20
49/49 - 0s - loss: 0.3325 - accuracy: 0.8746 - binary_crossentropy: 0.3015 - val_loss: 0.3608 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3304
Epoch 10/20
49/49 - 0s - loss: 0.3332 - accuracy: 0.8744 - binary_crossentropy: 0.3031 - val_loss: 0.3599 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3304
Epoch 11/20
49/49 - 0s - loss: 0.3305 - accuracy: 0.8750 - binary_crossentropy: 0.3012 - val_loss: 0.3563 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3274
Epoch 12/20
49/49 - 0s - loss: 0.3290 - accuracy: 0.8748 - binary_crossentropy: 0.3004 - val_loss: 0.3554 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3272
Epoch 13/20
49/49 - 0s - loss: 0.3272 - accuracy: 0.8752 - binary_crossentropy: 0.2991 - val_loss: 0.3526 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247
Epoch 14/20
49/49 - 0s - loss: 0.3251 - accuracy: 0.8760 - binary_crossentropy: 0.2972 - val_loss: 0.3522 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3243
Epoch 15/20
49/49 - 0s - loss: 0.3232 - accuracy: 0.8759 - binary_crossentropy: 0.2953 - val_loss: 0.3547 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3268
Epoch 16/20
49/49 - 0s - loss: 0.3214 - accuracy: 0.8770 - binary_crossentropy: 0.2936 - val_loss: 0.3522 - val_accuracy: 0.8601 - val_binary_crossentropy: 0.3246
Epoch 17/20
49/49 - 0s - loss: 0.3201 - accuracy: 0.8781 - binary_crossentropy: 0.2926 - val_loss: 0.3512 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3238
Epoch 18/20
49/49 - 0s - loss: 0.3194 - accuracy: 0.8766 - binary_crossentropy: 0.2921 - val_loss: 0.3544 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3271
Epoch 19/20
49/49 - 0s - loss: 0.3180 - accuracy: 0.8772 - binary_crossentropy: 0.2908 - val_loss: 0.3509 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3238
Epoch 20/20
49/49 - 0s - loss: 0.3167 - accuracy: 0.8768 - binary_crossentropy: 0.2896 - val_loss: 0.3491 - val_accuracy: 0.8608 - val_binary_crossentropy: 0.3221</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot_history([(<span class="string">'base'</span>, base_history),</span><br><span class="line">              (<span class="string">'L2'</span>, l2_history)</span><br><span class="line">             ])</span><br></pre></td></tr></table></figure>


<img width="947" alt="output_20_0" src="https://user-images.githubusercontent.com/59719711/91168556-cb07cb00-e710-11ea-88ae-600d0836acfe.png">



<p>결과에서 보듯이 모델 파라미터의 개수는 똑같지만 L2규제를 적용한 모델이 base model보다 과대적합에 훨씬 잘 견디고 있는 것을 볼 수 있습니다.</p>
<pre><code>- dropout 추가하기
    * 신경망에서 쓰이는 가장 효과적이고 널리 사용하는 규제 기법중 하나입니다.
    * dropout은 층을 이용해 네트워크에 추가할 수 있습니다.

두 개의 층에 dropout 규제를 추가하여 과대적합이 얼마나 감소하는지 알아 보겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">dpt_model = keras.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(NUM_WORDS,)),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dpt_model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">                   loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                   metrics=[<span class="string">'accuracy'</span>, <span class="string">'binary_crossentropy'</span>])</span><br><span class="line"></span><br><span class="line">dpt_history = dpt_model.fit(train_data, train_labels, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>,</span><br><span class="line">                             validation_data=(test_data, test_labels), verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/20
49/49 - 1s - loss: 0.6841 - accuracy: 0.5583 - binary_crossentropy: 0.6841 - val_loss: 0.6280 - val_accuracy: 0.7269 - val_binary_crossentropy: 0.6280
Epoch 2/20
49/49 - 0s - loss: 0.5848 - accuracy: 0.6974 - binary_crossentropy: 0.5848 - val_loss: 0.4655 - val_accuracy: 0.8180 - val_binary_crossentropy: 0.4655
Epoch 3/20
49/49 - 0s - loss: 0.4784 - accuracy: 0.7861 - binary_crossentropy: 0.4784 - val_loss: 0.3797 - val_accuracy: 0.8453 - val_binary_crossentropy: 0.3797
Epoch 4/20
49/49 - 0s - loss: 0.4250 - accuracy: 0.8195 - binary_crossentropy: 0.4250 - val_loss: 0.3453 - val_accuracy: 0.8510 - val_binary_crossentropy: 0.3453
Epoch 5/20
49/49 - 0s - loss: 0.3931 - accuracy: 0.8381 - binary_crossentropy: 0.3931 - val_loss: 0.3338 - val_accuracy: 0.8548 - val_binary_crossentropy: 0.3338
Epoch 6/20
49/49 - 0s - loss: 0.3758 - accuracy: 0.8480 - binary_crossentropy: 0.3758 - val_loss: 0.3299 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3299
Epoch 7/20
49/49 - 0s - loss: 0.3600 - accuracy: 0.8544 - binary_crossentropy: 0.3600 - val_loss: 0.3224 - val_accuracy: 0.8612 - val_binary_crossentropy: 0.3224
Epoch 8/20
49/49 - 0s - loss: 0.3493 - accuracy: 0.8607 - binary_crossentropy: 0.3493 - val_loss: 0.3227 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3227
Epoch 9/20
49/49 - 0s - loss: 0.3442 - accuracy: 0.8605 - binary_crossentropy: 0.3442 - val_loss: 0.3226 - val_accuracy: 0.8618 - val_binary_crossentropy: 0.3226
Epoch 10/20
49/49 - 0s - loss: 0.3317 - accuracy: 0.8674 - binary_crossentropy: 0.3317 - val_loss: 0.3230 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3230
Epoch 11/20
49/49 - 0s - loss: 0.3267 - accuracy: 0.8691 - binary_crossentropy: 0.3267 - val_loss: 0.3247 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247
Epoch 12/20
49/49 - 0s - loss: 0.3242 - accuracy: 0.8695 - binary_crossentropy: 0.3242 - val_loss: 0.3261 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3261
Epoch 13/20
49/49 - 0s - loss: 0.3153 - accuracy: 0.8721 - binary_crossentropy: 0.3153 - val_loss: 0.3289 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3289
Epoch 14/20
49/49 - 0s - loss: 0.3092 - accuracy: 0.8742 - binary_crossentropy: 0.3092 - val_loss: 0.3294 - val_accuracy: 0.8573 - val_binary_crossentropy: 0.3294
Epoch 15/20
49/49 - 0s - loss: 0.3103 - accuracy: 0.8772 - binary_crossentropy: 0.3103 - val_loss: 0.3312 - val_accuracy: 0.8576 - val_binary_crossentropy: 0.3312
Epoch 16/20
49/49 - 0s - loss: 0.3010 - accuracy: 0.8815 - binary_crossentropy: 0.3010 - val_loss: 0.3363 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3363
Epoch 17/20
49/49 - 0s - loss: 0.3010 - accuracy: 0.8788 - binary_crossentropy: 0.3010 - val_loss: 0.3338 - val_accuracy: 0.8570 - val_binary_crossentropy: 0.3338
Epoch 18/20
49/49 - 0s - loss: 0.2975 - accuracy: 0.8824 - binary_crossentropy: 0.2975 - val_loss: 0.3343 - val_accuracy: 0.8564 - val_binary_crossentropy: 0.3343
Epoch 19/20
49/49 - 0s - loss: 0.2923 - accuracy: 0.8823 - binary_crossentropy: 0.2923 - val_loss: 0.3417 - val_accuracy: 0.8556 - val_binary_crossentropy: 0.3417
Epoch 20/20
49/49 - 0s - loss: 0.2910 - accuracy: 0.8830 - binary_crossentropy: 0.2910 - val_loss: 0.3452 - val_accuracy: 0.8560 - val_binary_crossentropy: 0.3452</code></pre><p>검증 고고</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot_history([(<span class="string">'base'</span>, base_history),</span><br><span class="line">              (<span class="string">'dropout'</span>, dpt_history)</span><br><span class="line">             ])</span><br></pre></td></tr></table></figure>


<img width="947" alt="output_25_0" src="https://user-images.githubusercontent.com/59719711/91168597-db1faa80-e710-11ea-832a-55d34038d224.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot_history([(<span class="string">'base'</span>, base_history),</span><br><span class="line">              (<span class="string">'dropout'</span>, dpt_history),</span><br><span class="line">              (<span class="string">'L2'</span>, l2_history)</span><br><span class="line">             ])</span><br></pre></td></tr></table></figure>


<img width="947" alt="output_26_0" src="https://user-images.githubusercontent.com/59719711/91168603-dd820480-e710-11ea-8916-4ba873e21636.png">


<h6 id="과대적합을-방지하기-위한-결론"><a href="#과대적합을-방지하기-위한-결론" class="headerlink" title="과대적합을 방지하기 위한 결론"></a>과대적합을 방지하기 위한 결론</h6><pre><code>1. 더 많은 훈련 데이터를 학습시킨다.
2. 네트워크의 용량을 줄인다. (ex. Dense(16 ..)
3. 가중치 규제를 추가한다. (L2)
4. 드롭아웃을 추가한다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-21T06:29:24.000Z" title="2020-08-21T06:29:24.000Z">2020-08-21</time><span class="level-item">9 minutes read (About 1383 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/21/Tensorflow%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80-%EB%AA%A8%EB%8D%B8%EB%A7%81/">Tensorflow를 활용한 회귀 모델링</a></h1><div class="content"><h6 id="자동차-연비-예측하기"><a href="#자동차-연비-예측하기" class="headerlink" title="자동차 연비 예측하기"></a>자동차 연비 예측하기</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>2.4.0-dev20200724</code></pre><h6 id="Auto-MPG-데이터셋"><a href="#Auto-MPG-데이터셋" class="headerlink" title="Auto MPG 데이터셋"></a>Auto MPG 데이터셋</h6><pre><code>UCI 머신러닝 저장소에서 다운로드를 받자!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset_path = keras.utils.get_file(<span class="string">"auto-mpg.data"</span>, <span class="string">"http://archive.ics.uci.edu/ml/\</span></span><br><span class="line"><span class="string">                                    machine-learning-databases/auto-mpg/auto-mpg.data"</span>)</span><br><span class="line">dataset_path</span><br></pre></td></tr></table></figure>




<pre><code>&apos;/Users/wglee/.keras/datasets/auto-mpg.data&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 불러오기</span></span><br><span class="line">column_names = [<span class="string">'MPG'</span>, <span class="string">'Cylinders'</span>, <span class="string">'Displacement'</span>, <span class="string">'Horsepwer'</span>, <span class="string">'Weight'</span>, <span class="string">'Acceleration'</span>,\</span><br><span class="line">               <span class="string">'Model_year'</span>, <span class="string">'Origin'</span>]</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(dataset_path, names=column_names, na_values=<span class="string">'?'</span>, comment=<span class="string">'\t'</span>, sep=<span class="string">' '</span>,\</span><br><span class="line">                     skipinitialspace=<span class="literal">True</span>)</span><br><span class="line">df = dataset.copy()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.tail(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MPG</th>
      <th>Cylinders</th>
      <th>Displacement</th>
      <th>Horsepwer</th>
      <th>Weight</th>
      <th>Acceleration</th>
      <th>Model_year</th>
      <th>Origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>396</th>
      <td>28.0</td>
      <td>4</td>
      <td>120.0</td>
      <td>79.0</td>
      <td>2625.0</td>
      <td>18.6</td>
      <td>82</td>
      <td>1</td>
    </tr>
    <tr>
      <th>397</th>
      <td>31.0</td>
      <td>4</td>
      <td>119.0</td>
      <td>82.0</td>
      <td>2720.0</td>
      <td>19.4</td>
      <td>82</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'Origin'</span>].unique()</span><br></pre></td></tr></table></figure>




<pre><code>array([1, 3, 2])



null값 확인 결과 6개의 데이터가 누락된 것을 확인하였고 제거 정제 작업을 하였습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.isnull().sum()</span><br></pre></td></tr></table></figure>




<pre><code>MPG             0
Cylinders       0
Displacement    0
Horsepwer       6
Weight          0
Acceleration    0
Model_year      0
Origin          0
dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.dropna(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line">msno.matrix(df, figsize=(<span class="number">8</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7faaa5864f10&gt;</code></pre><img width="512" alt="output_10_1" src="https://user-images.githubusercontent.com/59719711/90860004-345fa500-e3c4-11ea-955f-87703c4be123.png">


<pre><code>&quot;Origin&quot; 열은 수치형이 아니고 범주형이므로 원-핫 인코딩(one-hot encoding)으로 변환</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">origin = df.pop(<span class="string">'Origin'</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'USA'</span>] = (origin == <span class="number">1</span>) * <span class="number">1.0</span></span><br><span class="line">df[<span class="string">'Europe'</span>] = (origin == <span class="number">2</span>) * <span class="number">2.0</span></span><br><span class="line">df[<span class="string">'Japan'</span>] = (origin == <span class="number">3</span>) * <span class="number">3.0</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.tail(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MPG</th>
      <th>Cylinders</th>
      <th>Displacement</th>
      <th>Horsepwer</th>
      <th>Weight</th>
      <th>Acceleration</th>
      <th>Model_year</th>
      <th>USA</th>
      <th>Europe</th>
      <th>Japan</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>396</th>
      <td>28.0</td>
      <td>4</td>
      <td>120.0</td>
      <td>79.0</td>
      <td>2625.0</td>
      <td>18.6</td>
      <td>82</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>397</th>
      <td>31.0</td>
      <td>4</td>
      <td>119.0</td>
      <td>82.0</td>
      <td>2720.0</td>
      <td>19.4</td>
      <td>82</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>데이터셋 분리 (train, test)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_df = df.sample(frac=<span class="number">0.7</span>, random_state=<span class="number">0</span>)</span><br><span class="line">test_df = df.drop(train_df.index)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(train_df)</span><br></pre></td></tr></table></figure>




<pre><code>274



데이터 EDA를 통해 데이터의 분포 및 통계치를 확인합니다</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.pairplot(train_df[[<span class="string">'MPG'</span>,<span class="string">'Cylinders'</span>,<span class="string">'Displacement'</span>,<span class="string">'Weight'</span>]], diag_kind=<span class="string">'kde'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<img width="746" alt="output_19_0" src="https://user-images.githubusercontent.com/59719711/90860062-50634680-e3c4-11ea-8688-798c5f6557e7.png">





<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_stats = train_df.describe()</span><br><span class="line"><span class="comment"># train_stats.pop("MPG")</span></span><br><span class="line">train_stats = train_stats.T <span class="comment">#transpose</span></span><br><span class="line">train_stats</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MPG</th>
      <td>274.0</td>
      <td>23.323358</td>
      <td>7.643458</td>
      <td>10.0</td>
      <td>17.0</td>
      <td>22.0</td>
      <td>29.000</td>
      <td>46.6</td>
    </tr>
    <tr>
      <th>Cylinders</th>
      <td>274.0</td>
      <td>5.467153</td>
      <td>1.690530</td>
      <td>3.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>8.000</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>Displacement</th>
      <td>274.0</td>
      <td>193.846715</td>
      <td>102.402201</td>
      <td>68.0</td>
      <td>105.0</td>
      <td>151.0</td>
      <td>260.000</td>
      <td>455.0</td>
    </tr>
    <tr>
      <th>Horsepwer</th>
      <td>274.0</td>
      <td>104.135036</td>
      <td>37.281034</td>
      <td>46.0</td>
      <td>76.0</td>
      <td>93.0</td>
      <td>128.000</td>
      <td>225.0</td>
    </tr>
    <tr>
      <th>Weight</th>
      <td>274.0</td>
      <td>2976.879562</td>
      <td>829.860536</td>
      <td>1649.0</td>
      <td>2250.5</td>
      <td>2822.5</td>
      <td>3573.000</td>
      <td>4997.0</td>
    </tr>
    <tr>
      <th>Acceleration</th>
      <td>274.0</td>
      <td>15.590876</td>
      <td>2.714719</td>
      <td>8.0</td>
      <td>14.0</td>
      <td>15.5</td>
      <td>17.275</td>
      <td>24.8</td>
    </tr>
    <tr>
      <th>Model_year</th>
      <td>274.0</td>
      <td>75.934307</td>
      <td>3.685839</td>
      <td>70.0</td>
      <td>73.0</td>
      <td>76.0</td>
      <td>79.000</td>
      <td>82.0</td>
    </tr>
    <tr>
      <th>USA</th>
      <td>274.0</td>
      <td>0.635036</td>
      <td>0.482301</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.000</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Europe</th>
      <td>274.0</td>
      <td>0.335766</td>
      <td>0.748893</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>Japan</th>
      <td>274.0</td>
      <td>0.591241</td>
      <td>1.195564</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>이번에는 train, test 분리가 아니라 feature와 label를 분리시켜 줍니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_labels = train_df[<span class="string">'MPG'</span>]</span><br><span class="line">test_labels = test_df[<span class="string">'MPG'</span>]</span><br></pre></td></tr></table></figure>

<h6 id="데이터-정규화"><a href="#데이터-정규화" class="headerlink" title="데이터 정규화"></a>데이터 정규화</h6><pre><code>feature의 크기와 범위가 다르면 정규화(normalization)를 하는 것이 권장됩니다. 정규화를 하지 않아도 모델링이 가능하지만 훈련시키기 어렵고 입력 단위에 의존적인 모델이 만들어지게 됩니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 데이터 정규화</span></span><br><span class="line"><span class="comment"># from sklearn.preprocessing import StandardScaler</span></span><br><span class="line"><span class="comment"># scaler = StandardScaler()</span></span><br><span class="line"><span class="comment"># train_df = scaler.fit_transform(train_df)</span></span><br><span class="line"><span class="comment"># test_df = scaler.fit_transform(test_df)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x - train_stats[<span class="string">'mean'</span>]) / train_stats[<span class="string">'std'</span>]</span><br><span class="line">normed_train_data = norm(train_df)</span><br><span class="line">normed_test_data = norm(test_df)</span><br></pre></td></tr></table></figure>

<h6 id="모델링"><a href="#모델링" class="headerlink" title="모델링"></a>모델링</h6><pre><code>모델을 구성해 보죠. 여기에서는 두 개의 완전 연결(densely connected) 은닉층으로 Sequential 모델을 만들겠습니다. 출력 층은 하나의 연속적인 값을 반환합니다. 나중에 두 번째 모델을 만들기 쉽도록 build_model 함수로 모델 구성 단계를 감싸겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델링</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = keras.Sequential([</span><br><span class="line">        layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>, input_shape=[len(train_df.keys())]),</span><br><span class="line">        layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        layers.Dense(<span class="number">1</span>)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    optimizer = tf.keras.optimizers.RMSprop(<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    model.compile(loss=<span class="string">'mse'</span>,</span><br><span class="line">                optimizer=optimizer,</span><br><span class="line">                metrics=[<span class="string">'mae'</span>, <span class="string">'mse'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = build_model()</span><br></pre></td></tr></table></figure>

<h6 id="모델-확인"><a href="#모델-확인" class="headerlink" title="모델 확인"></a>모델 확인</h6><pre><code>.summary() 메서드를 사용하여 모델의 간단한 정보를 출력해줍니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(model.summary())</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_15&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_45 (Dense)             (None, 128)               1408      
_________________________________________________________________
dense_46 (Dense)             (None, 64)                8256      
_________________________________________________________________
dense_47 (Dense)             (None, 1)                 65        
=================================================================
Total params: 9,729
Trainable params: 9,729
Non-trainable params: 0
_________________________________________________________________
None


모델을 한번 실행해 보죠. training 세트에서 10개의 샘플을 하나의 배치로 만들어 model_predict 메서드를 호출해 보겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example_batch = normed_train_data[:<span class="number">10</span>]</span><br><span class="line">example_result = model.predict(example_batch)</span><br><span class="line">example_result</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:5 out of the last 15 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7faa9a8f0200&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.





array([[-0.03285253],
       [-0.01362434],
       [-0.48285854],
       [ 0.01581845],
       [ 0.08219826],
       [ 0.08362657],
       [ 0.15519306],
       [ 0.28581452],
       [ 0.07680693],
       [ 0.01200353]], dtype=float32)</code></pre><h6 id="모델-훈련"><a href="#모델-훈련" class="headerlink" title="모델 훈련"></a>모델 훈련</h6><pre><code>에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrintDot</span><span class="params">(keras.callbacks.Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>: print(<span class="string">''</span>)</span><br><span class="line">        print(<span class="string">'.'</span>, end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">history = model.fit(</span><br><span class="line">  normed_train_data, train_labels,</span><br><span class="line">  epochs=EPOCHS, validation_split = <span class="number">0.2</span>, verbose=<span class="number">0</span>,</span><br><span class="line">  callbacks=[PrintDot()])</span><br></pre></td></tr></table></figure>


<pre><code>....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................

acc : 훈련 정확도
loss : 훈련 손실값
val_acc : 검증 정확도
val_loss : 검증 손실값</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hist = pd.DataFrame(history.history)</span><br><span class="line">hist[<span class="string">'epoch'</span>] = history.epoch</span><br><span class="line">hist.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>mae</th>
      <th>mse</th>
      <th>accuracy</th>
      <th>val_loss</th>
      <th>val_mae</th>
      <th>val_mse</th>
      <th>val_accuracy</th>
      <th>epoch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>0.102436</td>
      <td>0.234032</td>
      <td>0.102436</td>
      <td>0.0</td>
      <td>0.165683</td>
      <td>0.324213</td>
      <td>0.165683</td>
      <td>0.0</td>
      <td>995</td>
    </tr>
    <tr>
      <th>996</th>
      <td>0.124358</td>
      <td>0.292103</td>
      <td>0.124358</td>
      <td>0.0</td>
      <td>0.263786</td>
      <td>0.404004</td>
      <td>0.263786</td>
      <td>0.0</td>
      <td>996</td>
    </tr>
    <tr>
      <th>997</th>
      <td>0.130789</td>
      <td>0.295300</td>
      <td>0.130789</td>
      <td>0.0</td>
      <td>0.212862</td>
      <td>0.362374</td>
      <td>0.212862</td>
      <td>0.0</td>
      <td>997</td>
    </tr>
    <tr>
      <th>998</th>
      <td>0.116644</td>
      <td>0.275093</td>
      <td>0.116644</td>
      <td>0.0</td>
      <td>0.054454</td>
      <td>0.196261</td>
      <td>0.054454</td>
      <td>0.0</td>
      <td>998</td>
    </tr>
    <tr>
      <th>999</th>
      <td>0.106241</td>
      <td>0.280440</td>
      <td>0.106241</td>
      <td>0.0</td>
      <td>0.121306</td>
      <td>0.281089</td>
      <td>0.121306</td>
      <td>0.0</td>
      <td>999</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_history</span><span class="params">(history)</span>:</span></span><br><span class="line">       </span><br><span class="line">    hist = pd.DataFrame(history.history)</span><br><span class="line">    hist[<span class="string">'epoch'</span>] = history.epoch</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot(hist[<span class="string">'epoch'</span>], hist[<span class="string">'mae'</span>], label=<span class="string">'Train Error'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Mean Abs Error [MPG]'</span>)</span><br><span class="line">    plt.plot(hist[<span class="string">'epoch'</span>], hist[<span class="string">'val_mae'</span>],</span><br><span class="line">             label = <span class="string">'Val Error'</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>,<span class="number">5</span>])</span><br><span class="line">    plt.legend()</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Mean Square Error [$MPG^2$]'</span>)</span><br><span class="line">    plt.plot(hist[<span class="string">'epoch'</span>], hist[<span class="string">'mse'</span>],</span><br><span class="line">             label=<span class="string">'Train Error'</span>)</span><br><span class="line">    plt.plot(hist[<span class="string">'epoch'</span>], hist[<span class="string">'val_mse'</span>],</span><br><span class="line">             label = <span class="string">'Val Error'</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>,<span class="number">20</span>])</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_history(history)</span><br></pre></td></tr></table></figure>


<img width="512" alt="output_37_0" src="https://user-images.githubusercontent.com/59719711/90860211-94eee200-e3c4-11ea-8673-4330f5325b81.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = build_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># patience 매개변수는 성능 향상을 체크할 에포크 횟수입니다</span></span><br><span class="line">early_stop = keras.callbacks.EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,</span><br><span class="line">                    validation_split = <span class="number">0.2</span>, verbose=<span class="number">0</span>, callbacks=[early_stop, PrintDot()])</span><br><span class="line"></span><br><span class="line">plot_history(history)</span><br></pre></td></tr></table></figure>


<pre><code>......................................................................................</code></pre><img width="512" alt="output_38_1" src="https://user-images.githubusercontent.com/59719711/90860253-a33cfe00-e3c4-11ea-8a43-e8074c908d64.png">


<h6 id="모델-검증"><a href="#모델-검증" class="headerlink" title="모델 검증"></a>모델 검증</h6><pre><code>테스트 세트의 모델 성능을 확인</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"테스트 세트의 평균 절대 오차: &#123;:5.2f&#125; MPG"</span>.format(mae))</span><br></pre></td></tr></table></figure>

<pre><code>4/4 - 0s - loss: 0.4875 - mae: 0.5579 - mse: 0.4875
테스트 세트의 평균 절대 오차:  0.56 MPG</code></pre><h6 id="예측"><a href="#예측" class="headerlink" title="예측"></a>예측</h6><pre><code>테스트 세트에 있는 샘플을 이용해 MPG 값 예측</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_predictions = model.predict(normed_test_data).flatten()</span><br><span class="line"></span><br><span class="line">plt.scatter(test_labels, test_predictions)</span><br><span class="line">plt.xlabel(<span class="string">'True Values [MPG]'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Predictions [MPG]'</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.axis(<span class="string">'square'</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>,plt.xlim()[<span class="number">1</span>]])</span><br><span class="line">plt.ylim([<span class="number">0</span>,plt.ylim()[<span class="number">1</span>]])</span><br><span class="line">_ = plt.plot([<span class="number">-100</span>, <span class="number">100</span>], [<span class="number">-100</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure>


<img width="260" alt="output_42_0" src="https://user-images.githubusercontent.com/59719711/90860297-b780fb00-e3c4-11ea-8c8a-709eb6725538.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">error = test_predictions - test_labels</span><br><span class="line">plt.hist(error, bins = <span class="number">25</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Prediction Error [MPG]"</span>)</span><br><span class="line">_ = plt.ylabel(<span class="string">"Count"</span>)</span><br></pre></td></tr></table></figure>


<img width="386" alt="output_43_0" src="https://user-images.githubusercontent.com/59719711/90860330-c5368080-e3c4-11ea-9a6c-79c1fd8fd89c.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-12T12:06:35.000Z" title="2020-08-12T12:06:35.000Z">2020-08-12</time><span class="level-item">4 minutes read (About 530 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/12/%E1%84%80%E1%85%AE%E1%84%80%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A9%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B5%E1%86%BC-API-%E1%84%8F%E1%85%B5-%E1%84%87%E1%85%A1%E1%86%AF%E1%84%80%E1%85%B3%E1%86%B8-%E1%84%87%E1%85%A1%E1%86%AE%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-How-to-be-issued-the-Geocoding-API-key-from-Google/">구글 지오코딩 API 키 발급 받는 방법 (How to be issued the Geocoding API key from Google)</a></h1><div class="content"><pre><code>안녕하세요. 오늘은 구글 맵 위, 위치에 마커를 찍어 지도 상 위치를 한눈에 쉽게 알아보기 위한 GPS 좌표에 대한 부분을 알아보려고 합니다. 일반적으로 쓰이는 주소(서울특별시 종로구 ....)와 GPS 좌표를 서로 변환하는 기능을 쉽게 구현할 수 있도록 구글에서 Geocoding API를 제공하고 있습니다. 

Geocoding API 사용 설정과 API 키 발급 과정에 대해서 설명하겠습니다. 과정은 조금 복잡할 수도 있기지만 쉽게 따라 하실 수 있도록 자세히 설명해보겠습니다.</code></pre><h6 id="1-구글-클라우드-콘솔-사이트에-방문"><a href="#1-구글-클라우드-콘솔-사이트에-방문" class="headerlink" title="1. 구글 클라우드 콘솔 사이트에 방문"></a>1. 구글 클라우드 콘솔 사이트에 방문</h6><pre><code>아래 링크를 클릭해 구글 지도 플랫폼 사이트로 접속해주세요.

https://cloud.google.com/maps-platform/

구글 지도 플랫폼 사이트에서 “시작하기” 혹은 “콘솔” 버튼을 눌러 계속 진행해주세요.</code></pre><h6 id="2-새-프로젝트를-만들기"><a href="#2-새-프로젝트를-만들기" class="headerlink" title="2. 새 프로젝트를 만들기"></a>2. 새 프로젝트를 만들기</h6><pre><code>프로젝트 선택 -&gt; 새 프로젝트 버튼을 클릭해주세요.</code></pre><img width="821" alt="1번" src="https://user-images.githubusercontent.com/59719711/90012853-208ab380-dcdf-11ea-854c-3ffe24075d9a.png">

<h6 id="3-API-사용-설정하기"><a href="#3-API-사용-설정하기" class="headerlink" title="3. API 사용 설정하기"></a>3. API 사용 설정하기</h6><pre><code>프로젝트를 만든 후 이제 사용할 API를 추가해야 합니다.

구글 클라우드 플랫폼의 API 및 서비스 -&gt; 라이브러리 메뉴로 이동해주세요.</code></pre><img width="818" alt="3번" src="https://user-images.githubusercontent.com/59719711/90012912-3e581880-dcdf-11ea-8a99-4b0074cf833a.png">

<pre><code>검색창에 “Geocoding API”를 입력해주세요.

클릭!!!!!</code></pre><img width="824" alt="4번" src="https://user-images.githubusercontent.com/59719711/90012963-4fa12500-dcdf-11ea-8901-07c4aa5e08d1.png">

<pre><code>Geocoding API의 “사용 설정” 버튼을 클릭해주세요.</code></pre><img width="816" alt="5번" src="https://user-images.githubusercontent.com/59719711/90012987-5a5bba00-dcdf-11ea-8d42-e748d9d8d6dc.png">

<h6 id="4-사용자-인증-정보-만들기"><a href="#4-사용자-인증-정보-만들기" class="headerlink" title="4. 사용자 인증 정보 만들기"></a>4. 사용자 인증 정보 만들기</h6><pre><code>이제 자신의 API 키를 발급받을 수 있습니다.

구글 클라우드 플랫폼의 API 및 서비스 -&gt; 사용자 인증 정보 메뉴로 이동해주세요.</code></pre><img width="821" alt="6번" src="https://user-images.githubusercontent.com/59719711/90013001-647db880-dcdf-11ea-9bb0-d9c45b430473.png">

<pre><code>사용자 인증 정보 만들기 -&gt; API 키 선택</code></pre><p>6번</p>
<h6 id="5-API-키-발급-완료"><a href="#5-API-키-발급-완료" class="headerlink" title="5. API 키 발급 완료"></a>5. API 키 발급 완료</h6><pre><code>이제 API 키를 복사해 사용할 수 있습니다.</code></pre><ul>
<li>키 제한의 경우 소중한 자신의 API KEY를 아무나 함부로 쓸 수 없도록 하는 설정입니다. 설정을 안해도 KEY는 설정이 가능하지만 제한을 거는 것을 추천드립니다.</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-07-24T05:54:01.000Z" title="2020-07-24T05:54:01.000Z">2020-07-24</time><span class="level-item">a minute read (About 151 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/24/MySQL-Ubuntu%EC%97%90%EC%84%9C-MySQL-%EC%99%84%EC%A0%84-%EC%82%AD%EC%A0%9C%ED%95%98%EA%B8%B0/">[MySQL] Ubuntu에서 MySQL 완전 삭제하기</a></h1><div class="content"><pre><code>MySQL Workbench를 활용하여 데이터베이스 환경을 설정하고 작업 과정에서 시스템 계정을 삭제하는(?) 아주 큰 문제가 생겨서 사용자 생성 등 다양한 시도를 해봤지만 뭔가 꼬인것 같은 느낌이 들었다.


Mysql을 삭제하고 재설치가 필요할듯 하여 재설치 방법을 포스팅 하고자 한다.


아래의 명령어를 참고하자.</code></pre><h6 id="Mysql"><a href="#Mysql" class="headerlink" title="[Mysql]"></a>[Mysql]</h6><pre><code>sudo apt-get purge mysql-server
sudo apt-get purge mysql-common


sudo rm -rf /var/log/mysql
sudo rm -rf /var/log/mysql.*
sudo rm -rf /var/lib/mysql
sudo rm -rf /etc/mysql</code></pre></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-07-23T10:24:00.000Z" title="2020-07-23T10:24:00.000Z">2020-07-23</time><span class="level-item">4 minutes read (About 604 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/23/MySQL-workbench%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-ERD%E1%84%90%E1%85%AE%E1%86%AF-%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5-Database-Modeling/">[mysql] workbench에서 ERD툴 사용하기 (Database Modeling)</a></h1><div class="content"><h6 id="데이터베이스-모델링-Database-Modeling"><a href="#데이터베이스-모델링-Database-Modeling" class="headerlink" title="데이터베이스 모델링 (Database Modeling)"></a>데이터베이스 모델링 (Database Modeling)</h6><p>데이터베이스 모델링(또는 데이터 모델링)이란 현실 세계에서 사용되는 작업이나 사물들을 DBMS의 데이터베이스 개체로 옮기기 위한 과정이라고 할 수 있습니다.</p>
<p>데이터베이스 모델링은 모델링을 하는 사람이 어떤 사람이냐에 따라서 각기 다른 결과가 나올 수밖에 없고 ‘많은 실무 경험과 지식을 가진 사람이 더 좋은 모델링을 한다’ 라고 합니다.</p>
<p>3가지의 모델링 방식이 있다.<br>    - 개념적 모델링<br>    - 논리적 모델링<br>    - 물리적 모델링</p>
<p>개념적 모델링은 주로 업무 분석 단계에서 진행되며 논리적 모델링은 업무 분석의 후반부와 시스템 설계를 하는 부분에 걸쳐서 진행된다고 할 수 있습니다. 마지막으로 물리적 모델링은 시스템 설계 단계의 후반부에서 주로 진행됩니다.</p>
<p>그러나 모두 절대적인 것은 아니며 사람에 따라 조금씩 차이를 보이기도 합니다.</p>
<p>워크벤치는 이 모델링 툴을 제공해주는데 그것이 바로 ERD툴입니다.</p>
<h6 id="ERD란"><a href="#ERD란" class="headerlink" title="ERD란?"></a>ERD란?</h6><p>Entity Relationship Diagram의 약자로, 개체관계도라고 부릅니다</p>
<h6 id="장점"><a href="#장점" class="headerlink" title="장점"></a>장점</h6><ol>
<li>만들고자 하는 바를 더 명확하게 알 수 있다.</li>
<li>이해하고 소통하기에 편리하다.</li>
<li>RDBMS 데이터 설계가 쉬워진다.</li>
</ol>
<h6 id="데이터베이스"><a href="#데이터베이스" class="headerlink" title="데이터베이스"></a>데이터베이스</h6><p>모델링에서는 데이터베이스를 Schema라고 부릅니다.</p>
<p>실습을 해보기 전에 ERD에 대해 소개를 하였습니다. 실습은 워크벤치를 사용하였고. MySQL workbench는 약 10년 정도 상업용으로 개발되어 판매되다가, MySQL에서 workbench를 인수하여 오픈소스로 풀었다고 합니다.</p>
<h6 id="실습과정"><a href="#실습과정" class="headerlink" title="실습과정"></a>실습과정</h6><ol>
<li>FILE &gt; New Model 클릭<img width="693" alt="스크린샷 2020-07-23 오후 7 34 11" src="https://user-images.githubusercontent.com/59719711/88279718-80fd8500-cd1f-11ea-8ba7-7a070f7fba37.png">


</li>
</ol>
<ol start="2">
<li>Model의 이름 설정(안해도 됨)<img width="1578" alt="스크린샷 2020-07-23 오후 7 35 06" src="https://user-images.githubusercontent.com/59719711/88279754-8ce94700-cd1f-11ea-9e9f-e443e5efbc2a.png">


</li>
</ol>
<ol start="3">
<li>테이블 생성 &gt; 흰 바탕에 클릭 &gt; 컬럼 생성</li>
<li>PK와 FK 설정 (설정 시 place a relationship using existing columns 선택 &gt; 자식테이블의 해당 컬럼 열을 먼저 클릭 후 부모테이블의 컬럼 열 클릭)</li>
<li>관계 생성<img width="1567" alt="스크린샷 2020-07-23 오후 7 36 52" src="https://user-images.githubusercontent.com/59719711/88279797-983c7280-cd1f-11ea-9c2a-17459fb75dfc.png">


</li>
</ol>
<ol start="6">
<li>저장 &gt; database의 forward engineer 선택 &gt; continue <img width="1147" alt="스크린샷 2020-07-23 오후 7 37 55" src="https://user-images.githubusercontent.com/59719711/88279865-b86c3180-cd1f-11ea-9148-a8040d01949e.png">


</li>
</ol>
<p>최종적으로 모델링이 되어 있는 db와 table 생성(refresh all)</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-07-22T13:58:24.000Z" title="2020-07-22T13:58:24.000Z">2020-07-22</time><span class="level-item">8 minutes read (About 1203 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/22/MySQL-Storage-Engine-InnoDB-vs-MyISAM/">[MySQL] Storage Engine (InnoDB vs MyISAM)</a></h1><div class="content"><p>생각없이 Engine을 InnoDB만 사용했지 왜 이것을 사용해야 하는지 고민해본 적이 없었다.</p>
<p>하지만 많은 양의 데이터를 적재하면 데이터의 수가 매우 많거나 column의 갯수가 많아지면 공간부족 현상이 나타나게 되고, 그로 인해 엔진에 대해 고민하기 시작하였다.</p>
<p>대략 500개에 달하는 칼럼이 필요한 상황이였다. 그래서 어떻게 테이블에 적재해야 효율적인지 고민이 필요했다.</p>
<p>또한 InnoDB 테이블에 많은 칼럼을 추가하니 Row size too large. 라는 오류가 발생해서 Engine을 변경하는 방법을 생각하게 되었다.</p>
<p>우선 각 Stroage Engine에 대해 알아보았다.</p>
<p>Mysql Storage Engine은 물리적 저장장치에서 데이터를 어떤 식으로 구성하고 읽어올지를 결정하는 역할을 한다.</p>
<p>기본적으로 8가지의 스토리지 엔진이 탑재되어 있으며 CREATE TABLE문을 사용하여 테이블을 생성할 때 엔진 이름을 추가함으로써 간단하게 설정할 수 있다.</p>
<p>그 중 가장 많이 쓰이는 엔진은 InnoDB, MyISAM, Archive 3가지이다.</p>
<h6 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h6><p>테이블 생성 시, 따로 스토리지 엔진을 명시하지 않으면 default로 설정되는 스토리지 엔진이다.</p>
<p>InnoDB는 트랜잭션(tranjection)을 지원하고, 커밋(commit)과 롤백(roll-back) 그리고 데이터 복구 기능을 제공하므로 데이터를 효과적으로 관리할 수 있다.</p>
<p>InnoDB는 기본적으로 row-level locking을 제공하며, 또한 데이터를 clustered index에 저장하여 PK기반의 query의 비용을 줄인다.</p>
<p>또한, PK 제약을 제공하여 데이터 무결성을 보장한다.</p>
<p>여기서 clustered index에 저장한다는 것은 데이터를 PK순서에 맞게 저장한다는 뜻이므로 order by 등 쿼리에 유리할 수 있다. 또한 row-level locking을 제공한다는 뜻은 테이블에 CRUD할 때, 로우별로 락을 잡기 때문에 multi-thread에 보다 효율적이라는 말이다.</p>
<p>물론 장점만 있는 것은 아니다. InnoDB는 더욱 많은 메모리와 디스크를 사용한다. 또한 데이터가 깨졌을 때  단순 파일 백업/복구만으로 처리가 가능한 MyISAM과 달리 InnoDB의 경우 복구 방법이 어렵다. MyISAM이나 Memory 방식이 지원하지 않는 FK의 경우도 테이블 간 데이터 체크로 인한 lock, 특히 dead lock이 발생할 가능성이 있다.</p>
<h6 id="InnoDB의-최대-행-저장공간"><a href="#InnoDB의-최대-행-저장공간" class="headerlink" title="InnoDB의 최대 행 저장공간"></a>InnoDB의 최대 행 저장공간</h6><p>Mysql 테이블의 행 크기는 스토리지 엔진에 제약이 없다면 기본적으로 최대 65535바이트이다. 하지만 스토리지 엔진에 따라 제약이 추가되어 행 크기는 줄어들 수 있다.</p>
<p>BLOB, TEXT 컬럼의 내용은 행의 남은 부분이 아닌 별도의 공간에 저장되기 때문에 각각 9<del>12바이트만 영향을 준다. (포인트 저장 공간이 9</del>12바이트) 여기서 중요한 것은 위의 내용은 스토리지 엔진에 상관없이 기본적으로 이렇다는 것이다.</p>
<h6 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h6><p>트랜잭션(tranjection)을 지원하지 않고 table-level locking을 제공한다.</p>
<p>따라서 1개의 ROW을 READ하더라도 테이블 전체에 락을 잡기 때문에 multi-thread 환경에서 성능이 저하될 수 있다.</p>
<p>하지만 InnoDB에 비해 기능적으로 단순하므로 대부분의 작업은 InnoDB보다 속도면에서 우월하다.</p>
<p>단순한 조회의 경우 MyISAM이 InnoDB보다 빠르지만, Order By등 정렬들의 구문이 들어간다면 InnoDB보다 느리다.</p>
<p>왜냐하면 InnoDB는 클러스터링 인덱스에 저장하기 때문에 PK에 따라 데이터 파일이 정렬되어 있지만, MyISAM은 그렇지 않기 때문이다.</p>
<p>Full text searching을 지원한다.</p>
<p>MyISAM 엔진의 경우 최대 행 크기가 기본 MySQL 제약을 따르므로 최대 행 크기는 65535바이트가 될 것이다.</p>
<h6 id="Archive"><a href="#Archive" class="headerlink" title="Archive"></a>Archive</h6><p>로그 수집에 적합한 엔진이다. 데이터가 메모리상에서 압축되고 압축된 상태로 디스크에 저장되기 때문에 row-level locking이 가능하다.</p>
<p>다만, 한번 INSERT된 데이터는 UPDATE, DELETE를 사용할 수 없으며 인덱스를 지원하지 않는다. 따라서 거의 가공하지 않을 데이터에 대해서 관리하는데에 효율적일 수 있고, 테이블 파티셔닝도 지원한다. 다만 트랜잭션은 지원하지 않는다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-07-19T09:25:05.000Z" title="2020-07-19T09:25:05.000Z">2020-07-19</time><span class="level-item">2 minutes read (About 349 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/07/19/Python-SQLAlchemy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/">[Python] SQLAlchemy 사용하기</a></h1><div class="content"><pre><code>DataFrame을 MySQL에 저장하기 위해 먼저 엔진 커넥터가 필요합니다. 파이썬3에서는 MySQLdb를 지원하지 않기 때문에, pymysql로 불러와야 합니다. 꼭 pymysql이 아니어도 상관없지만, 사용해보면 mysql-connector 보다 빠르다는걸 체감할 수 있습니다. 먼저, 필요한 패키지를 설치해줍니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python3</span></span><br><span class="line">pip install pymysql</span><br><span class="line">pip install sqlalchemy</span><br></pre></td></tr></table></figure>

<h6 id="SQLAlchemy-Pymysql-MySQLdb"><a href="#SQLAlchemy-Pymysql-MySQLdb" class="headerlink" title="SQLAlchemy, Pymysql, MySQLdb"></a>SQLAlchemy, Pymysql, MySQLdb</h6><pre><code>install_as_MySQLdb() 함수를 통해 MySQLdb와 호환 가능합니다. 이제 sqlalchemy를 통해 DB에 연결할 수 있습니다. 주소에서 root, password는 DB에 맞게 변경해야 합니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="comment"># MySQL Connector using pymysql</span></span><br><span class="line">pymysql.install_as_MySQLdb()</span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"></span><br><span class="line">engine = create_engine(<span class="string">"mysql://root:"</span>+<span class="string">"password"</span>+<span class="string">"@public IP/db_name"</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">conn = engine.connect()</span><br></pre></td></tr></table></figure>

<h6 id="MySQL에-저장하기"><a href="#MySQL에-저장하기" class="headerlink" title="MySQL에 저장하기"></a>MySQL에 저장하기</h6><pre><code>이제 DataFrame을 MySQL에 테이블 형태로 저장할 차례입니다. 아래와 같이 pandas의 to_sql() 함수를 사용하여 저장하면 됩니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_sql(name=table_name, con=engine, if_exists=<span class="string">'append'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>자주 사용할 수 있으니 함수로 따로 설정해주면 원할 때마다 쉽게 사용할 수 있겠죠? 그리고 if_exists의 경우 만약 동일 테이블의 이름이 존재한다면 어떻게 처리하겠느냐의 파라미터를 주는 것인데 append 외에도 replace, delete 등 다양한 것이 있습니다.</code></pre></div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://user-images.githubusercontent.com/59719711/85970198-ab9c3c80-ba04-11ea-8990-4bdec6914e3c.jpeg" alt="wglee87"></figure><p class="title is-size-4 is-block line-height-inherit">wglee87</p><p class="is-size-6 is-block">Data has a better idea</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hwasung, KR</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">36</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">3</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wglee87" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wglee87"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://Instagram.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-10-13T13:12:04.000Z">2020-10-13</time></p><p class="title is-6"><a class="link-muted" href="/2020/10/13/Introduce-to-Activation-Function/">Introduce to Activation Function</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-26T10:15:22.000Z">2020-08-26</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/26/Hyper-Parameter-Tuner/">Hyper-Parameter(Tuner)</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-26T10:14:02.000Z">2020-08-26</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/26/Model-sava-and-load-in-tensorflow/">Model sava and load in tensorflow</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-25T11:21:43.000Z">2020-08-25</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/25/%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9-Overfitting-%EA%B3%BC-%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9-Underfitting/">과대적합(Overfitting)과 과소적합(Underfitting)</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-21T06:29:24.000Z">2020-08-21</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/21/Tensorflow%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80-%EB%AA%A8%EB%8D%B8%EB%A7%81/">Tensorflow를 활용한 회귀 모델링</a></p><p class="is-uppercase"></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/fastcampus/"><span class="tag">fastcampus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/" alt="Geony&#039;s Tech Blog" height="28"></a><p class="size-small"><span>&copy; 2020 WGLee87</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wglee87.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>