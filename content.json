{"pages":[],"posts":[{"title":"200418-공부내용","text":"파이썬 프로그래머스 코딩테스트 문제풀이 k리그1 데이터로 pandas 데이터 로드, 전처리 작업 연습 가상서버 연결 실패 (연결이 안됨) 수학 오전에 강사님의 강의 도저히 이해가 되질 않아서 처음부터 다시 차근차근 공부시작 수열과 스칼라, 벡터, 행렬 등 공부 (이정도는 이해) 내일도 수학 공부 예정 (큰일남) git과 블로그에 오늘의 공부내용 업로드 title: 200418-공부내용date: 2020-04-18 21:10:47tags:","link":"/2020/04/18/200418-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"200420-공부내용","text":"파이썬(Python) 학습 프로그래머스 코딩테스트 연습 startswith() startswith란 앞의 변수와 value값이 뒤의 변수의 value중 같은 단어로 시작하는 것이 있으면 True or False를 반환하는 함수 k리그1 데이터 전처리작업 선수 별 퍼포먼스 데이터의 누적합 연습 reduce, 반복문을 통해 코딩함 (실패) numpy 나 pandas dataframe 의 function중 cumsum()이라는 누적합 기능이 있음 cumsum(). 좋은 것을 또 하나 찾았고, 공부했다. def 함수를 만들어 apply 기능을 통해 적용시키는 방법은 조금 더 연습이 필요할듯 하다. 가상환경에서의 주피터 실행 계속적으로 실패중 포트 등록까지 하고 열었는데 열리지 않음 계속적으로. 강사님의 도움이 절실함 수학(math) 학습 행렬의 성질 행렬의 크기 및 부피를 나타내는 3가지 성질(놈(norm), 대각합(trace), 행렬식(determinants) 놈(norm)은 항상 0보다 같거나 크다. 벡터의 놈(norm)의 성질은 벡터의 놈의 제곱이 벡터의 제곱합과 같다. 대각합(trace)은 정방핸렬에 대해서만 정의 됨. 양의 정부호와 양의 준정부호 대칭행렬이란 A라는 행렬의 전치행렬과 원래의 A가 같은 행렬. 좌표와 변환 좌표벡터의 길이는 놈(norm)으로 구할 수 있음. 스칼라와 좌표벡터를 곱하면 방향은 그대로, 실수(스칼라)의 크기 만큼 길이가 길어짐. 여러개의 벡터를 스칼라값을 곱한 후 더한 것 - 선형조합. 좌표에서 벡터의 차는 좌표의 a와 b을 빼면 됨 (a-b) 유클리드 거리 sin@은 @가 0에 가까워질수록 0에 가까워지고, @가 90도에 가까워질수록 1에 가까워진다. cos@은 @가 0에 가까워질수록 1에 가까워지고, @가 90도에 가까워질수록 0에 가까워진다. a와 90도 만나는 b의 좌표를 직교라고 한다. ex) 1,0 과 0,1 코사인 유사도 두 벡터의 방향이 비슷할수록 벡터가 비슷하다고 간주 &gt; 코사인 유사도가 1이면 가장 유사하다’라고 봄. 고윳값 [정리] 중복된 고윳값을 각각 별개로 새ㅐㅇ각하고 복소수인 고윳값도 고려한다면 n차원 정방행렬의 고윳값은 항상 n개이다. 이상. (전혀 이해가 되지 않는다. 큰일이다)title: 200420-공부내용date: 2020-04-20 18:58:01tags:","link":"/2020/04/20/200420-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"Data Analytic on Football","text":"[숫자는 거짓말하지 않는다: 왜 축구 클럽들이 그렇게 애널리틱스를 중요하게 생각할까(The numbers don’t lie: why football clubs place such importance on analytics)] 웨스트 햄의 로리 캠벨은 빅 클럽에서 핵심 결정에 정보를 제공하는 떠오르는 축구 애널리스트들 중의 하나이다. 선수들이 떠난 한참 후의 웨스트 햄 유나이티드의 채드웰 헬스 훈련장의 조용한 구석에서 로리 캠벨은 컴퓨터 화면을 응시하고 있다. 이것이 보비 무어와 제프 허스트 경의 두 번째 집이었던 이래로 그 주변의 거주지들은 오직 피상적으로 변했을 뿐이나, 필드 밖의 준비의 복잡성은 완전히 바뀌고 있다. 캠벨은 웨스트 햄의 기술 스카우트이자 분석가이다. 옥스포드 졸업생이자, 알라스테어 캠벨의 아들들 중 하나인 그는 성공적인 포커 선수였으며, 약간의 선수 경험과 코칭 배경을 가졌다. 그의 초점은 축구에 관한 무한한 통계적이 데이터들을 이해하는 것이며, 그래서 클럽의 핵심적인 의사 결정자들에게 무엇이 정말 중요한지를 전달하는 것이다. 은골로 칸테는 올시즌의 영입이고 축구 애널리틱스의 승리이다. 더구나, 축구의 가장 효과적인 분석 작업들 중 몇몇과 단순히 구매력을 거의 반영하지 않고 있는 프리미어 리그 테이블 사이의 잠재적인 상관 관계는 분명하다. 레스터 시티와 웨스트 햄은 오늘 만나 경기를 갖지만, 예를 들어 어떻게 그들이, 만체스터 유나이티드가 마루앙 펠라이니, 안더 에레라 그리고 바스티안 슈바인슈타이거에 7천만 파운드를 쏟아부을 때, 은골로 칸테, 디미트리 파예 그리고 리야드 마레즈를 천 6백만 파운드에 발견해냈을까? 그리고 무엇이 토튼햄 핫스퍼가 델리 알리와 에릭 다이어로 이끌었는지 혹은 사우스햄튼을 사디오 마네와 버질 반 다이크로 이끌었을까? 왜 팀들이 전에 없이 적은 크로스를 하고 있을까? 무엇이 레스터의 독특한 특징일까? 왜 펩 과르디올라와 같은 감독들이 먼 거리에서의 슈팅을 장려하지 않을까? 그리고 모든 시즌들 중에서 가장 놀라운, 테이블이 거짓말을 하지 않는다는 것은 정말 사실일까? 캠벨은 웨스트 햄의 기술 스카우트이자 애널리스트이다. 애널리틱스는 최소한 부분적인 답을 제공한다. 비록 캠벨이 분석의 가치가 여전히 의사결정의 기저를 구성하고 있는 경험, 직관, 본질적인 지식과 접촉들을 대체한다기 보다는 보조하는 것이라고 확신하지만 말이다. 그는 “비효율적인 어떤 시장도 기회입니다.”라고 말한다. “축구가 재능을 가치 평가하는 세트나 동의된 방식을 갖고 있지 않고 너무나 임의적이라는 사실은 기회입니다. 통계와 애널리틱스는 차이가 있습니다. 통계는 당신에게 일어난 사건들에 대해서 말해줍니다. 그들은 맥락없이는 아무 것도 의미하지 않습니다. 레스터의 직접적인 스타일은 그들을 뚜렷한 프리미어 리그 아웃라이어로 만들었다. “애널리틱스는 그러한 통계들을 미래의 퍼포먼스를 예측하기 위해 해석하는 것입니다. 당신은 모든 것을 측정할 수 있습니다. 어려운 것은 무엇이 중요한지를 알아내는 것입니다. 한가지 좋은 것은 축구는 꽤 단순하다는 것입니다. 모든 것은 결국에는 어떻게든 골과 연관되어 있습니다, 그것이 우리의 득점 기회를 늘려주거나 아니면 실점을 막아주거나요. 그것은 또한 감독이 팀이 어떻게 플레이하기를 원하는 틀 안에서 통합니다.” 보다 더 많은 통찰들이 사우스햄튼의 훈련시설에서 발견될 수 있다. 가장 놀라운 곳은 클럽의 34세의 스카우팅과 선수선발 이사인 로스 윌슨이 자리잡고 있는 방이다. 그의 바로 앞에는 15개의 일련의 화면 앞에서 젊은 스태프들의 팀이 정보들을 처리하고 있다. 몇몇은 축구 분석의 특정 분야에서의 학위를 보유한 인턴들이다. 윌슨의 오른쪽에는 보다 희끗희끗한 머리를 가진 관계자인 로드 루딕과 같은 사람이 있다. 그는 뉴포트의 필드에서 뛰던 8살의 가레스 베일을 발견한 스카우트이다. 윌슨의 왼쪽에는 “블랙 박스”라는 단어가 미스테리하게 걸려있는 문이 있다. 폴 미첼은 사우스햄튼에서 토튼햄으로 이동해서 선수선발 및 분석 팀장이 되었다. 사우스햄튼은 이 미니-시네마에 쓰이는 맞춤형 소프트웨어를 꾸준히 수정하고 있다. 그 소프트웨어는 몇번의 클릭들 만으로 전세계에 있는 어떤 선수에 대한 것도 볼 수 있다. 다른 클럽들은 비슷한 기술을 개발 중이며, 수학적으로 재능들을 골라내는 사람들 사이에서, 이적 시장이 떠오르고 있다. 아스날은 올 해 벤 뤼글워스를 레스터로부터 빼냈고, 미국 기반의 분석 회사인 statDNA를 2백만 파운드에 사들였다. 마우리시오 포체티노와 함께, 토튼햄은 그들의 선수선발과 분석 팀장인 폴 미첼을 사우스햄튼에서 선발했다. 부상으로 27세에 선수 커리어를 끝냈던 미첼은 이렇게 말한다. “저는 한 번 좋은 경기를 가질 때 다른 80경기에서는 그렇게 좋지 않다는 간단한 이론을 발견했습니다.” 포커에서처럼, 캠벨은 선수 선발을 “당신의 베팅의 경제적인 리스크를 가용한 정보를 가지고 관리하는 것”이라고 부른다. 하지만 다른 점은 그의 산업의 다른 멤버들과의 미팅이 분명히 핵심적이라는 것을 강조한다. 과거의 축구에서의 혁신의 시도가 주저앉은 곳에는 종종 의사소통의 실수나 개인간의 충돌들에 기반한다. 클라이브 우드와드 경은 보통 생각되는 것보다 해리 레드납과 더 나은 관계를 가지고 있다. 하지만 거기에 루퍼트 로, 데이브 바셋, 데니스 와이즈와 사이먼 클리포드를 더하면 당신이 그것이 어디서 잘못되었는지 파악하기 위해서 에르큘 포와드의 추리력까지 필요하지도 않다. 애널리틱스가 차이를 만들어내는 곳에는 주로 문화가 정립된다. “당신이 플레이할 필요가 있는 클럽들 중에는 전통이 깊이 배어있는 클럽들이 있을 것입니다. 하지만 운이 좋게도 내가 있었던 클럽들은 사고방식이 매우 열려있었습니다.” 윌슨이 말한다. 캠벨은 이렇게 더한다: “애널리틱스 세계가 힘겨워하는 부분은 전통적인 축구 세계로의 다리를 놓는 것입니다. 정보를 더 침투시키기 위해서요. 수학적인 측면에서는 완벽히 논리적으로 말이되는 많은 정보들을 주고 수십년간 발전해 온 스포츠가 그것을 하루 아침에 받아들이기를 기대하는 것은 사실 꽤 건방진 것이다. 나는 이것이 가장 큰 도전으로 남아있다고 말하고 싶다. 정보를 전달할 수 있기 위해서는 당신은 역학관계와 당신과 함께 일하는 사람들의 성격들을 이해해야만 한다. 나는 그것이 왜 애널리틱스가 다른 비지니스에서 그랬던 것처럼 축구에 침투하지 못한 이유라고 생각한다.” 아스날 감독 아슨 벵거는 또한 애널리틱스를 수용해오고 있다. 하지만 이것은 변하고 있다. 캠벨은 그가 슬래븐 빌리치와 선수 선발 디렉터인 토니 헨리 아래에서 일할 수 있어서 매우 운이 좋았다고 말한다. 그리고 모두의 역할에 분명함이 있었다. 그들이 원하는 것은 간단히 그들의 결정을 도와줄 정보에 대한 믿을만한 평가였다. 나이 많은 감독들 역시 접근하고 있다. 클라우디오 라니에리는 한 예이다. 아슨 벵거는 올 시즌 공식자리에서 아스날의 “기대 골 값 (xG)”에 대해 언급하면서 충격을 촉발시켰다. 그것은 팀이 통계적으로 얼마나 자주 득점할 수 있는지에 대한 스포츠 베팅과 애널리틱스에서 핵심적인 측정값이다. 보루시아 도르트문트의 코치 토마스 투헬은 xG에 대해 더 배우기 위해 매튜 베넘을 찾아갔다. 베넘은 스포츠 베팅에서 수백만을 벌었고 그 이후로 브렌트포드와 FC밋츌란을 사들였다. 선수를 개인적으로 관찰하기 위해 폭넓게 움직이는 캠벨과 마찬가지로, 베넘은 축구처럼 낮은 득점을 가진 스포츠에서의 어떤 수학 모델의 뚜렷한 변동성을 보완하기 위한 “눈으로 하는 스카우팅”의 중요성을 강조한다. 포커처럼, 랜덤과 통제불가능한 사건들이 판단들을 형성하는데 서두르는 가운데에 종종 간과하는 부분적인 역할을 한다. 캠벨은 이렇게 말한다. “이것은 축구를 흥미롭게 만드는 것입니다만 예측불가능은 항상 심각한 비효율을 동반합니다. 운을 불평하는 프로 포커 선수들은 편협한 것입니다. 운이라는 것은 그들이 사는 것을 가능케하는 것입니다. 만약 내가 정말 나쁜 선수와 포커를 친다면, 그는 100번 중에 40번을 이길 것입니다. 만약 내가 개리 카스파로프와 체스를 둔다면 그가 100번을 이기겠죠. 체스 선수들은 베팅으로 돈을 벌지 않습니다. 왜냐하면 아무도 그들에게 돈을 걸지 않으니까요.” 레스터의 동화같은 시즌은 통계적인 모델들이 틀렸음을 입증해오고 있다. 애널리틱스는 점점 더 많은 의견들로 가득찬 현재 지형 안에서 목소리를 내기 위해 싸우고 있다. 캠벨은 이렇게 말한다. “아슨 벵거는 우리가 수직적에서 수평적인 사회로 움직이고 있다고 말했습니다. 수직적인 것은 꼭대기에 리더가 있고 모두가 따르는 것입니다. 수평적인 것은 정보와 의견들에 폭격을 당하는 리더를 가진 것입니다. 그것이 리더가 어떤 것이 중요하고 어떤 것이 노이즈인지를 구분하는 것이 정말 중요한 부분입니다.” 그러면 처음 질문으로 돌아가 보자. 칸테, 마네 그리고 파예는 궁극적으로 기민한 축구적인 결정들이었다. 하지만 애널리틱스 커뮤니티의 기저에 있는 퍼포먼스 지표들의 승리였다. 분명한 통계적인 증거는 크로스들이 스루 볼 보다 적은 확률의 전술이라는 것이고 먼 거리에서의 슈팅은 더 나은 포지션으로 패스하는 것 보다 적은 골을 생산한다는 것이다. 수학 교수이자 사커매틱스의 저자 데이빗 섬터에 따르면, 레스터와 리그의 다른 팀들과의 충격적인 차이는 어떻게 그들이 상대적으로 길고 직선적인 패스들로 볼을 전방으로 빠르게 움직이는지이다. 그러면 테이블은 절대 거짓말하지 않는다는 클리셰는 거짓일까? 글쎄, 아마도 그것은 진실 전부를 말하지는 않는다. 거의 모든 xG모델은 아스날이 사실 엄청난 기회를 놓쳤고 선두에 있었어야 한다고 말하고 있다. 대부분의 모델은 만약 이번 시즌이 무한한 경기 수를 가졌을 때 레스터가 4위에서 8위 사이에 놓고 있다. 분산과 운은, 38경기 프로그램에서 상당한 양으로 남아있다. 여전히 그 차이들은 좁고, 만약 지난 해가 어떤 것도 새로 증명하는 것이 아니라면, 열심히 일하는 것과 스마트함이 클럽의 은행 계좌의 사이즈 보다 더 중요할 수 있다. 출 처 : http://www.telegraph.co.uk/football/2016/04/16/the-numbers-dont-lie-why-football-clubs-place-such-importance-on/","link":"/2020/05/15/Data-Analytics-on-Football/"},{"title":"DataFrame Functions","text":"데이터 분포 변환대부분의 모델은 변수가 특정 분포를 따른다는 가정을 기반으로 한다. 예를 들어 선형 모델의 경우, 종속변수가 정규분포와 유사할 경우 성능이 높아지는 것으로 알려져 있다. 자주 쓰이는 방법은 Log, Exp, Sqrt 등 함수를 이용해 데이터 분포를 변환하는 것이다.123456789import mathfrom sklearn import preprocessing# 특정 변수에만 함수 적용df['X_log'] = preprocessing.scale(np.log(df['X']+1)) # 로그df['X_sqrt'] = preprocessing.scale(np.sqrt(df['X']+1)) # 제곱근# 데이터 프레임 전체에 함수 적용 (단, 숫자형 변수만 있어야 함)df_log = df.apply(lambda x: np.log(x+1)) 중복된 행 제거위, 아래 행이 모두 같은 성분을 가지는 행이 여러개 있을때, 하나만 사용하기(데이터프레임) 중복된 행이 제거되고 unique한 값만 가져올 수 있다.123df.drop_duplicates()df.drop_duplicated() 는 boolean값으로 반환! 12df['name'] = df['name'].apply(lambda e: e.split()[0])df['email'].str.get(i=0) 데이터프레임이나 시리즈형식에서 문자를 나누고 [0]번째 문자만 가져오기 데이터 프레임 모든 열에 특정 스칼라값 or 특정 컬럼.value 연산1df[컬럼명] *= (스칼라값) / 해당 데이터프레임 컬럼이 와도 됨 1df[컬럼명] = df[컬럼명].div(스칼라값 or 컬럼, axis=0) 12 a=10, b=20, c=3 Operator Description Example + 더하기 a + b 30 - 빼기 a - b -10 * 곱하기 a * b 200 / 나누기 b / a 2.0 % 나머지 b % a 0 ** 제곱 a ** c 1000 // 몫 a // c 3","link":"/2020/06/04/DataFrame-Functions/"},{"title":"200422-공부내용","text":"수학(math) 데이터분석에서의 미,적분 최적화(optimize) git 2번째 강의(branch 강의) 첫번째 협업 연습(fork and merge) title: 200422-공부내용date: 2020-04-22 21:00:53tags:","link":"/2020/04/22/200422-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"[MySQL] Ubuntu에서 MySQL 완전 삭제하기","text":"MySQL Workbench를 활용하여 데이터베이스 환경을 설정하고 작업 과정에서 시스템 계정을 삭제하는(?) 아주 큰 문제가 생겨서 사용자 생성 등 다양한 시도를 해봤지만 뭔가 꼬인것 같은 느낌이 들었다. Mysql을 삭제하고 재설치가 필요할듯 하여 재설치 방법을 포스팅 하고자 한다. 아래의 명령어를 참고하자.[Mysql]sudo apt-get purge mysql-server sudo apt-get purge mysql-common sudo rm -rf /var/log/mysql sudo rm -rf /var/log/mysql.* sudo rm -rf /var/lib/mysql sudo rm -rf /etc/mysql","link":"/2020/07/24/MySQL-Ubuntu%EC%97%90%EC%84%9C-MySQL-%EC%99%84%EC%A0%84-%EC%82%AD%EC%A0%9C%ED%95%98%EA%B8%B0/"},{"title":"Hyper-Parameter(Tuner)","text":"12345import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersimport IPython Keras Tuner를 설치하고 가져옵니다. 12!pip install -q -U keras-tunerimport kerastuner as kt 데이터 세트 다운로드 및 준비 - 이 자습서에서는 Keras Tuner를 사용하여 Fashion MNIST 데이터 세트 에서 의류 이미지를 분류하는 기계 학습 모델에 가장 적합한 하이퍼 파라미터를 찾습니다. 123456# 데이터 로드(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()# Normalize pixel values between 0 and 1img_train = img_train.astype('float32') / 255.0img_test = img_test.astype('float32') / 255.0 1img_train.shape, label_train.shape ((60000, 28, 28), (60000,))모델 정의 12345678910111213141516171819def model_builder(hp): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) # Tune the number of units in the first Dense layer # Choose an optimal value between 32-512 hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32) model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) model.add(keras.layers.Dense(10)) # Tune the learning rate for the optimizer # Choose an optimal value from 0.01, 0.001, or 0.0001 hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate), loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy']) return model 튜너를 인스턴스화하고 하이퍼 튜닝을 수행합니다 튜너를 인스턴스화하여 하이퍼 튜닝을 수행합니다. Hyperband Tuner에는 RandomSearch , Hyperband , BayesianOptimization 및 Sklearn 네 가지 튜너가 있습니다. 이 자습서에서는 하이퍼 밴드 튜너를 사용합니다. 하이퍼 밴드 튜너를 인스턴스화하려면 하이퍼 모델, 최적화 할 objective 및 훈련 할 최대 max_epochs ( max_epochs )를 지정해야합니다.123456tuner = kt.Hyperband(model_builder, objective = 'val_accuracy', max_epochs = 10, factor = 3, directory = 'my_dir', project_name = 'intro_to_kt') INFO:tensorflow:Reloading Oracle from existing project my_dir/intro_to_kt/oracle.json INFO:tensorflow:Reloading Tuner from my_dir/intro_to_kt/tuner0.json123class ClearTrainingOutput(tf.keras.callbacks.Callback): def on_train_end(*args, **kwargs): IPython.display.clear_output(wait = True) 12345678910tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])# Get the optimal hyperparametersbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connectedlayer is {best_hps.get('units')} and the optimal learning rate for the optimizeris {best_hps.get('learning_rate')}.\"\"\") Trial complete Trial summary |-Trial ID: a07676c4549fc425444c7c101819cb0a |-Score: 0.8574000000953674 |-Best step: 0 Hyperparameters: |-learning_rate: 0.0001 |-tuner/bracket: 0 |-tuner/epochs: 10 |-tuner/initial_epoch: 0 |-tuner/round: 0 |-units: 64 INFO:tensorflow:Oracle triggered exit The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is 384 and the optimal learning rate for the optimizer is 0.001.123# Build the model with the optimal hyperparameters and train it on the datamodel = tuner.hypermodel.build(best_hps)model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test)) Epoch 1/10 1875/1875 [==============================] - ETA: 8:42 - loss: 2.1883 - accuracy: 0.18 - ETA: 20s - loss: 1.5613 - accuracy: 0.4721 - ETA: 11s - loss: 1.3144 - accuracy: 0.550 - ETA: 9s - loss: 1.1853 - accuracy: 0.591 - ETA: 7s - loss: 1.1046 - accuracy: 0.61 - ETA: 6s - loss: 1.0539 - accuracy: 0.63 - ETA: 6s - loss: 1.0195 - accuracy: 0.64 - ETA: 6s - loss: 0.9893 - accuracy: 0.65 - ETA: 5s - loss: 0.9689 - accuracy: 0.66 - ETA: 5s - loss: 0.9544 - accuracy: 0.67 - ETA: 6s - loss: 0.9429 - accuracy: 0.67 - ETA: 5s - loss: 0.9283 - accuracy: 0.68 - ETA: 5s - loss: 0.9130 - accuracy: 0.68 - ETA: 5s - loss: 0.9015 - accuracy: 0.68 - ETA: 5s - loss: 0.8923 - accuracy: 0.69 - ETA: 5s - loss: 0.8822 - accuracy: 0.69 - ETA: 5s - loss: 0.8716 - accuracy: 0.69 - ETA: 5s - loss: 0.8618 - accuracy: 0.70 - ETA: 5s - loss: 0.8483 - accuracy: 0.70 - ETA: 5s - loss: 0.8360 - accuracy: 0.71 - ETA: 4s - loss: 0.8242 - accuracy: 0.71 - ETA: 4s - loss: 0.8147 - accuracy: 0.71 - ETA: 4s - loss: 0.8061 - accuracy: 0.72 - ETA: 4s - loss: 0.8001 - accuracy: 0.72 - ETA: 4s - loss: 0.7961 - accuracy: 0.72 - ETA: 4s - loss: 0.7900 - accuracy: 0.72 - ETA: 4s - loss: 0.7842 - accuracy: 0.72 - ETA: 4s - loss: 0.7790 - accuracy: 0.73 - ETA: 4s - loss: 0.7746 - accuracy: 0.73 - ETA: 4s - loss: 0.7703 - accuracy: 0.73 - ETA: 4s - loss: 0.7662 - accuracy: 0.73 - ETA: 4s - loss: 0.7617 - accuracy: 0.73 - ETA: 4s - loss: 0.7573 - accuracy: 0.73 - ETA: 4s - loss: 0.7536 - accuracy: 0.73 - ETA: 4s - loss: 0.7493 - accuracy: 0.73 - ETA: 4s - loss: 0.7453 - accuracy: 0.74 - ETA: 3s - loss: 0.7407 - accuracy: 0.74 - ETA: 3s - loss: 0.7365 - accuracy: 0.74 - ETA: 3s - loss: 0.7324 - accuracy: 0.74 - ETA: 3s - loss: 0.7288 - accuracy: 0.74 - ETA: 3s - loss: 0.7252 - accuracy: 0.74 - ETA: 3s - loss: 0.7212 - accuracy: 0.74 - ETA: 3s - loss: 0.7171 - accuracy: 0.75 - ETA: 3s - loss: 0.7132 - accuracy: 0.75 - ETA: 3s - loss: 0.7094 - accuracy: 0.75 - ETA: 3s - loss: 0.7059 - accuracy: 0.75 - ETA: 3s - loss: 0.7021 - accuracy: 0.75 - ETA: 3s - loss: 0.6986 - accuracy: 0.75 - ETA: 3s - loss: 0.6962 - accuracy: 0.75 - ETA: 3s - loss: 0.6929 - accuracy: 0.75 - ETA: 2s - loss: 0.6897 - accuracy: 0.75 - ETA: 2s - loss: 0.6866 - accuracy: 0.76 - ETA: 2s - loss: 0.6839 - accuracy: 0.76 - ETA: 2s - loss: 0.6810 - accuracy: 0.76 - ETA: 2s - loss: 0.6781 - accuracy: 0.76 - ETA: 2s - loss: 0.6753 - accuracy: 0.76 - ETA: 2s - loss: 0.6728 - accuracy: 0.76 - ETA: 2s - loss: 0.6704 - accuracy: 0.76 - ETA: 2s - loss: 0.6682 - accuracy: 0.76 - ETA: 2s - loss: 0.6657 - accuracy: 0.76 - ETA: 2s - loss: 0.6633 - accuracy: 0.76 - ETA: 2s - loss: 0.6607 - accuracy: 0.76 - ETA: 2s - loss: 0.6584 - accuracy: 0.76 - ETA: 2s - loss: 0.6561 - accuracy: 0.77 - ETA: 2s - loss: 0.6541 - accuracy: 0.77 - ETA: 2s - loss: 0.6521 - accuracy: 0.77 - ETA: 1s - loss: 0.6500 - accuracy: 0.77 - ETA: 1s - loss: 0.6479 - accuracy: 0.77 - ETA: 1s - loss: 0.6457 - accuracy: 0.77 - ETA: 1s - loss: 0.6433 - accuracy: 0.77 - ETA: 1s - loss: 0.6410 - accuracy: 0.77 - ETA: 1s - loss: 0.6388 - accuracy: 0.77 - ETA: 1s - loss: 0.6366 - accuracy: 0.77 - ETA: 1s - loss: 0.6347 - accuracy: 0.77 - ETA: 1s - loss: 0.6324 - accuracy: 0.77 - ETA: 1s - loss: 0.6302 - accuracy: 0.77 - ETA: 1s - loss: 0.6281 - accuracy: 0.77 - ETA: 1s - loss: 0.6260 - accuracy: 0.78 - ETA: 1s - loss: 0.6240 - accuracy: 0.78 - ETA: 1s - loss: 0.6221 - accuracy: 0.78 - ETA: 1s - loss: 0.6202 - accuracy: 0.78 - ETA: 0s - loss: 0.6182 - accuracy: 0.78 - ETA: 0s - loss: 0.6163 - accuracy: 0.78 - ETA: 0s - loss: 0.6144 - accuracy: 0.78 - ETA: 0s - loss: 0.6126 - accuracy: 0.78 - ETA: 0s - loss: 0.6108 - accuracy: 0.78 - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - ETA: 0s - loss: 0.6077 - accuracy: 0.78 - ETA: 0s - loss: 0.6061 - accuracy: 0.78 - ETA: 0s - loss: 0.6043 - accuracy: 0.78 - ETA: 0s - loss: 0.6026 - accuracy: 0.78 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.5995 - accuracy: 0.78 - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.5965 - accuracy: 0.79 - 6s 3ms/step - loss: 0.5951 - accuracy: 0.7907 - val_loss: 0.4127 - val_accuracy: 0.8468 Epoch 2/10 1875/1875 [==============================] - ETA: 8s - loss: 0.4197 - accuracy: 0.84 - ETA: 3s - loss: 0.4062 - accuracy: 0.85 - ETA: 3s - loss: 0.3977 - accuracy: 0.85 - ETA: 3s - loss: 0.3848 - accuracy: 0.86 - ETA: 3s - loss: 0.3777 - accuracy: 0.86 - ETA: 3s - loss: 0.3730 - accuracy: 0.86 - ETA: 3s - loss: 0.3704 - accuracy: 0.86 - ETA: 3s - loss: 0.3683 - accuracy: 0.86 - ETA: 3s - loss: 0.3674 - accuracy: 0.86 - ETA: 3s - loss: 0.3670 - accuracy: 0.86 - ETA: 3s - loss: 0.3664 - accuracy: 0.86 - ETA: 3s - loss: 0.3659 - accuracy: 0.86 - ETA: 3s - loss: 0.3653 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3651 - accuracy: 0.86 - ETA: 3s - loss: 0.3652 - accuracy: 0.86 - ETA: 3s - loss: 0.3654 - accuracy: 0.86 - ETA: 3s - loss: 0.3655 - accuracy: 0.86 - ETA: 2s - loss: 0.3657 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3651 - accuracy: 0.86 - 4s 2ms/step - loss: 0.3651 - accuracy: 0.8674 - val_loss: 0.3735 - val_accuracy: 0.8663 Epoch 3/10 1875/1875 [==============================] - ETA: 7s - loss: 0.2668 - accuracy: 0.87 - ETA: 4s - loss: 0.2867 - accuracy: 0.88 - ETA: 4s - loss: 0.2901 - accuracy: 0.89 - ETA: 4s - loss: 0.2983 - accuracy: 0.89 - ETA: 3s - loss: 0.3031 - accuracy: 0.89 - ETA: 3s - loss: 0.3054 - accuracy: 0.89 - ETA: 3s - loss: 0.3067 - accuracy: 0.89 - ETA: 3s - loss: 0.3087 - accuracy: 0.88 - ETA: 3s - loss: 0.3110 - accuracy: 0.88 - ETA: 3s - loss: 0.3125 - accuracy: 0.88 - ETA: 3s - loss: 0.3140 - accuracy: 0.88 - ETA: 3s - loss: 0.3150 - accuracy: 0.88 - ETA: 3s - loss: 0.3159 - accuracy: 0.88 - ETA: 3s - loss: 0.3167 - accuracy: 0.88 - ETA: 3s - loss: 0.3174 - accuracy: 0.88 - ETA: 3s - loss: 0.3180 - accuracy: 0.88 - ETA: 3s - loss: 0.3185 - accuracy: 0.88 - ETA: 3s - loss: 0.3189 - accuracy: 0.88 - ETA: 3s - loss: 0.3194 - accuracy: 0.88 - ETA: 3s - loss: 0.3199 - accuracy: 0.88 - ETA: 3s - loss: 0.3202 - accuracy: 0.88 - ETA: 3s - loss: 0.3205 - accuracy: 0.88 - ETA: 3s - loss: 0.3208 - accuracy: 0.88 - ETA: 3s - loss: 0.3212 - accuracy: 0.88 - ETA: 3s - loss: 0.3214 - accuracy: 0.88 - ETA: 3s - loss: 0.3215 - accuracy: 0.88 - ETA: 2s - loss: 0.3217 - accuracy: 0.88 - ETA: 2s - loss: 0.3219 - accuracy: 0.88 - ETA: 2s - loss: 0.3220 - accuracy: 0.88 - ETA: 2s - loss: 0.3221 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3224 - accuracy: 0.88 - ETA: 2s - loss: 0.3225 - accuracy: 0.88 - ETA: 2s - loss: 0.3226 - accuracy: 0.88 - ETA: 2s - loss: 0.3227 - accuracy: 0.88 - ETA: 2s - loss: 0.3229 - accuracy: 0.88 - ETA: 2s - loss: 0.3230 - accuracy: 0.88 - ETA: 2s - loss: 0.3232 - accuracy: 0.88 - ETA: 2s - loss: 0.3233 - accuracy: 0.88 - ETA: 1s - loss: 0.3234 - accuracy: 0.88 - ETA: 1s - loss: 0.3235 - accuracy: 0.88 - ETA: 1s - loss: 0.3236 - accuracy: 0.88 - ETA: 1s - loss: 0.3237 - accuracy: 0.88 - ETA: 1s - loss: 0.3239 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3241 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - 4s 2ms/step - loss: 0.3238 - accuracy: 0.8810 - val_loss: 0.3695 - val_accuracy: 0.8646 Epoch 4/10 1875/1875 [==============================] - ETA: 7s - loss: 0.2077 - accuracy: 0.87 - ETA: 4s - loss: 0.3105 - accuracy: 0.85 - ETA: 4s - loss: 0.3010 - accuracy: 0.86 - ETA: 3s - loss: 0.2976 - accuracy: 0.87 - ETA: 3s - loss: 0.2957 - accuracy: 0.87 - ETA: 3s - loss: 0.2952 - accuracy: 0.88 - ETA: 3s - loss: 0.2951 - accuracy: 0.88 - ETA: 3s - loss: 0.2948 - accuracy: 0.88 - ETA: 3s - loss: 0.2947 - accuracy: 0.88 - ETA: 3s - loss: 0.2943 - accuracy: 0.88 - ETA: 3s - loss: 0.2940 - accuracy: 0.88 - ETA: 3s - loss: 0.2937 - accuracy: 0.88 - ETA: 3s - loss: 0.2934 - accuracy: 0.88 - ETA: 3s - loss: 0.2931 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 3s - loss: 0.2925 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2925 - accuracy: 0.88 - ETA: 2s - loss: 0.2926 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2929 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2931 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2937 - accuracy: 0.88 - ETA: 1s - loss: 0.2938 - accuracy: 0.88 - ETA: 1s - loss: 0.2939 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2941 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2943 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2949 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2951 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2956 - accuracy: 0.88 - 4s 2ms/step - loss: 0.2956 - accuracy: 0.8898 - val_loss: 0.3532 - val_accuracy: 0.8745 Epoch 5/10 1875/1875 [==============================] - ETA: 7s - loss: 0.3435 - accuracy: 0.87 - ETA: 4s - loss: 0.3009 - accuracy: 0.89 - ETA: 3s - loss: 0.2893 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2894 - accuracy: 0.89 - ETA: 3s - loss: 0.2882 - accuracy: 0.89 - ETA: 3s - loss: 0.2872 - accuracy: 0.89 - ETA: 3s - loss: 0.2868 - accuracy: 0.89 - ETA: 3s - loss: 0.2870 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2888 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2887 - accuracy: 0.89 - ETA: 3s - loss: 0.2883 - accuracy: 0.89 - ETA: 3s - loss: 0.2881 - accuracy: 0.89 - ETA: 3s - loss: 0.2880 - accuracy: 0.89 - ETA: 3s - loss: 0.2879 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 2s - loss: 0.2875 - accuracy: 0.89 - ETA: 2s - loss: 0.2872 - accuracy: 0.89 - ETA: 2s - loss: 0.2870 - accuracy: 0.89 - ETA: 2s - loss: 0.2867 - accuracy: 0.89 - ETA: 2s - loss: 0.2865 - accuracy: 0.89 - ETA: 2s - loss: 0.2863 - accuracy: 0.89 - ETA: 2s - loss: 0.2861 - accuracy: 0.89 - ETA: 2s - loss: 0.2859 - accuracy: 0.89 - ETA: 2s - loss: 0.2858 - accuracy: 0.89 - ETA: 2s - loss: 0.2856 - accuracy: 0.89 - ETA: 2s - loss: 0.2854 - accuracy: 0.89 - ETA: 2s - loss: 0.2852 - accuracy: 0.89 - ETA: 2s - loss: 0.2851 - accuracy: 0.89 - ETA: 2s - loss: 0.2850 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2839 - accuracy: 0.89 - 5s 2ms/step - loss: 0.2839 - accuracy: 0.8954 - val_loss: 0.3473 - val_accuracy: 0.8731 Epoch 6/10 1875/1875 [==============================] - ETA: 8s - loss: 0.1747 - accuracy: 0.93 - ETA: 4s - loss: 0.2120 - accuracy: 0.92 - ETA: 4s - loss: 0.2342 - accuracy: 0.91 - ETA: 4s - loss: 0.2445 - accuracy: 0.91 - ETA: 4s - loss: 0.2483 - accuracy: 0.91 - ETA: 4s - loss: 0.2509 - accuracy: 0.91 - ETA: 3s - loss: 0.2532 - accuracy: 0.90 - ETA: 3s - loss: 0.2548 - accuracy: 0.90 - ETA: 3s - loss: 0.2559 - accuracy: 0.90 - ETA: 3s - loss: 0.2568 - accuracy: 0.90 - ETA: 3s - loss: 0.2574 - accuracy: 0.90 - ETA: 3s - loss: 0.2578 - accuracy: 0.90 - ETA: 3s - loss: 0.2582 - accuracy: 0.90 - ETA: 3s - loss: 0.2586 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 2s - loss: 0.2589 - accuracy: 0.90 - ETA: 2s - loss: 0.2590 - accuracy: 0.90 - ETA: 2s - loss: 0.2591 - accuracy: 0.90 - ETA: 2s - loss: 0.2592 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2594 - accuracy: 0.90 - ETA: 2s - loss: 0.2595 - accuracy: 0.90 - ETA: 2s - loss: 0.2596 - accuracy: 0.90 - ETA: 2s - loss: 0.2597 - accuracy: 0.90 - ETA: 2s - loss: 0.2599 - accuracy: 0.90 - ETA: 2s - loss: 0.2601 - accuracy: 0.90 - ETA: 2s - loss: 0.2602 - accuracy: 0.90 - ETA: 2s - loss: 0.2604 - accuracy: 0.90 - ETA: 2s - loss: 0.2605 - accuracy: 0.90 - ETA: 2s - loss: 0.2606 - accuracy: 0.90 - ETA: 2s - loss: 0.2608 - accuracy: 0.90 - ETA: 2s - loss: 0.2609 - accuracy: 0.90 - ETA: 1s - loss: 0.2610 - accuracy: 0.90 - ETA: 1s - loss: 0.2611 - accuracy: 0.90 - ETA: 1s - loss: 0.2612 - accuracy: 0.90 - ETA: 1s - loss: 0.2613 - accuracy: 0.90 - ETA: 1s - loss: 0.2614 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2616 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2618 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2621 - accuracy: 0.90 - ETA: 1s - loss: 0.2622 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 0s - loss: 0.2624 - accuracy: 0.90 - ETA: 0s - loss: 0.2625 - accuracy: 0.90 - ETA: 0s - loss: 0.2626 - accuracy: 0.90 - ETA: 0s - loss: 0.2627 - accuracy: 0.90 - ETA: 0s - loss: 0.2628 - accuracy: 0.90 - ETA: 0s - loss: 0.2629 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2631 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2635 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2635 - accuracy: 0.9027 - val_loss: 0.3392 - val_accuracy: 0.8760 Epoch 7/10 1875/1875 [==============================] - ETA: 8s - loss: 0.2374 - accuracy: 0.90 - ETA: 3s - loss: 0.2432 - accuracy: 0.91 - ETA: 3s - loss: 0.2501 - accuracy: 0.91 - ETA: 3s - loss: 0.2520 - accuracy: 0.91 - ETA: 3s - loss: 0.2517 - accuracy: 0.91 - ETA: 3s - loss: 0.2506 - accuracy: 0.91 - ETA: 3s - loss: 0.2494 - accuracy: 0.91 - ETA: 3s - loss: 0.2490 - accuracy: 0.91 - ETA: 3s - loss: 0.2492 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2491 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2488 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2485 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2490 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2501 - accuracy: 0.9087 - val_loss: 0.3413 - val_accuracy: 0.8810 Epoch 8/10 1875/1875 [==============================] - ETA: 11s - loss: 0.1224 - accuracy: 0.937 - ETA: 5s - loss: 0.2162 - accuracy: 0.927 - ETA: 5s - loss: 0.2236 - accuracy: 0.92 - ETA: 5s - loss: 0.2277 - accuracy: 0.91 - ETA: 4s - loss: 0.2284 - accuracy: 0.91 - ETA: 4s - loss: 0.2283 - accuracy: 0.91 - ETA: 3s - loss: 0.2286 - accuracy: 0.91 - ETA: 3s - loss: 0.2295 - accuracy: 0.91 - ETA: 3s - loss: 0.2307 - accuracy: 0.91 - ETA: 3s - loss: 0.2317 - accuracy: 0.91 - ETA: 3s - loss: 0.2323 - accuracy: 0.91 - ETA: 3s - loss: 0.2324 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 2s - loss: 0.2327 - accuracy: 0.91 - ETA: 2s - loss: 0.2329 - accuracy: 0.91 - ETA: 2s - loss: 0.2332 - accuracy: 0.91 - ETA: 2s - loss: 0.2335 - accuracy: 0.91 - ETA: 2s - loss: 0.2338 - accuracy: 0.91 - ETA: 2s - loss: 0.2340 - accuracy: 0.91 - ETA: 2s - loss: 0.2342 - accuracy: 0.91 - ETA: 2s - loss: 0.2343 - accuracy: 0.91 - ETA: 2s - loss: 0.2345 - accuracy: 0.91 - ETA: 2s - loss: 0.2347 - accuracy: 0.91 - ETA: 2s - loss: 0.2349 - accuracy: 0.91 - ETA: 2s - loss: 0.2351 - accuracy: 0.91 - ETA: 2s - loss: 0.2353 - accuracy: 0.91 - ETA: 2s - loss: 0.2355 - accuracy: 0.91 - ETA: 2s - loss: 0.2356 - accuracy: 0.91 - ETA: 2s - loss: 0.2357 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2365 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2367 - accuracy: 0.91 - ETA: 1s - loss: 0.2369 - accuracy: 0.91 - ETA: 1s - loss: 0.2370 - accuracy: 0.91 - ETA: 1s - loss: 0.2371 - accuracy: 0.91 - ETA: 1s - loss: 0.2372 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2374 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2376 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2379 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2381 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - 5s 2ms/step - loss: 0.2386 - accuracy: 0.9109 - val_loss: 0.3364 - val_accuracy: 0.8836 Epoch 9/10 1875/1875 [==============================] - ETA: 11s - loss: 0.3092 - accuracy: 0.843 - ETA: 4s - loss: 0.2166 - accuracy: 0.911 - ETA: 4s - loss: 0.2219 - accuracy: 0.91 - ETA: 4s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2254 - accuracy: 0.91 - ETA: 3s - loss: 0.2260 - accuracy: 0.91 - ETA: 3s - loss: 0.2264 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2265 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 2s - loss: 0.2271 - accuracy: 0.91 - ETA: 2s - loss: 0.2273 - accuracy: 0.91 - ETA: 2s - loss: 0.2275 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2277 - accuracy: 0.91 - ETA: 2s - loss: 0.2278 - accuracy: 0.91 - ETA: 2s - loss: 0.2279 - accuracy: 0.91 - ETA: 2s - loss: 0.2280 - accuracy: 0.91 - ETA: 2s - loss: 0.2282 - accuracy: 0.91 - ETA: 2s - loss: 0.2283 - accuracy: 0.91 - ETA: 2s - loss: 0.2284 - accuracy: 0.91 - ETA: 2s - loss: 0.2286 - accuracy: 0.91 - ETA: 2s - loss: 0.2287 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2289 - accuracy: 0.91 - ETA: 1s - loss: 0.2290 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2299 - accuracy: 0.9138 - val_loss: 0.3266 - val_accuracy: 0.8846 Epoch 10/10 1875/1875 [==============================] - ETA: 6s - loss: 0.1648 - accuracy: 0.93 - ETA: 3s - loss: 0.1805 - accuracy: 0.93 - ETA: 3s - loss: 0.1840 - accuracy: 0.93 - ETA: 3s - loss: 0.1904 - accuracy: 0.92 - ETA: 3s - loss: 0.1970 - accuracy: 0.92 - ETA: 3s - loss: 0.2009 - accuracy: 0.92 - ETA: 3s - loss: 0.2040 - accuracy: 0.92 - ETA: 3s - loss: 0.2060 - accuracy: 0.92 - ETA: 3s - loss: 0.2075 - accuracy: 0.92 - ETA: 3s - loss: 0.2087 - accuracy: 0.92 - ETA: 3s - loss: 0.2099 - accuracy: 0.92 - ETA: 3s - loss: 0.2112 - accuracy: 0.92 - ETA: 3s - loss: 0.2121 - accuracy: 0.92 - ETA: 3s - loss: 0.2127 - accuracy: 0.92 - ETA: 3s - loss: 0.2132 - accuracy: 0.92 - ETA: 3s - loss: 0.2135 - accuracy: 0.92 - ETA: 3s - loss: 0.2138 - accuracy: 0.92 - ETA: 3s - loss: 0.2141 - accuracy: 0.92 - ETA: 3s - loss: 0.2144 - accuracy: 0.92 - ETA: 3s - loss: 0.2146 - accuracy: 0.91 - ETA: 3s - loss: 0.2148 - accuracy: 0.91 - ETA: 3s - loss: 0.2151 - accuracy: 0.91 - ETA: 3s - loss: 0.2154 - accuracy: 0.91 - ETA: 3s - loss: 0.2157 - accuracy: 0.91 - ETA: 3s - loss: 0.2159 - accuracy: 0.91 - ETA: 2s - loss: 0.2161 - accuracy: 0.91 - ETA: 2s - loss: 0.2163 - accuracy: 0.91 - ETA: 2s - loss: 0.2165 - accuracy: 0.91 - ETA: 2s - loss: 0.2167 - accuracy: 0.91 - ETA: 2s - loss: 0.2168 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2171 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2177 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2181 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2183 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2185 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2187 - accuracy: 0.91 - ETA: 1s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2190 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2193 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2195 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2198 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2198 - accuracy: 0.9179 - val_loss: 0.3605 - val_accuracy: 0.8808 &lt;tensorflow.python.keras.callbacks.History at 0x7fdcd087f710&gt;12 12 12","link":"/2020/08/26/Hyper-Parameter-Tuner/"},{"title":"[MySQL] Storage Engine (InnoDB vs MyISAM)","text":"생각없이 Engine을 InnoDB만 사용했지 왜 이것을 사용해야 하는지 고민해본 적이 없었다. 하지만 많은 양의 데이터를 적재하면 데이터의 수가 매우 많거나 column의 갯수가 많아지면 공간부족 현상이 나타나게 되고, 그로 인해 엔진에 대해 고민하기 시작하였다. 대략 500개에 달하는 칼럼이 필요한 상황이였다. 그래서 어떻게 테이블에 적재해야 효율적인지 고민이 필요했다. 또한 InnoDB 테이블에 많은 칼럼을 추가하니 Row size too large. 라는 오류가 발생해서 Engine을 변경하는 방법을 생각하게 되었다. 우선 각 Stroage Engine에 대해 알아보았다. Mysql Storage Engine은 물리적 저장장치에서 데이터를 어떤 식으로 구성하고 읽어올지를 결정하는 역할을 한다. 기본적으로 8가지의 스토리지 엔진이 탑재되어 있으며 CREATE TABLE문을 사용하여 테이블을 생성할 때 엔진 이름을 추가함으로써 간단하게 설정할 수 있다. 그 중 가장 많이 쓰이는 엔진은 InnoDB, MyISAM, Archive 3가지이다. InnoDB테이블 생성 시, 따로 스토리지 엔진을 명시하지 않으면 default로 설정되는 스토리지 엔진이다. InnoDB는 트랜잭션(tranjection)을 지원하고, 커밋(commit)과 롤백(roll-back) 그리고 데이터 복구 기능을 제공하므로 데이터를 효과적으로 관리할 수 있다. InnoDB는 기본적으로 row-level locking을 제공하며, 또한 데이터를 clustered index에 저장하여 PK기반의 query의 비용을 줄인다. 또한, PK 제약을 제공하여 데이터 무결성을 보장한다. 여기서 clustered index에 저장한다는 것은 데이터를 PK순서에 맞게 저장한다는 뜻이므로 order by 등 쿼리에 유리할 수 있다. 또한 row-level locking을 제공한다는 뜻은 테이블에 CRUD할 때, 로우별로 락을 잡기 때문에 multi-thread에 보다 효율적이라는 말이다. 물론 장점만 있는 것은 아니다. InnoDB는 더욱 많은 메모리와 디스크를 사용한다. 또한 데이터가 깨졌을 때 단순 파일 백업/복구만으로 처리가 가능한 MyISAM과 달리 InnoDB의 경우 복구 방법이 어렵다. MyISAM이나 Memory 방식이 지원하지 않는 FK의 경우도 테이블 간 데이터 체크로 인한 lock, 특히 dead lock이 발생할 가능성이 있다. InnoDB의 최대 행 저장공간Mysql 테이블의 행 크기는 스토리지 엔진에 제약이 없다면 기본적으로 최대 65535바이트이다. 하지만 스토리지 엔진에 따라 제약이 추가되어 행 크기는 줄어들 수 있다. BLOB, TEXT 컬럼의 내용은 행의 남은 부분이 아닌 별도의 공간에 저장되기 때문에 각각 912바이트만 영향을 준다. (포인트 저장 공간이 912바이트) 여기서 중요한 것은 위의 내용은 스토리지 엔진에 상관없이 기본적으로 이렇다는 것이다. MyISAM트랜잭션(tranjection)을 지원하지 않고 table-level locking을 제공한다. 따라서 1개의 ROW을 READ하더라도 테이블 전체에 락을 잡기 때문에 multi-thread 환경에서 성능이 저하될 수 있다. 하지만 InnoDB에 비해 기능적으로 단순하므로 대부분의 작업은 InnoDB보다 속도면에서 우월하다. 단순한 조회의 경우 MyISAM이 InnoDB보다 빠르지만, Order By등 정렬들의 구문이 들어간다면 InnoDB보다 느리다. 왜냐하면 InnoDB는 클러스터링 인덱스에 저장하기 때문에 PK에 따라 데이터 파일이 정렬되어 있지만, MyISAM은 그렇지 않기 때문이다. Full text searching을 지원한다. MyISAM 엔진의 경우 최대 행 크기가 기본 MySQL 제약을 따르므로 최대 행 크기는 65535바이트가 될 것이다. Archive로그 수집에 적합한 엔진이다. 데이터가 메모리상에서 압축되고 압축된 상태로 디스크에 저장되기 때문에 row-level locking이 가능하다. 다만, 한번 INSERT된 데이터는 UPDATE, DELETE를 사용할 수 없으며 인덱스를 지원하지 않는다. 따라서 거의 가공하지 않을 데이터에 대해서 관리하는데에 효율적일 수 있고, 테이블 파티셔닝도 지원한다. 다만 트랜잭션은 지원하지 않는다.","link":"/2020/07/22/MySQL-Storage-Engine-InnoDB-vs-MyISAM/"},{"title":"Linear Regression with scale, categorical regression, and partial regression","text":"12345678910111213141516import matplotlibfrom matplotlib import font_manager, rcimport platformtry : if platform.system() == 'windows': # windows의 경우 font_name = font_manager.FomntProperties(fname=\"c:/Windows/Font\") rc('font', family = font_name) else: # mac의 경우 rc('font', family = 'AppleGothic')except : passmatplotlib.rcParams['axes.unicode_minus'] = False 스케일링 12 이 summary report는 어제 보스턴 집값 데이터를 활용하여 선형회귀분석을 한 결과값이야. 아랫부분의 Cond. No. 라고 쓰여져 있는 부분인데 조건수(conditional number)는 가장 큰 고유치와 가장 작은 고유치의 비율을 뜻해.12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows × 14 columns 이것은 일부러 TAX변수를 크게 만들어 조건수를 증폭시켜본 데이터야12345678import statsmodels.api as smdfX = sm.add_constant(dfx)dfX2 = dfX.copy()dfX2[\"TAX\"] *= 1e13df2 = pd.concat([dfX2, dfy], axis=1)model2 = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df2)result2 = model2.fit()print(result2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.333 Model: OLS Adj. R-squared: 0.329 Method: Least Squares F-statistic: 83.39 Date: Wed, 13 May 2020 Prob (F-statistic): 8.62e-44 Time: 16:03:11 Log-Likelihood: -1737.9 No. Observations: 506 AIC: 3484. Df Residuals: 502 BIC: 3501. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept -0.0038 0.000 -8.543 0.000 -0.005 -0.003 CRIM -0.1567 0.046 -3.376 0.001 -0.248 -0.066 ZN 0.1273 0.016 7.752 0.000 0.095 0.160 INDUS -0.1971 0.019 -10.433 0.000 -0.234 -0.160 CHAS 0.0034 0.000 12.430 0.000 0.003 0.004 NOX -0.0023 0.000 -9.285 0.000 -0.003 -0.002 RM 0.0267 0.002 14.132 0.000 0.023 0.030 AGE 0.1410 0.017 8.443 0.000 0.108 0.174 DIS -0.0286 0.004 -7.531 0.000 -0.036 -0.021 RAD 0.1094 0.018 6.163 0.000 0.075 0.144 TAX 1.077e-15 2.66e-16 4.051 0.000 5.55e-16 1.6e-15 PTRATIO -0.1124 0.011 -10.390 0.000 -0.134 -0.091 B 0.0516 0.003 19.916 0.000 0.046 0.057 LSTAT -0.6569 0.056 -11.790 0.000 -0.766 -0.547 ============================================================================== Omnibus: 39.447 Durbin-Watson: 0.863 Prob(Omnibus): 0.000 Jarque-Bera (JB): 46.611 Skew: 0.704 Prob(JB): 7.56e-11 Kurtosis: 3.479 Cond. No. 1.19e+17 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.19e+17. This might indicate that there are strong multicollinearity or other numerical problems. 조건수(Conditional No.)가 1000조 수준으로 증가한 것을 볼 수 있지? 오른쪽 제일 상단에 보이는 R-squared라는 값으로 표시되는 성능지표도 크게 감소한것을 볼 수 있어. R-squared 는 이 모델 성능에 대해 몇점인지를 알려주는 기능이라고 보면 되(0.333으로 나왔으니 100점 만점에 33.3점이라는 소리야) statsmodels에서는 scale() 이라는 명령을 사용하여 스케일링을 할 수 있는데, 이 방식으로 스케일을 하면 스케일링에 사용된 평균과 표준편차를 저장하였다가 나중에 predict() 라는 명령을 사용할 때도 같은 스케일을 사용하기 때문에 편리한 것을 알 수 있어. 다만! 스케일링을 할 때에는 카테고리 변수, 즉 범주형 데이터는 스케일링을 하지 않는다는 것에 주의 해주면 되.123456feature_names = list(boston.feature_names)feature_names.remove(\"CHAS\") feature_names = ['scale({})'.format(name) for name in feature_names] + ['CHAS']model3 = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(feature_names), data=df2)result3 = model3.fit()print(result3.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.741 Model: OLS Adj. R-squared: 0.734 Method: Least Squares F-statistic: 108.1 Date: Wed, 13 May 2020 Prob (F-statistic): 6.72e-135 Time: 16:11:05 Log-Likelihood: -1498.8 No. Observations: 506 AIC: 3026. Df Residuals: 492 BIC: 3085. Df Model: 13 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 22.3470 0.219 101.943 0.000 21.916 22.778 scale(CRIM) -0.9281 0.282 -3.287 0.001 -1.483 -0.373 scale(ZN) 1.0816 0.320 3.382 0.001 0.453 1.710 scale(INDUS) 0.1409 0.421 0.334 0.738 -0.687 0.969 scale(NOX) -2.0567 0.442 -4.651 0.000 -2.926 -1.188 scale(RM) 2.6742 0.293 9.116 0.000 2.098 3.251 scale(AGE) 0.0195 0.371 0.052 0.958 -0.710 0.749 scale(DIS) -3.1040 0.420 -7.398 0.000 -3.928 -2.280 scale(RAD) 2.6622 0.577 4.613 0.000 1.528 3.796 scale(TAX) -2.0768 0.633 -3.280 0.001 -3.321 -0.833 scale(PTRATIO) -2.0606 0.283 -7.283 0.000 -2.617 -1.505 scale(B) 0.8493 0.245 3.467 0.001 0.368 1.331 scale(LSTAT) -3.7436 0.362 -10.347 0.000 -4.454 -3.033 CHAS 2.6867 0.862 3.118 0.002 0.994 4.380 ============================================================================== Omnibus: 178.041 Durbin-Watson: 1.078 Prob(Omnibus): 0.000 Jarque-Bera (JB): 783.126 Skew: 1.521 Prob(JB): 8.84e-171 Kurtosis: 8.281 Cond. No. 10.6 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 독립변수 데이터를 스케일링한것만으로 조건수의 수치가 확 내려간 것을 볼 수 있어. 어때? 쉽지? 조건수를 될 수 있으면 낮춰주는게 각 독립변수의 오차범위를 줄여줄 수 있다고 해. 그래서 큰 값의 데이터들은 스케일링을 통해 사이즈를 줄여주는 거지.범주형 독립변수의 회귀분석이번에는 연속형 데이터가 아닌 범주형 데이터의 회귀분석을 하는 방법에 대해 알아보자! 범주형 데이터는 측정 결과가 몇 개의 범주 또는 향목의 형태로 나타나는 자료를 말하는데 그것을 숫자로 표현한 것이라고 할 수 있어. 예를 들면 남자는 1 여자는 0 이런식이지! 아무튼.. 여기서 다룰 학습은 그러한 범주형 독립변수(데이터)의 회귀분석 모델링 시 에는 앞서 배운 더미변수화가 필수라는 거야. 풀랭크(full-rank) 방식과 축소랭크(reduced-rank) 방식이 있는데 풀랭크방식에서는 더미변수의 값을 원핫인코딩(one-hot-encoding) 방식으로 지정을 하는거야 예를 들어서.. 남자는 1 이고 여자는 0 이면 남자 : d1=1, d2=0 여자 : d1=0, d2=1 이런식으로 쓴다는 거지 축소랭크 방식에서는 특정한 하나의 범주값을 기준값(reference, baseline)으로 하고 기준값에 대응하는 더미변수의 가중치는 항상 1으로 놓아 계산 하는 방법이지 무슨 말인지 어렵지? 그럼 실 데이터로 예를 들어보도록 할게. 아래의 데이터는 1920년부터 1939년까지 영국 노팅험 지역의 기온을 나타낸 데이터야. 이 데이터에서 독립 변수는 월(monath)이며 범주값으로 처리를 할거야 그리고 value로 표기된 값이 종속변수인 해당 월의 평균 기온이라고 할 수 있지. 분석의 목적은 독립변수인 월 값을 이용하여 종속변수인 월 평균 기온을 예측하는 것이야.123456789101112131415161718import datetimefrom calendar import isleapdef convert_partial_year(number): #연 단위 숫자에서 날짜를 계산하는 코드 year = int(number) d = datetime.timedelta(days=(number - year) * (365 + isleap(year))) day_one = datetime.datetime(year, 1, 1) date = d + day_one return datedf_nottem = sm.datasets.get_rdataset(\"nottem\").datadf_nottem[\"date0\"] = df_nottem[[\"time\"]].applymap(convert_partial_year)df_nottem[\"date\"] = pd.DatetimeIndex(df_nottem[\"date0\"]).round('60min') + datetime.timedelta(seconds=3600*24)df_nottem[\"month\"] = df_nottem[\"date\"].dt.strftime(\"%m\").astype('category')del df_nottem[\"date0\"], df_nottem[\"date\"]df_nottem .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } time value month 0 1920.000000 40.6 01 1 1920.083333 40.8 02 2 1920.166667 44.4 03 3 1920.250000 46.7 04 4 1920.333333 54.1 05 ... ... ... ... 235 1939.583333 61.8 08 236 1939.666667 58.2 09 237 1939.750000 46.7 10 238 1939.833333 46.6 11 239 1939.916667 37.8 12 240 rows × 3 columns 박스플롯을 통해 해당 월에 종속변수데이터가 어디에 주로 모여있는지 확인을 해보는거야12df_nottem.boxplot(\"value\", \"month\")plt.show() %% 범주형 독립변수의 경우 앞서 시행한 회귀분석에서 추가해준 상수항(add_constant)을 추가하지 않아.123456# 풀랭크 방식model = sm.OLS.from_formula(\"value ~ C(month) + 0\", df_nottem)result = model.fit()print(result.summary())# C()로 카테고리로 처리 OLS Regression Results ============================================================================== Dep. Variable: value R-squared: 0.930 Model: OLS Adj. R-squared: 0.927 Method: Least Squares F-statistic: 277.3 Date: Wed, 13 May 2020 Prob (F-statistic): 2.96e-125 Time: 16:28:22 Log-Likelihood: -535.82 No. Observations: 240 AIC: 1096. Df Residuals: 228 BIC: 1137. Df Model: 11 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- C(month)[01] 39.6950 0.518 76.691 0.000 38.675 40.715 C(month)[02] 39.1900 0.518 75.716 0.000 38.170 40.210 C(month)[03] 42.1950 0.518 81.521 0.000 41.175 43.215 C(month)[04] 46.2900 0.518 89.433 0.000 45.270 47.310 C(month)[05] 52.5600 0.518 101.547 0.000 51.540 53.580 C(month)[06] 58.0400 0.518 112.134 0.000 57.020 59.060 C(month)[07] 61.9000 0.518 119.592 0.000 60.880 62.920 C(month)[08] 60.5200 0.518 116.926 0.000 59.500 61.540 C(month)[09] 56.4800 0.518 109.120 0.000 55.460 57.500 C(month)[10] 49.4950 0.518 95.625 0.000 48.475 50.515 C(month)[11] 42.5800 0.518 82.265 0.000 41.560 43.600 C(month)[12] 39.5300 0.518 76.373 0.000 38.510 40.550 ============================================================================== Omnibus: 5.430 Durbin-Watson: 1.529 Prob(Omnibus): 0.066 Jarque-Bera (JB): 5.299 Skew: -0.281 Prob(JB): 0.0707 Kurtosis: 3.463 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.123model = sm.OLS.from_formula(\"value ~ C(month)\", df_nottem)result = model.fit()print(result.summary()) OLS Regression Results ============================================================================== Dep. Variable: value R-squared: 0.930 Model: OLS Adj. R-squared: 0.927 Method: Least Squares F-statistic: 277.3 Date: Wed, 13 May 2020 Prob (F-statistic): 2.96e-125 Time: 16:32:20 Log-Likelihood: -535.82 No. Observations: 240 AIC: 1096. Df Residuals: 228 BIC: 1137. Df Model: 11 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 39.6950 0.518 76.691 0.000 38.675 40.715 C(month)[T.02] -0.5050 0.732 -0.690 0.491 -1.947 0.937 C(month)[T.03] 2.5000 0.732 3.415 0.001 1.058 3.942 C(month)[T.04] 6.5950 0.732 9.010 0.000 5.153 8.037 C(month)[T.05] 12.8650 0.732 17.575 0.000 11.423 14.307 C(month)[T.06] 18.3450 0.732 25.062 0.000 16.903 19.787 C(month)[T.07] 22.2050 0.732 30.335 0.000 20.763 23.647 C(month)[T.08] 20.8250 0.732 28.450 0.000 19.383 22.267 C(month)[T.09] 16.7850 0.732 22.931 0.000 15.343 18.227 C(month)[T.10] 9.8000 0.732 13.388 0.000 8.358 11.242 C(month)[T.11] 2.8850 0.732 3.941 0.000 1.443 4.327 C(month)[T.12] -0.1650 0.732 -0.225 0.822 -1.607 1.277 ============================================================================== Omnibus: 5.430 Durbin-Watson: 1.529 Prob(Omnibus): 0.066 Jarque-Bera (JB): 5.299 Skew: -0.281 Prob(JB): 0.0707 Kurtosis: 3.463 Cond. No. 12.9 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 축소랭크 방식 ( +0 제거) - &apos;기준이 되는 데이터 값에서 다른 범주형 데이터 값이 얼마나 다르냐&apos; 의 의미로 보면 됨, 즉 1월을 기준으로 2월에는 차이가 얼마나 있느냐 를 나타내는거야. 풀랭크 방식과 축소랭크 방식의 차이를 조금은 알 수 있게된거같아. 이 이상은 내가 이해를 못했기 때문에 넘어가도록 할게.부분회귀만약 회귀분석을 한 후에 새로운 독립변수를 추가하여 다시 회귀분석을 한다면 그 전에 회귀분석으로 구했던 가중치의 값은 변할까 변하지 않을까? 예를 들어 𝑥1 이라는 독립변수만으로 회귀분석한 결과가 다음과 같다고 하자. 이 때 새로운 독립변수 𝑥2 를 추가하여 회귀분석을 하게 되면 이 때 나오는 𝑥1 에 대한 가중치 𝑤′1 가 원래의 𝑤1 과 같을까 다를까? 답부터 말하자면 일반적으로 𝑤′1 의 값은 원래의 𝑤1 의 값과 다르다.123456789101112from sklearn.datasets import load_bostonboston = load_boston()dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)dfX = sm.add_constant(dfX0)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfX, dfy], axis=1)model_boston = sm.OLS(dfy, dfX)result_boston = model_boston.fit()print(result_boston.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.741 Model: OLS Adj. R-squared: 0.734 Method: Least Squares F-statistic: 108.1 Date: Wed, 13 May 2020 Prob (F-statistic): 6.72e-135 Time: 18:01:08 Log-Likelihood: -1498.8 No. Observations: 506 AIC: 3026. Df Residuals: 492 BIC: 3085. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 36.4595 5.103 7.144 0.000 26.432 46.487 CRIM -0.1080 0.033 -3.287 0.001 -0.173 -0.043 ZN 0.0464 0.014 3.382 0.001 0.019 0.073 INDUS 0.0206 0.061 0.334 0.738 -0.100 0.141 CHAS 2.6867 0.862 3.118 0.002 0.994 4.380 NOX -17.7666 3.820 -4.651 0.000 -25.272 -10.262 RM 3.8099 0.418 9.116 0.000 2.989 4.631 AGE 0.0007 0.013 0.052 0.958 -0.025 0.027 DIS -1.4756 0.199 -7.398 0.000 -1.867 -1.084 RAD 0.3060 0.066 4.613 0.000 0.176 0.436 TAX -0.0123 0.004 -3.280 0.001 -0.020 -0.005 PTRATIO -0.9527 0.131 -7.283 0.000 -1.210 -0.696 B 0.0093 0.003 3.467 0.001 0.004 0.015 LSTAT -0.5248 0.051 -10.347 0.000 -0.624 -0.425 ============================================================================== Omnibus: 178.041 Durbin-Watson: 1.078 Prob(Omnibus): 0.000 Jarque-Bera (JB): 783.126 Skew: 1.521 Prob(JB): 8.84e-171 Kurtosis: 8.281 Cond. No. 1.51e+04 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.51e+04. This might indicate that there are strong multicollinearity or other numerical problems. 이렇게 보면 AGE는 집값을 결정하는데 음의 상관관계를 가진다고 볼 수 있다고 할 수 있어12sns.regplot(x=\"AGE\", y=\"MEDV\", data=df)plt.show() plot_partregress(endog, exog_i, exog_others, data=None, obs_labels=True, ret_coords=False) endog: 종속변수 문자열 exog_i: 분석 대상이 되는 독립변수 문자열 exog_others: 나머지 독립변수 문자열의 리스트 data: 모든 데이터가 있는 데이터프레임 obs_labels: 데이터 라벨링 여부 ret_coords: 잔차 데이터 반환 여부 하지만! 다른 독립변수에 영향을 받은 AGE가 집값에 영향을 미쳤는지에 대한 부분을 확인해보면 그래프는 다음과 같아. AGE 데이터에 대한 부분회귀인셈이지.1234567others = list(set(df.columns).difference(set([\"MEDV\", \"AGE\"])))p, resids = sm.graphics.plot_partregress( \"MEDV\", \"AGE\", others, data=df, obs_labels=False, ret_coords=True)plt.show()# 크게 상관이 없다는 거로 나오게 되 sm.graphics.plot_partregress_grid 명령을 쓰면 전체 데이터에 대해 한번에 부분회귀 플롯을 그릴 수 있어. plot_partregress_grid(result, fig) result: 회귀분석 결과 객체 fig: plt.figure 객체1234fig = plt.figure(figsize=(8, 20))sm.graphics.plot_partregress_grid(result_boston, fig=fig)fig.suptitle(\"\")plt.show() CCPR 플롯 CCPR(Component-Component plus Residual) 플롯도 부분회귀 플롯과 마찬가지로 특정한 하나의 변수의 영향을 살펴보기 위한 것이야 부분회귀분석과 비슷하지만 다른점이 하나 있는데 그것은 위에서 언급한 다른 변수에 영향을 받은 AGE가 아니라 AGE 데이터 그 자체와 집값의 상관관계를 보기위한 방법이라고 보면 되12sm.graphics.plot_ccpr(result_boston, \"AGE\")plt.show() CCPR 플롯에서는 부분회귀 플롯과 달리 독립변수가 원래의 값 그대로 나타난다는 점을 다시 한번 상기 시켜줄게1234fig = plt.figure(figsize=(8, 15))sm.graphics.plot_ccpr_grid(result_boston, fig=fig)fig.suptitle(\"\")plt.show() plot_regress_exog(result, exog_idx) result: 회귀분석 결과 객체 exog_idx: 분석 대상이 되는 독립변수 문자열123fig = sm.graphics.plot_regress_exog(result_boston, \"AGE\")plt.tight_layout(pad=4, h_pad=0.5, w_pad=0.5)plt.show()","link":"/2020/05/13/Linear-Regression-with-scale-categorical-regression-and-partial-regression/"},{"title":"[NodeJS]_n_을_통하여_NodeJS_버전_변경하기","text":"NodeJS의 경우 버전 변경이 굉장히 잦고 버전마다 의존성 패키지가 매우 크기 때문에 여기서는 NodeJS 버전을 간단히 변경하는 n 을 소개하도록 할게1. npm 을 통하여 n 설치하기우선 현재 nodejs 의 버전을 확인해 봅니다. 1!node -v v12.17.0 그리고 npm 을 통하여 n 을 global 로 설치하자! (nodeJS를 컨트롤 할 수 있는 라이브러리라 아주 중요해!)1!sudo npm install -g n Password: 그리고 n을 재대로 설치 되었는지 확인을 위하여 버전을 확인하도록 하자12&lt;img width=\"483\" alt=\"스크린샷 2020-05-31 오전 10 43 54\" src=\"https://user-images.githubusercontent.com/59719711/83342570-0a01cc80-a32c-11ea-83d3-a67d05de3533.png\"&gt; 2. n 을 이용하여 버전 변경하기버전 변경방법은 매우 간단해. n 뒤에 lts, latest 혹은 버전을 적으면 끝!12345678# lts 버전 설치n lts # 최신 버전 설치n latest # 특정 버전 설치n 11 마지막으로 NodeJS의 버전을 변경 후 프로젝트의 node_modules를 삭제하고 yarn 혹은 npm으로 패키지를 재설치 하거나 업그레이드 하는 것을 추천해. 안그러면 종종 오류가 발생해","link":"/2020/05/31/NodeJS-n-%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%84%8B%E1%85%A7-NodeJS-%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB-%E1%84%87%E1%85%A7%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"Model sava and load in tensorflow","text":"설정필요한 라이브러리를 설치하고 텐서플로를 임포트(import)합니다.1pip install -q pyyaml h5py # HDF5 포맷으로 모델을 저장하기 위해서 필요합니다 Note: you may need to restart the kernel to use updated packages.123456import osimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersprint(tf.__version__) 2.4.0-dev20200724 예제 데이터셋 받기1234567(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()train_labels = train_labels[:1000]test_labels = test_labels[:1000]train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0 모델링 작업1234567891011121314151617181920# Sequential 모델 정의def create_model(): model = tf.keras.models.Sequential([ keras.layers.Dense(512, activation='relu', input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10) ]) model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) return model# 모델 객체 생성model = create_model()# 출력model.summary() Model: &quot;sequential_8&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_8 (Dropout) (None, 512) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________ 훈련하는 동안 체크포인트 저장하기 훈련 중간과 훈련 마지막에 체크포인트(checkpoint)를 자동으로 저장하도록 하는 것이 많이 사용하는 방법입니다. 다시 훈련하지 않고 모델을 재사용하거나 훈련 과정이 중지된 경우 이어서 훈련을 진행할 수 있습니다. tf.keras.callbacks.ModelCheckpoint은 이런 작업을 수행하는 콜백(callback)입니다. 이 콜백은 체크포인트 작업을 조정할 수 있도록 여러가지 매개변수를 제공합니다.1234567891011121314151617checkpoint_path = \"training_1/cp.ckpt\"checkpoint_dir = os.path.dirname(checkpoint_path)# 모델의 가중치를 저장하는 콜백 만들기cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)# 새로운 콜백으로 모델 훈련하기model.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), callbacks=[cp_callback]) # 콜백을 훈련에 전달합니다# 옵티마이저의 상태를 저장하는 것과 관련되어 경고가 발생할 수 있습니다.# 이 경고는 (그리고 이 노트북의 다른 비슷한 경고는) 이전 사용 방식을 권장하지 않기 위함이며 무시해도 좋습니다. WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback. Epoch 1/10 16/32 [==============&gt;...............] - ETA: 0s - loss: 1.8756 - accuracy: 0.3736 Epoch 00001: saving model to training_1/cp.ckpt 32/32 [==============================] - 1s 30ms/step - loss: 1.5677 - accuracy: 0.5056 - val_loss: 0.6899 - val_accuracy: 0.7870 Epoch 2/10 31/32 [============================&gt;.] - ETA: 0s - loss: 0.4283 - accuracy: 0.8845 Epoch 00002: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 8ms/step - loss: 0.4276 - accuracy: 0.8844 - val_loss: 0.5193 - val_accuracy: 0.8380 Epoch 3/10 20/32 [=================&gt;............] - ETA: 0s - loss: 0.2892 - accuracy: 0.9208 Epoch 00003: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.2828 - accuracy: 0.9232 - val_loss: 0.4733 - val_accuracy: 0.8510 Epoch 4/10 19/32 [================&gt;.............] - ETA: 0s - loss: 0.1721 - accuracy: 0.9687 Epoch 00004: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9622 - val_loss: 0.4489 - val_accuracy: 0.8490 Epoch 5/10 17/32 [==============&gt;...............] - ETA: 0s - loss: 0.1666 - accuracy: 0.9582 Epoch 00005: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.9605 - val_loss: 0.4112 - val_accuracy: 0.8580 Epoch 6/10 30/32 [===========================&gt;..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9851 Epoch 00006: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 0.9846 - val_loss: 0.4088 - val_accuracy: 0.8650 Epoch 7/10 17/32 [==============&gt;...............] - ETA: 0s - loss: 0.0798 - accuracy: 0.9883 Epoch 00007: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9870 - val_loss: 0.4074 - val_accuracy: 0.8680 Epoch 8/10 32/32 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9899 Epoch 00008: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9900 - val_loss: 0.4204 - val_accuracy: 0.8590 Epoch 9/10 28/32 [=========================&gt;....] - ETA: 0s - loss: 0.0588 - accuracy: 0.9915 Epoch 00009: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.0574 - accuracy: 0.9920 - val_loss: 0.4110 - val_accuracy: 0.8640 Epoch 10/10 31/32 [============================&gt;.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9978 Epoch 00010: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 0.9978 - val_loss: 0.3962 - val_accuracy: 0.8660 &lt;tensorflow.python.keras.callbacks.History at 0x7fd0f59b3f10&gt; 이 코드는 tensorflow 체크포인트 파일을 만들고 에포크가 종료될 때마다 업데이트합니다:1ls {checkpoint_dir} checkpoint cp.ckpt.index cp.ckpt.data-00000-of-0000112345# 새로운 모델 생성model = create_model()loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"훈련되지 않은 모델의 정확도: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 2.3409 - accuracy: 0.1250 훈련되지 않은 모델의 정확도: 12.50% 저장했던 모델을 로드하고 다시 평가해보도록 하겠습니다.123456# 가중치 로드model.load_weights(checkpoint_path)# 모델 재평가loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 0.3962 - accuracy: 0.8660 복원된 모델의 정확도: 86.60% 체크포인트 콜백 매개변수123456789101112131415161718192021222324# 파일 이름에 에포크 번호를 포함시킵니다(`str.format` 포맷)checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"checkpoint_dir = os.path.dirname(checkpoint_path)# 다섯 번째 에포크마다 가중치를 저장하기 위한 콜백을 만듭니다cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5)# 새로운 모델 객체를 만듭니다model = create_model()# `checkpoint_path` 포맷을 사용하는 가중치를 저장합니다model.save_weights(checkpoint_path.format(epoch=0))# 새로운 콜백을 사용하여 모델을 훈련합니다model.fit(train_images, train_labels, epochs=50, callbacks=[cp_callback], validation_data=(test_images,test_labels), verbose=0) WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen. WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback. Epoch 00005: saving model to training_2/cp-0005.ckpt Epoch 00010: saving model to training_2/cp-0010.ckpt Epoch 00015: saving model to training_2/cp-0015.ckpt Epoch 00020: saving model to training_2/cp-0020.ckpt Epoch 00025: saving model to training_2/cp-0025.ckpt Epoch 00030: saving model to training_2/cp-0030.ckpt Epoch 00035: saving model to training_2/cp-0035.ckpt Epoch 00040: saving model to training_2/cp-0040.ckpt Epoch 00045: saving model to training_2/cp-0045.ckpt Epoch 00050: saving model to training_2/cp-0050.ckpt &lt;tensorflow.python.keras.callbacks.History at 0x7fd0f1d481d0&gt;1ls {checkpoint_dir} checkpoint cp-0025.ckpt.index cp-0000.ckpt.data-00000-of-00001 cp-0030.ckpt.data-00000-of-00001 cp-0000.ckpt.index cp-0030.ckpt.index cp-0005.ckpt.data-00000-of-00001 cp-0035.ckpt.data-00000-of-00001 cp-0005.ckpt.index cp-0035.ckpt.index cp-0010.ckpt.data-00000-of-00001 cp-0040.ckpt.data-00000-of-00001 cp-0010.ckpt.index cp-0040.ckpt.index cp-0015.ckpt.data-00000-of-00001 cp-0045.ckpt.data-00000-of-00001 cp-0015.ckpt.index cp-0045.ckpt.index cp-0020.ckpt.data-00000-of-00001 cp-0050.ckpt.data-00000-of-00001 cp-0020.ckpt.index cp-0050.ckpt.index cp-0025.ckpt.data-00000-of-0000112latest = tf.train.latest_checkpoint(checkpoint_dir)latest &apos;training_2/cp-0050.ckpt&apos;123456789# 모델 초기화 및 생성model = create_model()# 모델 로드model.load_weights(latest)# 모델 복원, 평가loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc)) WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. 32/32 - 0s - loss: 0.4795 - accuracy: 0.8720 복원된 모델의 정확도: 87.20% 수동으로 가중치 저장하기123456789101112# 가중치를 저장합니다model.save_weights('./checkpoints/my_checkpoint')# 새로운 모델 객체를 만듭니다model = create_model()# 가중치를 복원합니다model.load_weights('./checkpoints/my_checkpoint')# 모델을 평가합니다loss,acc = model.evaluate(test_images, test_labels, verbose=2)print(\"복원된 모델의 정확도: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 0.4795 - accuracy: 0.8720 복원된 모델의 정확도: 87.20%전체 모델 저장하기1234567# 새로운 모델 객체를 만들고 훈련합니다model = create_model()model.fit(train_images, train_labels, epochs=10)# SavedModel로 전체 모델을 저장합니다!mkdir -p saved_modelmodel.save('saved_model/my_model') Epoch 1/10 32/32 [==============================] - 0s 15ms/step - loss: 1.6664 - accuracy: 0.4644 Epoch 2/10 32/32 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8490 Epoch 3/10 32/32 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.9225 Epoch 4/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9644 Epoch 5/10 32/32 [==============================] - 0s 4ms/step - loss: 0.1473 - accuracy: 0.9746 Epoch 6/10 32/32 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9736 Epoch 7/10 32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9785 Epoch 8/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9967 Epoch 9/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9974 Epoch 10/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9988 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. INFO:tensorflow:Assets written to: saved_model/my_model/assets12345# my_model 디렉토리!ls saved_model# assests 폴더, saved_model.pb, variables 폴더!ls saved_model/my_model \u001b[34mmy_model\u001b[m\u001b[m \u001b[34massets\u001b[m\u001b[m saved_model.pb \u001b[34mvariables\u001b[m\u001b[m1234new_model = tf.keras.models.load_model('saved_model/my_model')# 모델 구조를 확인합니다new_model.summary() Model: &quot;sequential_23&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_46 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_23 (Dropout) (None, 512) 0 _________________________________________________________________ dense_47 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________12345# 복원된 모델을 평가합니다loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)print('복원된 모델의 정확도: {:5.2f}%'.format(100*acc))print(new_model.predict(test_images).shape) 32/32 - 0s - loss: 0.4205 - accuracy: 0.0880 복원된 모델의 정확도: 8.80% (1000, 10)HDF5 파일로 저장하기1234567# 새로운 모델 객체를 만들고 훈련합니다model = create_model()model.fit(train_images, train_labels, epochs=10)# 전체 모델을 HDF5 파일로 저장합니다# '.h5' 확장자는 이 모델이 HDF5로 저장되었다는 것을 나타냅니다model.save('my_model.h5') Epoch 1/10 32/32 [==============================] - 0s 14ms/step - loss: 1.6326 - accuracy: 0.5135 Epoch 2/10 32/32 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8959 Epoch 3/10 32/32 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.9177 Epoch 4/10 32/32 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9320 Epoch 5/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9757 Epoch 6/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9879 Epoch 7/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9864 Epoch 8/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9946 Epoch 9/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9925 Epoch 10/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.999412345# 가중치와 옵티마이저를 포함하여 정확히 동일한 모델을 다시 생성합니다new_model = tf.keras.models.load_model('my_model.h5')# 모델 구조를 출력합니다new_model.summary() Model: &quot;sequential_25&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_50 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_25 (Dropout) (None, 512) 0 _________________________________________________________________ dense_51 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________12loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)print('복원된 모델의 정확도: {:5.2f}%'.format(100*acc)) 32/32 - 0s - loss: 0.4255 - accuracy: 0.0890 복원된 모델의 정확도: 8.90%12","link":"/2020/08/26/Model-sava-and-load-in-tensorflow/"},{"title":"[Mysql]Database 용량 확인","text":"[전체 데이터베이스 용량 확인]select table_schema ‘Linear_Regression’, sum(data_length + index_length) / 1024 / 1024 ‘size(MB)’ from information_schema.tables group by table_schema [특정 DB명 status 확인]show table status like ‘DB명’ [특정 DB 용량 늘리기]alter table ‘DB명’ max_rows = 400000000 avg_row_length=1500","link":"/2020/06/09/Mysql-Database-%EC%9A%A9%EB%9F%89-%ED%99%95%EC%9D%B8/"},{"title":"[mysql] workbench에서 ERD툴 사용하기 (Database Modeling)","text":"데이터베이스 모델링 (Database Modeling)데이터베이스 모델링(또는 데이터 모델링)이란 현실 세계에서 사용되는 작업이나 사물들을 DBMS의 데이터베이스 개체로 옮기기 위한 과정이라고 할 수 있습니다. 데이터베이스 모델링은 모델링을 하는 사람이 어떤 사람이냐에 따라서 각기 다른 결과가 나올 수밖에 없고 ‘많은 실무 경험과 지식을 가진 사람이 더 좋은 모델링을 한다’ 라고 합니다. 3가지의 모델링 방식이 있다. - 개념적 모델링 - 논리적 모델링 - 물리적 모델링 개념적 모델링은 주로 업무 분석 단계에서 진행되며 논리적 모델링은 업무 분석의 후반부와 시스템 설계를 하는 부분에 걸쳐서 진행된다고 할 수 있습니다. 마지막으로 물리적 모델링은 시스템 설계 단계의 후반부에서 주로 진행됩니다. 그러나 모두 절대적인 것은 아니며 사람에 따라 조금씩 차이를 보이기도 합니다. 워크벤치는 이 모델링 툴을 제공해주는데 그것이 바로 ERD툴입니다. ERD란?Entity Relationship Diagram의 약자로, 개체관계도라고 부릅니다 장점 만들고자 하는 바를 더 명확하게 알 수 있다. 이해하고 소통하기에 편리하다. RDBMS 데이터 설계가 쉬워진다. 데이터베이스모델링에서는 데이터베이스를 Schema라고 부릅니다. 실습을 해보기 전에 ERD에 대해 소개를 하였습니다. 실습은 워크벤치를 사용하였고. MySQL workbench는 약 10년 정도 상업용으로 개발되어 판매되다가, MySQL에서 workbench를 인수하여 오픈소스로 풀었다고 합니다. 실습과정 FILE &gt; New Model 클릭 Model의 이름 설정(안해도 됨) 테이블 생성 &gt; 흰 바탕에 클릭 &gt; 컬럼 생성 PK와 FK 설정 (설정 시 place a relationship using existing columns 선택 &gt; 자식테이블의 해당 컬럼 열을 먼저 클릭 후 부모테이블의 컬럼 열 클릭) 관계 생성 저장 &gt; database의 forward engineer 선택 &gt; continue 최종적으로 모델링이 되어 있는 db와 table 생성(refresh all)","link":"/2020/07/23/MySQL-workbench%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-ERD%E1%84%90%E1%85%AE%E1%86%AF-%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5-Database-Modeling/"},{"title":"[PYTHON]칼럼 내 특정 값을 가진 row만 가져오기","text":"1# 데이터 프레임에서 특정 칼럼 내에서 특정 값이 포함된 row만 가져오기 라이브러리 및 데이터 불러오기1234# seaborn 데이터 가져오기iris_df = sns.load_dataset('iris')iris_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 150 rows × 5 columns 여러개를 할 경우 | 로 구분하면 됩니다 (ex: setosa | virginica) 12iris_filtered = iris_df[iris_df['species'].str.contains('setosa|virginica')]iris_filtered .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 100 rows × 5 columns 12","link":"/2020/06/29/PYTHON-%EC%B9%BC%EB%9F%BC-%EB%82%B4-%ED%8A%B9%EC%A0%95-%EA%B0%92%EC%9D%84-%EA%B0%80%EC%A7%84-row%EB%A7%8C-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0/"},{"title":"Python Pandas 에서 특정 컬럼값의 row를 제거하기","text":"파이썬의 Pandas를 사용하면서 특정값의 row 가 존재할 때, 이 row를 제거하려면 그 값이 들어가는 row를 제외한 나머지 값들을 다시 dataframe으로 load하면 손쉽게 데이터를 처리할 수 있다. 위의 그림은 랜덤으로 생성한 데이터에서 몇몇 부분의 데이터가 0이 존재하는 그림이다. 여기에서 0값을 가지는 데이터를 제거하는 데이터 전처리 작업이다. 아래 그림은 그 결과값이다. null값이 아닌 특정값을 제거하고자 할 때 사용하면 데이터 전처리를 할 떄 매우 유용할 것으로 보인다.","link":"/2020/05/07/Python-Pandas-%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-%E1%84%90%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC-%E1%84%8F%E1%85%A5%E1%86%AF%E1%84%85%E1%85%A5%E1%86%B7%E1%84%80%E1%85%A1%E1%86%B9%E1%84%8B%E1%85%B4-row%E1%84%85%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%A6%E1%84%80%E1%85%A5%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"[Python] SQLAlchemy 사용하기","text":"DataFrame을 MySQL에 저장하기 위해 먼저 엔진 커넥터가 필요합니다. 파이썬3에서는 MySQLdb를 지원하지 않기 때문에, pymysql로 불러와야 합니다. 꼭 pymysql이 아니어도 상관없지만, 사용해보면 mysql-connector 보다 빠르다는걸 체감할 수 있습니다. 먼저, 필요한 패키지를 설치해줍니다.123# python3pip install pymysqlpip install sqlalchemy SQLAlchemy, Pymysql, MySQLdbinstall_as_MySQLdb() 함수를 통해 MySQLdb와 호환 가능합니다. 이제 sqlalchemy를 통해 DB에 연결할 수 있습니다. 주소에서 root, password는 DB에 맞게 변경해야 합니다.123456789import pandas as pdfrom sqlalchemy import create_engine# MySQL Connector using pymysqlpymysql.install_as_MySQLdb()import MySQLdbengine = create_engine(\"mysql://root:\"+\"password\"+\"@public IP/db_name\", encoding='utf-8')conn = engine.connect() MySQL에 저장하기이제 DataFrame을 MySQL에 테이블 형태로 저장할 차례입니다. 아래와 같이 pandas의 to_sql() 함수를 사용하여 저장하면 됩니다.1df.to_sql(name=table_name, con=engine, if_exists='append') 자주 사용할 수 있으니 함수로 따로 설정해주면 원할 때마다 쉽게 사용할 수 있겠죠? 그리고 if_exists의 경우 만약 동일 테이블의 이름이 존재한다면 어떻게 처리하겠느냐의 파라미터를 주는 것인데 append 외에도 replace, delete 등 다양한 것이 있습니다.","link":"/2020/07/19/Python-SQLAlchemy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/"},{"title":"Pandas: 한 셀의 데이터를 여러 행으로 나누기","text":"12df = pd.DataFrame({'alphabet': ['hello,world,in,python', 'python,is,great', 'data,science']})df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet 0 hello,world,in,python 1 python,is,great 2 data,science 위와 같이 한 셀에 들어있는 문자열을 컴마로 구분해서 한 글자씩 여러 행으로 나누고 싶다. 해결책: 문자열을 split 해 각 행을 여러 컬럼으로 나눈 후 병합하는 방법으로 구현할 수 있다. 먼저, 각 alphabet 컬럼의 문자열을 배열로 나눈다.12result = df['alphabet'].str.split(',')result 0 [hello, world, in, python] 1 [python, is, great] 2 [data, science] Name: alphabet, dtype: object 배열이 Series를 리턴하게 apply를 적용하면 Series -&gt; DataFrame으로 변환할 수 있다.12result = result.apply(lambda x: pd.Series(x))result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 hello world in python 1 python is great NaN 2 data science NaN NaN stack()을 활용하여 컬럼을 행으로 변환한다.1result.stack() 0 0 hello 1 world 2 in 3 python 1 0 python 1 is 2 great 2 0 data 1 science dtype: object stack()을 실행하면, 위와 같이 멀티 인덱스를 가진 Series가 된다. 알파벳 낱자만 가져오기 위해 인덱스를 초기화하고, 기준이 된 인덱스도 제거해보자.1result.stack().reset_index(level=1, drop=True) 0 hello 0 world 0 in 0 python 1 python 1 is 1 great 2 data 2 science dtype: object 데이터프레임으로 변환하자12result = result.stack().reset_index(level=1, drop=True).to_frame('alphabet_single')result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet_single 0 hello 0 world 0 in 0 python 1 python 1 is 1 great 2 data 2 science 원본 프레임과 위의 프레임을 merge 해보자12result = df.merge(result, left_index=True, right_index=True, how='left')result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet alphabet_single 0 hello,world,in,python hello 0 hello,world,in,python world 0 hello,world,in,python in 0 hello,world,in,python python 1 python,is,great python 1 python,is,great is 1 python,is,great great 2 data,science data 2 data,science science","link":"/2020/07/13/Pandas-%ED%95%9C-%EC%85%80%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%97%AC%EB%9F%AC-%ED%96%89%EC%9C%BC%EB%A1%9C-%EB%82%98%EB%88%84%EA%B8%B0/"},{"title":"SQLite db view tool.md","text":"DB Browser for SQLite http://sqlitebrowser.org/ 사이트 들어가서, 해당 OS에서 맞는 파일을 다운로드 하자. 설치 완료 후, 프로그램을 가동하여, 열기를 통해 SQLite 의 파일을 열어서 테이블 형식으로 아래와 같이 볼 수 있다. 사이트에서 접속하여 위에 카테고리 부분에 download를 클릭 후 해당 OS에 맞는 것을 설치해주면 끝. 쉽지?","link":"/2020/06/02/SQLite-db-view-tool-md/"},{"title":"Tensorflow를 활용한 회귀 모델링","text":"자동차 연비 예측하기12345678import pathlibimport matplotlib.pyplot as pltimport seaborn as snsimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersprint(tf.__version__) 2.4.0-dev20200724Auto MPG 데이터셋UCI 머신러닝 저장소에서 다운로드를 받자!123dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/\\ machine-learning-databases/auto-mpg/auto-mpg.data\")dataset_path &apos;/Users/wglee/.keras/datasets/auto-mpg.data&apos;1234567# 데이터 불러오기column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepwer', 'Weight', 'Acceleration',\\ 'Model_year', 'Origin']dataset = pd.read_csv(dataset_path, names=column_names, na_values='?', comment='\\t', sep=' ',\\ skipinitialspace=True)df = dataset.copy() 1df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPG Cylinders Displacement Horsepwer Weight Acceleration Model_year Origin 396 28.0 4 120.0 79.0 2625.0 18.6 82 1 397 31.0 4 119.0 82.0 2720.0 19.4 82 1 1df['Origin'].unique() array([1, 3, 2]) null값 확인 결과 6개의 데이터가 누락된 것을 확인하였고 제거 정제 작업을 하였습니다.1df.isnull().sum() MPG 0 Cylinders 0 Displacement 0 Horsepwer 6 Weight 0 Acceleration 0 Model_year 0 Origin 0 dtype: int641df.dropna(inplace=True) 12import missingno as msnomsno.matrix(df, figsize=(8, 2)) &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7faaa5864f10&gt; &quot;Origin&quot; 열은 수치형이 아니고 범주형이므로 원-핫 인코딩(one-hot encoding)으로 변환1origin = df.pop('Origin') 123df['USA'] = (origin == 1) * 1.0df['Europe'] = (origin == 2) * 2.0df['Japan'] = (origin == 3) * 3.0 1df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPG Cylinders Displacement Horsepwer Weight Acceleration Model_year USA Europe Japan 396 28.0 4 120.0 79.0 2625.0 18.6 82 1.0 0.0 0.0 397 31.0 4 119.0 82.0 2720.0 19.4 82 1.0 0.0 0.0 데이터셋 분리 (train, test)12train_df = df.sample(frac=0.7, random_state=0)test_df = df.drop(train_df.index) 1len(train_df) 274 데이터 EDA를 통해 데이터의 분포 및 통계치를 확인합니다12sns.pairplot(train_df[['MPG','Cylinders','Displacement','Weight']], diag_kind='kde')plt.show() 1234train_stats = train_df.describe()# train_stats.pop(\"MPG\")train_stats = train_stats.T #transposetrain_stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max MPG 274.0 23.323358 7.643458 10.0 17.0 22.0 29.000 46.6 Cylinders 274.0 5.467153 1.690530 3.0 4.0 4.0 8.000 8.0 Displacement 274.0 193.846715 102.402201 68.0 105.0 151.0 260.000 455.0 Horsepwer 274.0 104.135036 37.281034 46.0 76.0 93.0 128.000 225.0 Weight 274.0 2976.879562 829.860536 1649.0 2250.5 2822.5 3573.000 4997.0 Acceleration 274.0 15.590876 2.714719 8.0 14.0 15.5 17.275 24.8 Model_year 274.0 75.934307 3.685839 70.0 73.0 76.0 79.000 82.0 USA 274.0 0.635036 0.482301 0.0 0.0 1.0 1.000 1.0 Europe 274.0 0.335766 0.748893 0.0 0.0 0.0 0.000 2.0 Japan 274.0 0.591241 1.195564 0.0 0.0 0.0 0.000 3.0 이번에는 train, test 분리가 아니라 feature와 label를 분리시켜 줍니다.12train_labels = train_df['MPG']test_labels = test_df['MPG'] 데이터 정규화feature의 크기와 범위가 다르면 정규화(normalization)를 하는 것이 권장됩니다. 정규화를 하지 않아도 모델링이 가능하지만 훈련시키기 어렵고 입력 단위에 의존적인 모델이 만들어지게 됩니다.12345# # 데이터 정규화# from sklearn.preprocessing import StandardScaler# scaler = StandardScaler()# train_df = scaler.fit_transform(train_df)# test_df = scaler.fit_transform(test_df) 1234def norm(x): return (x - train_stats['mean']) / train_stats['std']normed_train_data = norm(train_df)normed_test_data = norm(test_df) 모델링모델을 구성해 보죠. 여기에서는 두 개의 완전 연결(densely connected) 은닉층으로 Sequential 모델을 만들겠습니다. 출력 층은 하나의 연속적인 값을 반환합니다. 나중에 두 번째 모델을 만들기 쉽도록 build_model 함수로 모델 구성 단계를 감싸겠습니다.1234567891011121314# 모델링def build_model(): model = keras.Sequential([ layers.Dense(128, activation='relu', input_shape=[len(train_df.keys())]), layers.Dense(64, activation='relu'), layers.Dense(1) ]) optimizer = tf.keras.optimizers.RMSprop(0.001) model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse']) return model 1model = build_model() 모델 확인.summary() 메서드를 사용하여 모델의 간단한 정보를 출력해줍니다.1print(model.summary()) Model: &quot;sequential_15&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_45 (Dense) (None, 128) 1408 _________________________________________________________________ dense_46 (Dense) (None, 64) 8256 _________________________________________________________________ dense_47 (Dense) (None, 1) 65 ================================================================= Total params: 9,729 Trainable params: 9,729 Non-trainable params: 0 _________________________________________________________________ None 모델을 한번 실행해 보죠. training 세트에서 10개의 샘플을 하나의 배치로 만들어 model_predict 메서드를 호출해 보겠습니다.123example_batch = normed_train_data[:10]example_result = model.predict(example_batch)example_result WARNING:tensorflow:5 out of the last 15 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7faa9a8f0200&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. array([[-0.03285253], [-0.01362434], [-0.48285854], [ 0.01581845], [ 0.08219826], [ 0.08362657], [ 0.15519306], [ 0.28581452], [ 0.07680693], [ 0.01200353]], dtype=float32)모델 훈련에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다.1234567891011class PrintDot(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): if epoch % 100 == 0: print('') print('.', end='')EPOCHS = 1000history = model.fit( normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()]) .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... acc : 훈련 정확도 loss : 훈련 손실값 val_acc : 검증 정확도 val_loss : 검증 손실값123hist = pd.DataFrame(history.history)hist['epoch'] = history.epochhist.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss mae mse accuracy val_loss val_mae val_mse val_accuracy epoch 995 0.102436 0.234032 0.102436 0.0 0.165683 0.324213 0.165683 0.0 995 996 0.124358 0.292103 0.124358 0.0 0.263786 0.404004 0.263786 0.0 996 997 0.130789 0.295300 0.130789 0.0 0.212862 0.362374 0.212862 0.0 997 998 0.116644 0.275093 0.116644 0.0 0.054454 0.196261 0.054454 0.0 998 999 0.106241 0.280440 0.106241 0.0 0.121306 0.281089 0.121306 0.0 999 12345678910111213141516171819202122232425262728def plot_history(history): hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch plt.figure(figsize=(8,8)) plt.subplot(2,1,1) plt.plot(hist['epoch'], hist['mae'], label='Train Error') plt.xlabel('Epoch') plt.ylabel('Mean Abs Error [MPG]') plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error') plt.ylim([0,5]) plt.legend() plt.subplot(2,1,2) plt.xlabel('Epoch') plt.ylabel('Mean Square Error [$MPG^2$]') plt.plot(hist['epoch'], hist['mse'], label='Train Error') plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error') plt.ylim([0,20]) plt.legend() plt.show()plot_history(history) 123456789model = build_model()# patience 매개변수는 성능 향상을 체크할 에포크 횟수입니다early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])plot_history(history) ...................................................................................... 모델 검증테스트 세트의 모델 성능을 확인123loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)print(\"테스트 세트의 평균 절대 오차: {:5.2f} MPG\".format(mae)) 4/4 - 0s - loss: 0.4875 - mae: 0.5579 - mse: 0.4875 테스트 세트의 평균 절대 오차: 0.56 MPG예측테스트 세트에 있는 샘플을 이용해 MPG 값 예측12345678910test_predictions = model.predict(normed_test_data).flatten()plt.scatter(test_labels, test_predictions)plt.xlabel('True Values [MPG]')plt.ylabel('Predictions [MPG]')plt.axis('equal')plt.axis('square')plt.xlim([0,plt.xlim()[1]])plt.ylim([0,plt.ylim()[1]])_ = plt.plot([-100, 100], [-100, 100]) 1234error = test_predictions - test_labelsplt.hist(error, bins = 25)plt.xlabel(\"Prediction Error [MPG]\")_ = plt.ylabel(\"Count\") 12","link":"/2020/08/21/Tensorflow%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80-%EB%AA%A8%EB%8D%B8%EB%A7%81/"},{"title":"mglearn에 대해","text":"간단하게 그림을 그리거나 필요한 데이터를 바로 불러들이기 위해 mglearn을 사용합니다. 깃허브에 있는 노트북을 실행할 때는 이 모듈에 관해 신경 쓸 필요가 없습니다. 만약 다른 곳에서 mglearn 함수를 호출하려면, pip install mglearn 명령으로 설치하는 것이 가장 쉬운 방법입니다.7 노트_ 이 책은 NumPy, matplotlib, pandas를 많이 사용합니다. 따라서 모든 코드는 다음의 네 라이브러리를 임포트한다고 가정합니다. from IPython.display import displayimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport mglearn","link":"/2020/06/17/mglearn%EC%97%90-%EB%8C%80%ED%95%B4/"},{"title":"git is awesome","text":"What is git? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Why to use git?How to use git?","link":"/2020/04/17/git-is-awesome/"},{"title":"Ubuntu UTF-8 설정하기","text":"아래 명령어를 통해 현재 설정된 언어정보를 확인할 수 있습니다. locale 한글팩을 설치해줍니다. apt-get -y install language-pack-ko 한글 utf8 언어팩을 설치합니다. locale-gen ko_KR.UTF-8 아래 명령어로 언어팩을 선택할 수 있습니다. dpkg-reconfigure locales “ko_KR.UTF-8 UTF-8” 에 해당하는 번호를 입력하고 엔터! (저는 290번이였네요. 혹시 다를수 있으니 확인하세요)“ko_KR.UTF-8” 을 선택합니다. (저는 3번과 4번에 똑같이 나타났습니다. 그냥 3번 눌렀습니다.) 아래 명령어로 시스템 정보를 업데이트하면 적용됩니다.update-locale LANG=ko_KR.UTF-8 LC_MESSAGES=POSIX 로그아웃 후 다시 로그인한 후(SSH 재접속) 다시 locale 명령어를 입력하면 적용됩니다!저는 아래와 같이 나타났습니다. LANG=ko_KR.UTF-8LANGUAGE=LC_CTYPE=”ko_KR.UTF-8”LC_NUMERIC=”ko_KR.UTF-8”LC_TIME=”ko_KR.UTF-8”LC_COLLATE=”ko_KR.UTF-8”LC_MONETARY=”ko_KR.UTF-8”LC_MESSAGES=POSIXLC_PAPER=”ko_KR.UTF-8”LC_NAME=”ko_KR.UTF-8”LC_ADDRESS=”ko_KR.UTF-8”LC_TELEPHONE=”ko_KR.UTF-8”LC_MEASUREMENT=”ko_KR.UTF-8”LC_IDENTIFICATION=”ko_KR.UTF-8”LC_ALL=","link":"/2020/07/10/Ubuntu-UTF-8-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/"},{"title":"leverage-outliar-cooks_distance_anova","text":"1import statsmodels.api as sm 레버리지 (leverage)독립변수의 전체 데이터가 아닌 개별적인 데이터 표본 하나하나가 회귀분석 결과에 미치는 영향력은 레버리지 분석이나 아웃라이어 분석을 통해 알 수 있다. 레버리지(leverage)는 실제 종속변수값 𝑦 가 예측치(predicted target) 𝑦̂ 에 미치는 영향을 나타낸 값이다. self-influence, self-sensitivity 라고도 한다.12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows × 14 columns 123dfX = sm.add_constant(dfx)df0 = pd.concat([dfX, dfy],axis=1)df0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 1.0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 1.0 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 1.0 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 1.0 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 1.0 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 1.0 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 1.0 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 1.0 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 1.0 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 1.0 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows × 15 columns statsmodels를 이용한 레버리지 계산레버리지 값은 RegressionResults 클래스의 get_influence 메서드로 다음과 같이 구할 수 있다.12345678910111213141516171819from sklearn.datasets import make_regression# 100개의 데이터 생성X0, y, coef = make_regression(n_samples=100, n_features=1, noise=20, coef=True, random_state=1)# 레버리지가 높은 가상의 데이터를 추가data_100 = (4, 300)data_101 = (3, 150)X0 = np.vstack([X0, np.array([data_100[:1], data_101[:1]])])X = sm.add_constant(X0) #상수항 추가y = np.hstack([y, [data_100[1], data_101[1]]])plt.figure(figsize=(14,6))plt.scatter(X0, y, s=30)plt.xlabel(\"x\")plt.ylabel(\"y\")plt.title(\"가상의 회귀분석용 데이터\")plt.show() 123model = sm.OLS(pd.DataFrame(y), pd.DataFrame(X))result = model.fit()print(result.summary()) OLS Regression Results ============================================================================== Dep. Variable: 0 R-squared: 0.936 Model: OLS Adj. R-squared: 0.935 Method: Least Squares F-statistic: 1464. Date: Thu, 14 May 2020 Prob (F-statistic): 1.61e-61 Time: 14:22:53 Log-Likelihood: -452.71 No. Observations: 102 AIC: 909.4 Df Residuals: 100 BIC: 914.7 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ 0 3.2565 2.065 1.577 0.118 -0.840 7.353 1 78.3379 2.048 38.260 0.000 74.276 82.400 ============================================================================== Omnibus: 16.191 Durbin-Watson: 1.885 Prob(Omnibus): 0.000 Jarque-Bera (JB): 36.807 Skew: -0.534 Prob(JB): 1.02e-08 Kurtosis: 5.742 Cond. No. 1.14 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 선형회귀 결과에서 get_influence 메서드를 호출하면 영향도 정보 객체를 구할 수 있고, 이 객체는 hat_matrix_diag 속성으로 레버리지 벡터의 값을 가지고도 있어12345678influence = result.get_influence()hat = influence.hat_matrix_diagplt.figure(figsize=(14,6))plt. stem(hat)plt.axhline(0.02, c = 'g', ls = '--') # c = color , ls = linestyleplt.title('각 데이터의 레버리지 값')plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. &quot;&quot;&quot; 그래프를 그리는 코드에서 0.02의 값은 레버리지 평균값을 구하는 공식 독립변수의 갯수 / 데이터의 갯수 로 구하면 된다1![스크린샷 2020-05-14 오후 2 31 46](https://user-images.githubusercontent.com/59719711/81926278-b2215100-961c-11ea-9613-c5ba582d5de0.png) 123456789plt.figure(figsize=(14,6))ax = plt.subplot()plt.scatter(X0, y,s=30)sm.graphics.abline_plot(model_results=result, ax=ax)idx = hat &gt; 0.05plt.scatter(X0[idx], y[idx], s=300, c=\"r\", alpha=0.5)plt.title(\"회귀분석 결과와 레버리지 포인트\")plt.show() 그래프를 토대로 해석을 하자면, 데이터가 혼자만 너무 작거나 너무 크게 단독으로 존재할수록 레버리지가 커짐을 알 수 있어. 이 말은 저런 데이터은 전체 회귀분석 결과값에 큰 영향을 미친다는 말이야아웃라이어(outlier)데이터와 동떨어진 값을 가지는 데이터, 즉 잔차가 큰 데이터를 아웃라이어(outlier)라고 하는데, 잔차의 크기는 독립 변수의 영향을 받으므로 아웃라이어를 찾으려면 이 영향을 제거한 표준화된 잔차를 계산해야 한다고 해. 무슨말인지 잘 모르겠지만 그래 statsmodels를 이용한 표준화 잔차 계산 잔차는 RegressionResult 객체의 resid 속성에 있다.1234plt.figure(figsize=(14, 6))plt.stem(result.resid)plt.title(\"각 데이터의 잔차\")plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. 표준화 잔차는 resid_pearson 속성에 있고, 보통 표준화 잔차가 2~4보다 크면 아웃라이어로 보는게 일반적이야123456plt.figure(figsize=(14,6))plt. stem(result.resid_pearson)plt.axhline(3, c='g', ls='--')plt.axhline(-3, c='g', ls='--')plt.title('각 데이터의 표준화 잔차')plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. Cook’s Distance회귀 분석에는 레버리지 따로, 잔차의 크기가 큰 데이터가 아웃라이어가 되고 그것을 보는 따로따로의 기능도 있지만 이 두개를 동시에 보는 방법이 바로 Cook&apos;s Distance야. 아마도 Cook이라는 사람이 만들었을 가능성이.. 넘어가자 동시에 보는 기준이라고 생각하면 되고, 둘중 하나만 커지더라도 이 Cook&apos;s distance 값은 커지게 돼 모든 데이터의 레버리지와 잔차를 동시에 보려면 plot_leverage_resid2 명령을 사용하는데, 이 명령은 x축으로 표준화 잔차의 제곱을 표시하고 y축으로 레버리지값을 표시한다. 그리고 데이터 아이디가 표시된 데이터들이 레버리지가 큰 아웃라이어 야123456plt.figure(figsize=(14,6))sm.graphics.plot_leverage_resid2(result)plt.show()sm.graphics.influence_plot(result)plt.show() &lt;Figure size 1008x432 with 0 Axes&gt; 1234567891011121314from statsmodels.graphics import utilscooks_d2, pvals = influence.cooks_distanceK = influence.k_varsfox_cr = 4 / (len(y) - K - 1)idx = np.where(cooks_d2 &gt; fox_cr)[0]ax = plt.subplot()plt.scatter(X0, y)plt.scatter(X0[idx], y[idx], s=300, c=\"r\", alpha=0.5)utils.annotate_axes(range(len(idx)), idx, list(zip(X0[idx], y[idx])), [(-20, 15)] * len(idx), size=\"small\", ax=ax)plt.title(\"Fox Recommendaion으로 선택한 아웃라이어\")plt.show() 보스턴 집값 예측 문제¶보스턴 집값 문제에 아웃라이어를 적용해 보자. MEDV가 50인 데이터는 상식적으로 생각해도 이상한 데이터이므로 아웃라이어라고 판단할 수 있다. 나머지 데이터 중에서 폭스 추천공식을 사용하여 아웃라이어를 제외한 결과는 다음과 같다.12345678910boston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)dfdfX = sm.add_constant(dfx)df0 = pd.concat([dfX, dfy],axis=1)df0 123456789101112131415161718pred = result_boston.predict(dfX)influence_boston = result_boston.get_influence()cooks_d2, pvals = influence_boston.cooks_distanceK = influence.k_varsfox_cr = 4 / (len(y) - K - 1)idx = np.where(cooks_d2 &gt; fox_cr)[0]# MEDV = 50 제거idx = np.hstack([idx, np.where(boston.target == 50)[0]])ax = plt.subplot()plt.scatter(dfy, pred)plt.scatter(dfy.MEDV[idx], pred[idx], s=200, c=\"r\", alpha=0.5)utils.annotate_axes(range(len(idx)), idx, list(zip(dfy.MEDV[idx], pred[idx])), [(-20, 15)] * len(idx), size=\"small\", ax=ax)plt.title(\"보스턴 집값 데이터에서 아웃라이어\")plt.show() 다음은 이렇게 아웃라이어를 제외한 후에 다시 회귀분석을 한 결과이다.123456idx2 = list(set(range(len(dfX))).difference(idx))dfX = dfX.iloc[idx2, :].reset_index(drop=True)dfy = dfy.iloc[idx2, :].reset_index(drop=True)model_boston2 = sm.OLS(dfy, dfX)result_boston2 = model_boston2.fit()print(result_boston2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.812 Model: OLS Adj. R-squared: 0.806 Method: Least Squares F-statistic: 156.1 Date: Thu, 14 May 2020 Prob (F-statistic): 2.41e-161 Time: 15:14:52 Log-Likelihood: -1285.2 No. Observations: 485 AIC: 2598. Df Residuals: 471 BIC: 2657. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 18.8999 4.107 4.602 0.000 10.830 26.969 CRIM -0.0973 0.024 -4.025 0.000 -0.145 -0.050 ZN 0.0278 0.010 2.651 0.008 0.007 0.048 INDUS -0.0274 0.046 -0.595 0.552 -0.118 0.063 CHAS 0.9228 0.697 1.324 0.186 -0.447 2.292 NOX -9.4922 2.856 -3.323 0.001 -15.105 -3.879 RM 5.0921 0.371 13.735 0.000 4.364 5.821 AGE -0.0305 0.010 -2.986 0.003 -0.051 -0.010 DIS -1.0562 0.150 -7.057 0.000 -1.350 -0.762 RAD 0.1990 0.049 4.022 0.000 0.102 0.296 TAX -0.0125 0.003 -4.511 0.000 -0.018 -0.007 PTRATIO -0.7777 0.098 -7.955 0.000 -0.970 -0.586 B 0.0107 0.002 5.348 0.000 0.007 0.015 LSTAT -0.2846 0.043 -6.639 0.000 -0.369 -0.200 ============================================================================== Omnibus: 45.944 Durbin-Watson: 1.184 Prob(Omnibus): 0.000 Jarque-Bera (JB): 65.791 Skew: 0.679 Prob(JB): 5.17e-15 Kurtosis: 4.188 Cond. No. 1.59e+04 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.59e+04. This might indicate that there are strong multicollinearity or other numerical problems. R-squared 의 성능점수가 올라간 것을 볼 수 있어. 이렇게 어떤 특정 데이터를 가지고 회귀분석 모델링을 할 때에는 하기 전에 레버리지가 큰 데이터와 아웃라이어의 값을 이러한 절차에 의해 뽑아서 제거하고 모델링을 한다면 더욱 성능이 좋은 회귀분석 모델링을 할 수 있는거야분산 분석선형회귀분석의 결과가 얼마나 좋은지는 단순히 잔차제곱합(RSS: Residula Sum of Square)으로 평가할 수 없다. 변수의 단위 즉, 스케일이 달라지면 회귀분석과 상관없이 잔차제곱합도 달라지기 때문이야 ( ex.1km와 1000m) 분산 분석(ANOVA: Analysis of Variance)은 종속변수의 분산과 독립변수의 분산간의 관계를 사용하여 선형회귀분석의 성능을 평가하고자 하는 방법이다. 분산 분석은 서로 다른 두 개의 선형회귀분석의 성능 비교에 응용할 수 있으며 독립변수가 카테고리 변수인 경우 각 카테고리 값에 따른 영향을 정량적으로 분석하는데도 사용할 수 있게 돼 여러 수식들이 존재하지만 내가 이해를 못하겠고 결론은 다음과 같아. 모형 예측치의 움직임의 크기(분산,ESS)은 종속변수의 움직임의 크기(분산,TSS)보다 클 수 없어 그리고 모형의 성능이 좋을수록 모형 예측치의 움직임의 크기는 종속변수의 움직임의 크기와 비슷해진다는 점이야F 검정을 사용한 변수 중요도 비교F검정은 각 독립변수의 중요도를 비교하기 위해 사용할 수 있다. 방법은 전체 모형과 각 변수 하나만을 뺀 모형들의 성능을 비교하는 것인데, 이는 간접적으로 각 독립 변수의 영향력을 측정하는 것이라고 할 수 있다. 예를 들어 보스턴 집값 데이터에서 CRIM이란 변수를 뺀 모델과 전체 모델의 비교하는 검정을 하면 이 검정 결과는 CRIM변수의 중요도를 나타낸다.123456model_full = sm.OLS.from_formula( \"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)model_reduced = sm.OLS.from_formula( \"MEDV ~ ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)sm.stats.anova_lm(model_reduced.fit(), model_full.fit()) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater return (a &lt; x) &amp; (x &lt; b) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less return (a &lt; x) &amp; (x &lt; b) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal cond2 = cond0 &amp; (x &lt;= _a) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 493.0 11322.004277 0.0 NaN NaN NaN 1 492.0 11078.784578 1.0 243.219699 10.801193 0.001087 anova_lm 명령에서는 typ 인수를 2로 지정하면 하나 하나의 변수를 뺀 축소 모형에서의 F 검정값을 한꺼번에 계산할 수 있다.아노바 분석 - F검정1234model = sm.OLS.from_formula( \"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)result = model.fit()sm.stats.anova_lm(result, typ=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(&gt;F) CRIM 243.219699 1.0 10.801193 1.086810e-03 ZN 257.492979 1.0 11.435058 7.781097e-04 INDUS 2.516668 1.0 0.111763 7.382881e-01 NOX 487.155674 1.0 21.634196 4.245644e-06 RM 1871.324082 1.0 83.104012 1.979441e-18 AGE 0.061834 1.0 0.002746 9.582293e-01 DIS 1232.412493 1.0 54.730457 6.013491e-13 RAD 479.153926 1.0 21.278844 5.070529e-06 TAX 242.257440 1.0 10.758460 1.111637e-03 PTRATIO 1194.233533 1.0 53.034960 1.308835e-12 B 270.634230 1.0 12.018651 5.728592e-04 LSTAT 2410.838689 1.0 107.063426 7.776912e-23 CHAS 218.970357 1.0 9.724299 1.925030e-03 Residual 11078.784578 492.0 NaN NaN 각각의 독립변수들의 전체와 비교했을 때 얼마만큼 중요도를 가지는데 정량적으로 나온 결과값이야. 여기서 주목해야할 부분은 PR&gt;(&gt;F)부분으로 summary에서도 나오는 p-value값을 디테일하게 풀어놓은 값이고 예를 들어 LSTAT, RM의 경우 10의 -23승, 10의 -18승으로 수치가 제일 낮은걸 알 수 있어. 그러면 이 2가지의 독립변수가 종속변수에 가장 큰 영향을 미쳤다고 해석하면 되는거야 표의 F값을 보고도 알 수 있지만 F값은 확률의 의미는 없기 때문에 단순 순위를 매기는거 라면 결정할 수 있지만 만약 귀무가설/대립가설을 accept 하냐 reject 하냐의 확률적 의미를 판단한다면 F값만으로는 불가능해12 범주형을 사용한 비선형성독립변수의 비선형성을 포착하는 또 다른 방법 중 하나는 강제로 범주형 값으로 만드는 것이다. 범주형 값이 되면서 독립변수의 오차가 생기지만 이로 인한 오차보다 비선형성으로 얻을 수 있는 이익이 클 수도 있다. 보스턴 집값 데이터에서 종속변수와 RM 변수의 관계는 선형에 가깝지만 방의 갯수가 아주 작아지거나 아주 커지면 선형모형에서 벗어난다.123plt.figure(figsize=(14,6))sns.scatterplot(x=\"RM\", y=\"MEDV\", data=df0, s=60)plt.show() 123model_rm = sm.OLS.from_formula('MEDV ~ RM', data=df0)result_rm = model_rm.fit()print(result_rm.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.484 Model: OLS Adj. R-squared: 0.483 Method: Least Squares F-statistic: 471.8 Date: Thu, 14 May 2020 Prob (F-statistic): 2.49e-74 Time: 19:28:18 Log-Likelihood: -1673.1 No. Observations: 506 AIC: 3350. Df Residuals: 504 BIC: 3359. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept -34.6706 2.650 -13.084 0.000 -39.877 -29.465 RM 9.1021 0.419 21.722 0.000 8.279 9.925 ============================================================================== Omnibus: 102.585 Durbin-Watson: 0.684 Prob(Omnibus): 0.000 Jarque-Bera (JB): 612.449 Skew: 0.726 Prob(JB): 1.02e-133 Kurtosis: 8.190 Cond. No. 58.4 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 이렇게 RM 데이터 전체를 놓고 보면 종속변수 y와 아주 큰 상관관계가 있는것으로 보이지만 위에 그래프에서 봤듯이, 방의 갯수가 아주 적거나, 많으면 선형성을 보이지 않는 구간에 대해 조금 더 디테일하게 상관관게를 보고 싶다면 RM 데이터를 강제로 범주화 시켜 RM 데이터가 가지는 비선형성을 잡을 수 있다.1234567rooms = np.arange(3,10)labels = [str(r) for r in rooms[:-1]]df0['CAT_RM'] = np.round(df['RM'])plt.figure(figsize=(14,6))sns.barplot('CAT_RM', 'MEDV', data=df0)plt.show() 이렇게 하면 RM 변수으로 인한 종속변수의 변화를 비선형 상수항으로 모형화 할 수 있다. 선형모형보다 성능이 향상된 것을 볼 수 있다.123model_rm2 = sm.OLS.from_formula(\"MEDV ~ C(np.round(RM))\", data=df0)result_rm2 = model_rm2.fit()print(result_rm2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.537 Model: OLS Adj. R-squared: 0.532 Method: Least Squares F-statistic: 115.8 Date: Thu, 14 May 2020 Prob (F-statistic): 3.57e-81 Time: 19:33:48 Log-Likelihood: -1645.6 No. Observations: 506 AIC: 3303. Df Residuals: 500 BIC: 3329. Df Model: 5 Covariance Type: nonrobust ========================================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------------------ Intercept 17.0200 2.814 6.049 0.000 11.492 22.548 C(np.round(RM))[T.5.0] -2.0741 2.998 -0.692 0.489 -7.964 3.816 C(np.round(RM))[T.6.0] 2.3460 2.836 0.827 0.409 -3.226 7.918 C(np.round(RM))[T.7.0] 11.0272 2.869 3.843 0.000 5.389 16.665 C(np.round(RM))[T.8.0] 28.5425 3.093 9.228 0.000 22.466 34.619 C(np.round(RM))[T.9.0] 23.6133 4.595 5.139 0.000 14.586 32.641 ============================================================================== Omnibus: 81.744 Durbin-Watson: 0.799 Prob(Omnibus): 0.000 Jarque-Bera (JB): 467.887 Skew: 0.542 Prob(JB): 2.51e-102 Kurtosis: 7.584 Cond. No. 31.1 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.시간 독립변수의 변형독립변수가 시간인 경우에는 특정 시점에서 경과된 시간값으로 변형해야 한다. 일간 전기 사용량 데이터를 예로 들어 설명한다.12345data = sm.datasets.get_rdataset(\"elecdaily\", package=\"fpp2\")df_elec = data.data.drop(columns=[\"WorkDay\", \"Temperature\"])df_elec[\"Date\"] = pd.date_range(\"2014-1-1\", \"2014-12-31\")df_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date 360 173.727990 2014-12-27 361 188.512817 2014-12-28 362 191.273009 2014-12-29 363 186.240144 2014-12-30 364 186.370181 2014-12-31 파이썬 datetime 자료형은 toordinal 명령으로 특정 시점으로부터 경과한 시간의 일단위 값을 구하거나 timestamp 메서드로 초단위 값을 구할 수 있다.12345import datetime as dtdf_elec[\"Ordinal\"] = df_elec.Date.map(dt.datetime.toordinal)df_elec[\"Timestamp\"] = df_elec.Date.map(dt.datetime.timestamp)df_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date Ordinal Timestamp 360 173.727990 2014-12-27 735594 1.419606e+09 361 188.512817 2014-12-28 735595 1.419692e+09 362 191.273009 2014-12-29 735596 1.419779e+09 363 186.240144 2014-12-30 735597 1.419865e+09 364 186.370181 2014-12-31 735598 1.419952e+09 여기에서는 일단위 시간 값을 사용하여 회귀분석을 한다. 시간 값의 경우 크기가 크므로 반드시 스케일링을 해 주어야 한다.123model5 = sm.OLS.from_formula(\"Demand ~ scale(Ordinal)\", data=df_elec)result5 = model5.fit()print(result5.summary()) OLS Regression Results ============================================================================== Dep. Variable: Demand R-squared: 0.031 Model: OLS Adj. R-squared: 0.028 Method: Least Squares F-statistic: 11.58 Date: Thu, 14 May 2020 Prob (F-statistic): 0.000739 Time: 19:35:40 Log-Likelihood: -1709.7 No. Observations: 365 AIC: 3423. Df Residuals: 363 BIC: 3431. Df Model: 1 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 221.2775 1.374 160.997 0.000 218.575 223.980 scale(Ordinal) -4.6779 1.374 -3.404 0.001 -7.381 -1.975 ============================================================================== Omnibus: 43.105 Durbin-Watson: 0.677 Prob(Omnibus): 0.000 Jarque-Bera (JB): 96.485 Skew: 0.614 Prob(JB): 1.12e-21 Kurtosis: 5.199 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. 하지만 시간 독립변수는 이 외에더 다양한 특징들을 숨기고 있다. 예들 들어 연도, 월, 일, 요일 데이터를 별도의 독립변수로 분리하거나 한 달 내에서 몇번째 날짜인지 월의 시작 또는 끝인지를 나타내는 값은 모두 특징값이 될 수 있다. 판다스에서는 dt 특수 연산자를 사용하여 이러한 값을 구할 수 있다.12345678910df_elec[\"Year\"] = df_elec['Date'].dt.yeardf_elec[\"Month\"] = df_elec.Date.dt.monthdf_elec[\"DayOfYear\"] = df_elec.Date.dt.dayofyeardf_elec[\"DayOfMonth\"] = df_elec.Date.dt.daysinmonthdf_elec[\"DayOfWeek\"] = df_elec.Date.dt.dayofweekdf_elec[\"WeekOfYear\"] = df_elec.Date.dt.weekofyeardf_elec[\"Weekday\"] = df_elec.Date.dt.weekdaydf_elec[\"IsMonthStart\"] = df_elec.Date.dt.is_month_startdf_elec[\"IsMonthEnd\"] = df_elec.Date.dt.is_month_enddf_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date Ordinal Timestamp Year Month DayOfYear DayOfMonth DayOfWeek WeekOfYear Weekday IsMonthStart IsMonthEnd 360 173.727990 2014-12-27 735594 1.419606e+09 2014 12 361 31 5 52 5 False False 361 188.512817 2014-12-28 735595 1.419692e+09 2014 12 362 31 6 52 6 False False 362 191.273009 2014-12-29 735596 1.419779e+09 2014 12 363 31 0 1 0 False False 363 186.240144 2014-12-30 735597 1.419865e+09 2014 12 364 31 1 1 1 False False 364 186.370181 2014-12-31 735598 1.419952e+09 2014 12 365 31 2 1 2 False True 이렇게 추가적인 특징값을 이용하여 구한 모형은 성능이 향상된다.1234567891011feature_names = df_elec.columns.tolist()feature_names.remove(\"Demand\")feature_names.remove(\"Date\")formula = \"\"\"Demand ~ scale(Ordinal) + C(Month) + DayOfYear + C(DayOfMonth) + C(DayOfWeek) + C(Weekday) + C(IsMonthStart) + C(IsMonthEnd)\"\"\"model6 = sm.OLS.from_formula(formula, data=df_elec)result6 = model6.fit()print(result6.summary()) OLS Regression Results ============================================================================== Dep. Variable: Demand R-squared: 0.537 Model: OLS Adj. R-squared: 0.511 Method: Least Squares F-statistic: 19.98 Date: Thu, 14 May 2020 Prob (F-statistic): 4.74e-46 Time: 19:37:49 Log-Likelihood: -1574.8 No. Observations: 365 AIC: 3192. Df Residuals: 344 BIC: 3273. Df Model: 20 Covariance Type: nonrobust =========================================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------------------- Intercept 58.6105 2.423 24.188 0.000 53.844 63.377 C(Month)[T.2] 14.5730 4.587 3.177 0.002 5.551 23.595 C(Month)[T.3] -1.2369 8.663 -0.143 0.887 -18.276 15.802 C(Month)[T.4] -29.1875 10.239 -2.851 0.005 -49.326 -9.049 C(Month)[T.5] 23.4037 15.493 1.511 0.132 -7.069 53.876 C(Month)[T.6] 11.3667 3.758 3.024 0.003 3.974 18.759 C(Month)[T.7] 64.8095 22.750 2.849 0.005 20.063 109.556 C(Month)[T.8] 66.5692 26.490 2.513 0.012 14.467 118.671 C(Month)[T.9] 22.7687 9.491 2.399 0.017 4.100 41.437 C(Month)[T.10] 59.0491 33.895 1.742 0.082 -7.619 125.717 C(Month)[T.11] 33.4276 16.778 1.992 0.047 0.427 66.429 C(Month)[T.12] 72.2523 41.334 1.748 0.081 -9.047 153.552 C(DayOfMonth)[T.30] 38.3755 13.530 2.836 0.005 11.763 64.988 C(DayOfMonth)[T.31] 5.6620 7.806 0.725 0.469 -9.691 21.015 C(DayOfWeek)[T.1] 3.4766 1.829 1.900 0.058 -0.121 7.075 C(DayOfWeek)[T.2] 1.5756 1.821 0.865 0.387 -2.006 5.157 C(DayOfWeek)[T.3] 2.8568 1.831 1.560 0.120 -0.745 6.459 C(DayOfWeek)[T.4] 0.8832 1.831 0.482 0.630 -2.719 4.485 C(DayOfWeek)[T.5] -12.8982 1.831 -7.045 0.000 -16.499 -9.297 C(DayOfWeek)[T.6] -16.4623 1.829 -8.999 0.000 -20.060 -12.864 C(Weekday)[T.1] 3.4766 1.829 1.900 0.058 -0.121 7.075 C(Weekday)[T.2] 1.5756 1.821 0.865 0.387 -2.006 5.157 C(Weekday)[T.3] 2.8568 1.831 1.560 0.120 -0.745 6.459 C(Weekday)[T.4] 0.8832 1.831 0.482 0.630 -2.719 4.485 C(Weekday)[T.5] -12.8982 1.831 -7.045 0.000 -16.499 -9.297 C(Weekday)[T.6] -16.4623 1.829 -8.999 0.000 -20.060 -12.864 C(IsMonthStart)[T.True] 1.2012 5.781 0.208 0.836 -10.169 12.571 C(IsMonthEnd)[T.True] 4.7608 5.781 0.824 0.411 -6.609 16.131 scale(Ordinal) -101.7884 4.209 -24.182 0.000 -110.068 -93.509 DayOfYear 0.6769 0.085 7.926 0.000 0.509 0.845 ============================================================================== Omnibus: 150.460 Durbin-Watson: 0.577 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1586.415 Skew: 1.422 Prob(JB): 0.00 Kurtosis: 12.809 Cond. No. 1.05e+18 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.49e-29. This might indicate that there are strong multicollinearity problems or that the design matrix is singular.12 12","link":"/2020/05/14/leverage-outliar-cooks-distance-anova/"},{"title":"wget 설치하는법","text":"Mac OS X에서 wget을 설치하는 방법wget는 정말 편리한 명령줄 유틸리티지만, 안타깝게도 OS X에는 포함되지 않았다. 물론 Curl은 적절한 대체물이 될 수 있지만, 종종 wget로 스크립트가 쓰여지고, curl을 사용하는 것으로 변환하는 것은 어렵고 시간이 오래 걸릴 수 있어서 wget이 선호되는 경향이 있다. homebrew로 wget을 설치하면 된다.1!brew install wget Updating Homebrew... \u001b[34m==&gt;\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m Updated 2 taps (homebrew/core and homebrew/cask). \u001b[34m==&gt;\u001b[0m \u001b[1mUpdated Formulae\u001b[0m plenv postgresql@11 swiftformat tomee-webprofile postgresql postgresql@9.5 tomee-plume postgresql@10 postgresql@9.6 tomee-plus \u001b[34m==&gt;\u001b[0m \u001b[1mUpdated Casks\u001b[0m cryo switchresx \u001b[33mWarning:\u001b[0m wget 1.20.3_2 is already installed and up-to-date To reinstall 1.20.3_2, run `brew reinstall wget`1234!brew uninstall --force node!brew uninstall icu4c &amp;&amp; brew install icu4c!brew unlink icu4c &amp;&amp; brew link icu4c --force!brew install node Uninstalling node... (4,660 files, 60.3MB) \u001b[31mError:\u001b[0m Refusing to uninstall /usr/local/Cellar/icu4c/66.1 because it is required by graphviz, harfbuzz and pango, which are currently installed. You can override this and force removal with: brew uninstall --ignore-dependencies icu4c Unlinking /usr/local/Cellar/icu4c/66.1... 0 symlinks removed \u001b[33mWarning:\u001b[0m Refusing to link macOS provided/shadowed software: icu4c If you need to have icu4c first in your PATH run: echo &apos;export PATH=&quot;/usr/local/opt/icu4c/bin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile echo &apos;export PATH=&quot;/usr/local/opt/icu4c/sbin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile For compilers to find icu4c you may need to set: export LDFLAGS=&quot;-L/usr/local/opt/icu4c/lib&quot; export CPPFLAGS=&quot;-I/usr/local/opt/icu4c/include&quot; Updating Homebrew... \u001b[34m==&gt;\u001b[0m \u001b[1mDownloading https://homebrew.bintray.com/bottles/node-14.3.0.catalina.bottle\u001b[0m \u001b[34m==&gt;\u001b[0m \u001b[1mDownloading from https://akamai.bintray.com/e3/e34c4c25365bc0f5cc245d791dd93\u001b[0m ######################################################################## 100.0% \u001b[34m==&gt;\u001b[0m \u001b[1mPouring node-14.3.0.catalina.bottle.tar.gz\u001b[0m \u001b[34m==&gt;\u001b[0m \u001b[1mCaveats\u001b[0m Bash completion has been installed to: /usr/local/etc/bash_completion.d \u001b[34m==&gt;\u001b[0m \u001b[1mSummary\u001b[0m 🍺 /usr/local/Cellar/node/14.3.0: 4,659 files, 60.8MB ** 필자가 갑자기 hexo가 작동되지 않아 적잖이 당황하던 중 해결법을 찾아서 이것도 공유해. !brew uninstall --force node !brew uninstall icu4c &amp;&amp; brew install icu4c !brew unlink icu4c &amp;&amp; brew link icu4c --force !brew install node 노드 언인스톨 후 재설치하면 문제 없이 작동한다.","link":"/2020/05/28/wget-%EC%84%A4%EC%B9%98%ED%95%98%EB%8A%94%EB%B2%95/"},{"title":"wglee87DB-엑셀파일-insert하기","text":"엑셀로 정리된 파일들을 mysql 데이터베이스로 한번에 가져오는 방법(기록용) 엑셀파일의 전처리 작업 가져올 데이터만 남기고 열 이름은 삭제 엑셀파일을 csv파일로 저장 다른이름으로 저장하기 &gt; encoding=utf8 (utf8 미설정 시 한글폰트가 깨질 수 있음) &gt; 확장자 csv로저장하기 mysql로 접속하기 mysql에 접속 (필자는 sequel pro를 활용) 데이터베이스, 테이블 생성 및 encoding 변환 단, 테이블 구조와 csv파일 구조가 같아야함 CREATE TABLE [테이블명](column이름 텍스트형식) encoding=utf8로 변환 ALTER DATABASE [DB명] CHARACTER SET = utf8 ALTER TABLE [테이블명] CHARACTER SET = utf8 데이터 insert LOAD DATA LOCAL INFILE [FILE_PATH] INTO TABLE DB명.TABLE명 FIELDS TERMINATED BY ‘,’; title: ‘[wglee87DB] 엑셀파일 insert하기’date: 2020-04-27 19:24:50tags:","link":"/2020/04/27/wglee87DB-%EC%97%91%EC%85%80%ED%8C%8C%EC%9D%BC-insert%ED%95%98%EA%B8%B0/"},{"title":"구글 지오코딩 API 키 발급 받는 방법 (How to be issued the Geocoding API key from Google)","text":"안녕하세요. 오늘은 구글 맵 위, 위치에 마커를 찍어 지도 상 위치를 한눈에 쉽게 알아보기 위한 GPS 좌표에 대한 부분을 알아보려고 합니다. 일반적으로 쓰이는 주소(서울특별시 종로구 ....)와 GPS 좌표를 서로 변환하는 기능을 쉽게 구현할 수 있도록 구글에서 Geocoding API를 제공하고 있습니다. Geocoding API 사용 설정과 API 키 발급 과정에 대해서 설명하겠습니다. 과정은 조금 복잡할 수도 있기지만 쉽게 따라 하실 수 있도록 자세히 설명해보겠습니다.1. 구글 클라우드 콘솔 사이트에 방문아래 링크를 클릭해 구글 지도 플랫폼 사이트로 접속해주세요. https://cloud.google.com/maps-platform/ 구글 지도 플랫폼 사이트에서 “시작하기” 혹은 “콘솔” 버튼을 눌러 계속 진행해주세요.2. 새 프로젝트를 만들기프로젝트 선택 -&gt; 새 프로젝트 버튼을 클릭해주세요. 3. API 사용 설정하기프로젝트를 만든 후 이제 사용할 API를 추가해야 합니다. 구글 클라우드 플랫폼의 API 및 서비스 -&gt; 라이브러리 메뉴로 이동해주세요. 검색창에 “Geocoding API”를 입력해주세요. 클릭!!!!! Geocoding API의 “사용 설정” 버튼을 클릭해주세요. 4. 사용자 인증 정보 만들기이제 자신의 API 키를 발급받을 수 있습니다. 구글 클라우드 플랫폼의 API 및 서비스 -&gt; 사용자 인증 정보 메뉴로 이동해주세요. 사용자 인증 정보 만들기 -&gt; API 키 선택6번 5. API 키 발급 완료이제 API 키를 복사해 사용할 수 있습니다. 키 제한의 경우 소중한 자신의 API KEY를 아무나 함부로 쓸 수 없도록 하는 설정입니다. 설정을 안해도 KEY는 설정이 가능하지만 제한을 거는 것을 추천드립니다.","link":"/2020/08/12/%E1%84%80%E1%85%AE%E1%84%80%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A9%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B5%E1%86%BC-API-%E1%84%8F%E1%85%B5-%E1%84%87%E1%85%A1%E1%86%AF%E1%84%80%E1%85%B3%E1%86%B8-%E1%84%87%E1%85%A1%E1%86%AE%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-How-to-be-issued-the-Geocoding-API-key-from-Google/"},{"title":"시계열 데이터 전처리하기","text":"Python pandas DataFrame 이나 Series 내 ‘문자열 칼럼’을 ‘숫자형’으로 변환(how to convert string columns to numeric data types in pandas DataFrame, Series) 하는 2가지 방법에 대한 해결책! (1) pd.to_numeric() 함수를 이용한 문자열 칼럼의 숫자형 변환(2) astype() 메소드를 이용한 문자열 칼럼의 숫자형 변환 1-1. 한개의 문자열 칼럼을 숫자형으로 바꾸기변수명[‘새로운컬럼’] = pd.to_numeric(변수명[‘숫자형으로 바꿀 문자형 컬럼’]) 1-2. apply() 함수와 to_numeric() 함수를 사용해 DataFrame 내 다수의 문자열 칼럼을 숫자형으로 바꾸기변수명[[‘새로운컬럼1’, ‘새로운컬럼2’]] = 변수명[[‘기존컬럼’, ‘기존컬럼’]].apply(pd.to_numeric) 1-3. 모두 한번에 바꾸기[새로운변수명] = [기존변수명].apply(pd.to_numeric) 2-1. DataFrame 내 모든 문자열 칼럼을 float로 한꺼번에 변환하기[세로운변수명] = [변수명].astype(float) 2-2. DataFrame 내 문자열 칼럼별로 int, float 데이터 형식 개별 지정해서 숫자형으로 변환하기[새로운변수명] = [변수명].astype({‘컬럼1’: int, ‘컬럼2’: np.float}) DataFrame에 문자가 포함된 칼럼이 같이 있을 경우 ValueError","link":"/2020/05/04/%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%84%92%E1%85%A7%E1%86%BC-%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5-%E1%84%89%E1%85%AE%E1%86%BA%E1%84%8C%E1%85%A1%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%85%E1%85%A9%20%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%86%AB%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"공공데이터csv_한글깨짐현상_문제해결.md","text":"pandas 공데이터 한글깨짐현상 문제해결 공공데이터 포털에서 다운로드 받은 csv파일을 pandas에서 로딩할 때 한글깨짐 현상을 해결하는 방법에 대하여 알아보겠습니다. 한글 깨짐 현상을 해결하기 전에 영어는 아무 문제가 없지만, 한글 파일을 읽어올 때는 종종 문제가 발생하게 됩니다. Encoding. 기본적인 이해 문자형 데이터는 컴퓨터가 인식을 하지 못하기 때문에 우리는 이것을 컴퓨터가 이해할 수 있도록 Bit 형태로 변형해야합니다. 1Byte = 8Bit Ascii 계열의 문자열은 0~127까지 표현되기 때문에 1Byte 안에 충분히 표현될 수 있습니다. 하지만, 한글은 Ascii 안에 표현이 불가하기 때문에 표현하기 위해서는 Byte가 충분히 더 필요합니다. 인코딩은 한글과 같은 Ascii 범위를 벗어난 문자를 표현하기 위한 변형 작업이라고 이해하시면 쉽습니다. 하지만, 문제는 이러한 인코딩 방식이 여러가지 입니다. 공공데이터(csv) 파일의 encoding 공공데이터파일은 utf8 방식이면 좋겠으나, 아쉽게도(?) cp949 혹은 euc-kr 형식으로 인코딩이 되어 있습니다. 이러한 이유때문에 pandas에서 파일을 불러오면,, ‘utf8 codec can’t decode byte 0xb1 in position 0: invalid start byte’ 라는 에러메세지가 발생하게 됩니다. 해결책1 pandas에서 파일을 불러올 시, ‘engine=python’ 이라는 코드를 같이 작성하기 df = pd.read_csv(‘test.csv’, engine=’python’) 불러오게 되면 작동은 하나 한글이 ???형식으로 나타나게 되기도 합니다. 해결책2 pandas에서 파일을 불러올 시, ‘encoding=utf8’ 이라는 코드를 같이 작성하기 df = pd.read_csv(‘test.csv’, encoding=’utf8’) 그러나 공공데이터의 경우 해결되지 않는 경우가 많습니다. 해결책3 Excel에서 파일의 인코딩 옵션 변경하기 파일을 불러온 후, 다름이름 저장하기를 통해 고급옵션의 파일인코딩 형식을 utf8로 변경 후 저장하기 해결책4해결책1번의 코드를 작성한 후, encoding=cp949 라는 코드를 통해 불러오기(이렇게 해서 공공데이터 불러오기를 성공하였으나, utf8의 호환성이 가장 좋기에 utf8로 불러오는 것이 가장 좋다고 판단되어집니다.)","link":"/2020/04/29/%EA%B3%B5%EA%B3%B5%EB%8D%B0%EC%9D%B4%ED%84%B0csv-%ED%95%9C%EA%B8%80%EA%B9%A8%EC%A7%90%ED%98%84%EC%83%81-%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0-md/"},{"title":"공부내용","text":"markdown, vim, git 기초개념 github에 repo 생성 후 파일 add, commit, push 하는 방법 github 블로그 생성 (hexo를 통해)","link":"/2020/04/25/%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"선형회귀분석 정리","text":"선형회귀분석 정리오늘의 데이터 사이언스 스쿨에서는 본격적으로 제대로 된 선형회귀분석에 대해 배울 수 있었어. 또 배운 내용을 바탕으로 파이썬을 이용하여 패키지를 이용해 직접 구현해볼 수도 있었는데, 요약된 정보들이 가지는 함축적인 의미가 있어. 길고, 어려운 내용들이었지만 간신히 맥락은 잡고 있는 것 같아 잊기 전에 빠른 정리를 통해 복습을 하려고 해. listen. 우선 선형회귀분석이란, 종속 변수 y와 한 개 이상의 독립 변수 x와의 선형 관계를 모델링하는 방법이야. 사실 이런 단어들을 이용해 설명하면 쉬운 것도 어렵게 느껴지기 마련인데, 내가 수업시간에 깨달은 선형회귀분석 내용은 아래와 같아. seaborn 데이터셋에 있는 tip 데이터를 가져와봤어. 총 지출 비용, 팁의 금액, 팁을 준 사람의 성별과 흡연 여부, 요일 시간, 함께 온 인원들의 정보 나열되어 있네. 막무가내로 서로 연관성 없이 나열되어 있는 것 같은 이 데이터들 속에서 그 어떤 상관성이 있을까?가령, 식사 인원이 많으면 많을 수록 팁의 비용이 커지거나, 저녁 타임에 오는 손님이 팁을 더 많이 준다 등 데이터를 통해 이러한 인사이트를 얻어 내야 하는 것인데, 이러한 선형회귀분석은 이러한 데이터의 성별, 흡연여부, 식사 시간대 등과 같은 데이터(독립변수, x)들이 팁(종속변수, y)에 영향을 주는지, 혹은 영향을 주지 않는지, 영향을 주면 어떤 항목이 가장 영향을 많이 주는지 알 수 있는 방법 중에 하나라고 할 수 있지. 그렇다면 팁 데이터를 벡터값 y로 놓고 나머지 벡터들, 즉 나머지 행렬 데이터를 x로 놓고 일반적으로 알고 있는 ‘y = wx’를 만들 수 있겠지? 여기서 기울기에 해당하는 ‘w’가 바로 우리가 알고 싶어 하는 가중치라는 것이야.x의 항목들(성별, 흡연여부, 식사시간대 등) 중 어떤 항목이 팁이 많고 적음에 상관이 있는지, 즉 팁에 긍정적인 영향을 미쳐서 팁의 금액이 올라가게 하는지, 혹은 부정적인 영향을 미쳐서 팁의 금액을 내려가게 하는지, 혹은 전혀 상관이 없는지 말야. 이 때 wx를 x에 대한 함수로 나타난다면 y와 x 의 관계는 이렇게 정리할 수 있어. (어렵다 참) 아무튼! 여기서 y 위의 ^와 물결표가 붙으면서 좀 더 깊게 들어가게 되는데, ^가 붙은 y를 y hat이라고 해. y는 우리가 마주하고 있는 현실 세계에서 일어난 실제 데이터이고, y hat은 예측한 가중치, w의 영향을 받은 ‘예측값’이라고 할 수 있어. 때문에 우리가 원하는 것은 어떤 값을 가질 지 모르는 가중치 w의 값을 조정하여 우리가 예측한 예측치 y hat과 실제 발생한 데이터 값 y의 차이(잔차)를 줄이는 것이라고 할 수 있지. 언급했던 y, y hat, w 모두는 스칼라 값이 아니라 벡터 혹은 행렬의 형태를 갖추고 있어. 위의 사진에서 가중치 값 w는 이 선형회귀 모형의 모수 즉 parameter 야. 그리고 x1, x2 … 는 tip 데이터에서는 종속 변수 tip데이터를 제외한 나머지, 즉 성별, 흡연여부, 식사 시간대 등이 되는거야. 우선 선형회귀분석에 결정론적 방법과 확률론적 방법이라는 2가지 방법이 있어. 결정론적 방법은 단순하게 독립변수 x에 대응하는 종속변수 y 값을 계산하는 함수를 만들어 내는 것인 반면 확률론적 방법은 이름 그대로 x, y 뒤에 어떤 ‘확률 모형’이 숨어져 있다는 가정이 ‘추가’되는데, 이 가정이 추가됨으로써 결정론적 방법보다 더 많은 정보를 가지고 시작하게 된다고 할 수 있지.(비록 가정이라 할지라도) 더 많은 정보를 가지고 시작함으로써 결정론적 방법보다 조금 더 깊은 인사이트를 도출할 수 있어. 12345678910111213141516import matplotlibfrom matplotlib import font_manager, rcimport platformtry : if platform.system() == 'windows': # windows의 경우 font_name = font_manager.FomntProperties(fname=\"c:/Windows/Font\") rc('font', family = font_name) else: # mac의 경우 rc('font', family = 'AppleGothic')except : passmatplotlib.rcParams['axes.unicode_minus'] = False 12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows × 14 columns 1sns.pairplot(df[['MEDV','RM']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2a600f90&gt; 1sns.pairplot(df[['MEDV','CRIM']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2a60d0d0&gt; 1sns.pairplot(df[['MEDV','AGE']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2adfd150&gt; 1sns.pairplot(df[['MEDV','CHAS']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2b239d50&gt; 123456789from sklearn.linear_model import LinearRegressionmodel = LinearRegression().fit(dfx, dfy)predicted = model.predict(dfx)plt.scatter(dfy, predicted, s=10)plt.xlabel(\"실제 가격\")plt.ylabel(\"예측 가격\")plt.title(\"보스턴 주택가격 예측결과\")plt.show() 1import statsmodels.api as sm 123model = sm.OLS(dfy, dfx)result = model.fit()result.params CRIM -0.092897 ZN 0.048715 INDUS -0.004060 CHAS 2.853999 NOX -2.868436 RM 5.928148 AGE -0.007269 DIS -0.968514 RAD 0.171151 TAX -0.009396 PTRATIO -0.392191 B 0.014906 LSTAT -0.416304 dtype: float641print(result.summary()) OLS Regression Results ======================================================================================= Dep. Variable: MEDV R-squared (uncentered): 0.959 Model: OLS Adj. R-squared (uncentered): 0.958 Method: Least Squares F-statistic: 891.3 Date: Tue, 12 May 2020 Prob (F-statistic): 0.00 Time: 19:14:28 Log-Likelihood: -1523.8 No. Observations: 506 AIC: 3074. Df Residuals: 493 BIC: 3128. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ CRIM -0.0929 0.034 -2.699 0.007 -0.161 -0.025 ZN 0.0487 0.014 3.382 0.001 0.020 0.077 INDUS -0.0041 0.064 -0.063 0.950 -0.131 0.123 CHAS 2.8540 0.904 3.157 0.002 1.078 4.630 NOX -2.8684 3.359 -0.854 0.394 -9.468 3.731 RM 5.9281 0.309 19.178 0.000 5.321 6.535 AGE -0.0073 0.014 -0.526 0.599 -0.034 0.020 DIS -0.9685 0.196 -4.951 0.000 -1.353 -0.584 RAD 0.1712 0.067 2.564 0.011 0.040 0.302 TAX -0.0094 0.004 -2.395 0.017 -0.017 -0.002 PTRATIO -0.3922 0.110 -3.570 0.000 -0.608 -0.176 B 0.0149 0.003 5.528 0.000 0.010 0.020 LSTAT -0.4163 0.051 -8.197 0.000 -0.516 -0.317 ============================================================================== Omnibus: 204.082 Durbin-Watson: 0.999 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1374.225 Skew: 1.609 Prob(JB): 3.90e-299 Kurtosis: 10.404 Cond. No. 8.50e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 8.5e+03. This might indicate that there are strong multicollinearity or other numerical problems.","link":"/2020/05/12/%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8_%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5/"},{"title":"비지도학습 : PCA 주성분 분석","text":"비지도 학습을 사용해 데이터를 변환하는 이유는 여러가지이다. 가장 일반적으로는 데이터를 시각화 압축, 추가 처리를 위해 정보가 더 잘 드러나도록 하기 위해서이다. 주성분 분석(PCA) PCA의 본질은 탐색적 분석이다. 즉, 변인을 탐색해서 변환을 통해 주성분을 결정하는 방법이다. 주성분이란 데이터를 구성하는 특성 중 데이터를 가장 잘 설명하는 특성을 말한다. 데이터의 특성이 많을때 중요하다고 판단되는 일부 특성을 활용하여 데이터를 설명 또는 모델링하고자 할때 PCA를 사용한다. 그러다보니 주성분을 알아보는 것 외에 차원을 축소하는 기능을 한다고 볼 수 있는 것이다. 12import mglearnmglearn.plots.plot_pca_illustration() 왼쪽 위 그래프에서 분산이 가장 큰 방향을 찾는다. 바로 성분 1이다. 이 방향은 데이터에서 가장 많은 정보를 담고 있다. 즉, 특성들의 상관관계가 가장 큰 방향이다. 그 후 첫 번째 방향과 직간인 방향 중에서 가장 많은 정보를 담은 방향을 찾는다. 해당 그래프에서 주성분을 하나로 선정한다면 2번째 성분은 제거된다. 그리고 다시 원래 모양으로 회전시킨다. 따라서 이러한 변환은 데이터에서 노이즈를 제거하거나 주성분에서 유지되는 정보를 시각화는데 종종 사용된다. 예시)판다스에서 가장 유명한 붓꽃 데이터를 불러온다. 그 후 각 특성의 데이터를 스케일링한 후 주성분 분석을 실시하였다 12iris_df = sns.load_dataset('iris')iris_df.tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 12345678from sklearn.preprocessing import StandardScaler # 표준화 패키지 라이브러리 x = df.drop(['target'], axis=1).values # 독립변인들의 value값만 추출y = df['target'].values # 종속변인 추출x = StandardScaler().fit_transform(x) # x객체에 x를 표준화한 데이터를 저장features = ['sepal length', 'sepal width', 'petal length', 'petal width']pd.DataFrame(x, columns=features).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length sepal width petal length petal width 0 -0.900681 1.032057 -1.341272 -1.312977 1 -1.143017 -0.124958 -1.341272 -1.312977 2 -1.385353 0.337848 -1.398138 -1.312977 3 -1.506521 0.106445 -1.284407 -1.312977 4 -1.021849 1.263460 -1.341272 -1.312977 123456from sklearn.decomposition import PCApca = PCA(n_components=2) # 주성분을 몇개로 할지 결정printcipalComponents = pca.fit_transform(x)principalDf = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2'])# 주성분으로 이루어진 데이터 프레임 구성principalDf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } principal component1 principal component2 0 -2.264542 0.505704 1 -2.086426 -0.655405 2 -2.367950 -0.318477 3 -2.304197 -0.575368 4 -2.388777 0.674767 ... ... ... 145 1.870522 0.382822 146 1.558492 -0.905314 147 1.520845 0.266795 148 1.376391 1.016362 149 0.959299 -0.022284 150 rows × 2 columns 1pca.explained_variance_ratio_ array([0.72770452, 0.23030523])주성분을 2개로 했을때는 첫번째 성분이 데이터를 72%를 설명한다. 1pca.components_ array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])위의 주성분 2개에서 각 변수 중요도 1234567891011121314151617finalDf = pd.concat([principalDf, df[['target']]], axis = 1)fig = plt.figure(figsize = (8, 8))ax = fig.add_subplot(1, 1, 1)ax.set_xlabel('Principal Component 1', fontsize = 15)ax.set_ylabel('Principal Component 2', fontsize = 15)ax.set_title('2 component PCA', fontsize=20)targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']colors = ['r', 'g', 'b']for target, color in zip(targets,colors): indicesToKeep = finalDf['target'] == target ax.scatter(finalDf.loc[indicesToKeep, 'principal component1'] , finalDf.loc[indicesToKeep, 'principal component2'] , c = color , s = 50)ax.legend(targets)ax.grid() 12 12 12 12 12","link":"/2020/06/23/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-PCA-%EC%A3%BC%EC%84%B1%EB%B6%84-%EB%B6%84%EC%84%9D/"},{"title":"과대적합(Overfitting)과 과소적합(Underfitting)","text":"과대적합(Overfitting)과 과소적합(Underfitting)일정 에포크 동안 훈련을 시키면 검증세트에서 모델 성능이 최고점에 도달한 다음 감소하기 시작한 것을 알 수 있습니다. 훈련 세트에서 높은 성능을 얻을 수 있지만 진짜 원하는 것은 테스트 세트(또는 이전에 본 적 없는 데이터)에 잘 일반화되는 모델입니다. 과소적합이란 테스트 세트의 성능이 향상될 여지가 아직 있을 때 일어납니다. 발생하는 원인은 여러가지입니다. 모델이 너무 단순하거나, 규제가 너무 많거나, 그냥 단순히 충분히 오래 훈련하지 않는 경우입니다. 즉 네트워크가 훈련 세트에서 적절한 패턴을 학습하지 못했다는 뜻입니다. 모델을 너무 오래 훈련하면 과대적합되기 시작하고 테스트 세트에서 일반화되지 못하는 패턴을 훈련 세트에서 학습합니다. 과대적합과 과소적합 사이에서 균형을 잡아야 합니다. 균형을 잘 잡고 과대적합을 방지하기 위한 2가지 규제방법을 알아보도록 하겠습니다1234567import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersimport numpy as npimport matplotlib.pyplot as pltprint(tf.__version__) 2.4.0-dev20200724 데이터셋 다운로드를 받고 원핫 인코딩으로 변환하자!1234567891011121314NUM_WORDS = 1000(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)def multi_hot_sequences(sequences, dimension): # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다 results = np.zeros((len(sequences), dimension)) for i, word_indices in enumerate(sequences): results[i, word_indices] = 1.0 # results[i]의 특정 인덱스만 1로 설정합니다 return resultstrain_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS) 1234plt.plot(train_data[0])plt.grid(False)plt.xticks(rotation=45)plt.show() 기준 모델을 만들어 기준보다 유닛의 수가 크거나 작은 모델과 비교를 해보겠습니다.1234567891011base_model = keras.Sequential([ keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(16, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])base_model.summary() Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 16) 16016 _________________________________________________________________ dense_12 (Dense) (None, 16) 272 _________________________________________________________________ dense_13 (Dense) (None, 1) 17 ================================================================= Total params: 16,305 Trainable params: 16,305 Non-trainable params: 0 _________________________________________________________________12base_history = base_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.2555 - accuracy: 0.8971 - binary_crossentropy: 0.2555 - val_loss: 0.3410 - val_accuracy: 0.8558 - val_binary_crossentropy: 0.3410 Epoch 2/20 49/49 - 0s - loss: 0.2436 - accuracy: 0.9030 - binary_crossentropy: 0.2436 - val_loss: 0.3454 - val_accuracy: 0.8540 - val_binary_crossentropy: 0.3454 Epoch 3/20 49/49 - 0s - loss: 0.2356 - accuracy: 0.9068 - binary_crossentropy: 0.2356 - val_loss: 0.3525 - val_accuracy: 0.8508 - val_binary_crossentropy: 0.3525 Epoch 4/20 49/49 - 0s - loss: 0.2259 - accuracy: 0.9102 - binary_crossentropy: 0.2259 - val_loss: 0.3638 - val_accuracy: 0.8482 - val_binary_crossentropy: 0.3638 Epoch 5/20 49/49 - 0s - loss: 0.2178 - accuracy: 0.9142 - binary_crossentropy: 0.2178 - val_loss: 0.3701 - val_accuracy: 0.8487 - val_binary_crossentropy: 0.3701 Epoch 6/20 49/49 - 0s - loss: 0.2093 - accuracy: 0.9188 - binary_crossentropy: 0.2093 - val_loss: 0.3809 - val_accuracy: 0.8469 - val_binary_crossentropy: 0.3809 Epoch 7/20 49/49 - 0s - loss: 0.2026 - accuracy: 0.9208 - binary_crossentropy: 0.2026 - val_loss: 0.3854 - val_accuracy: 0.8465 - val_binary_crossentropy: 0.3854 Epoch 8/20 49/49 - 0s - loss: 0.1963 - accuracy: 0.9240 - binary_crossentropy: 0.1963 - val_loss: 0.3996 - val_accuracy: 0.8430 - val_binary_crossentropy: 0.3996 Epoch 9/20 49/49 - 0s - loss: 0.1905 - accuracy: 0.9254 - binary_crossentropy: 0.1905 - val_loss: 0.4014 - val_accuracy: 0.8421 - val_binary_crossentropy: 0.4014 Epoch 10/20 49/49 - 0s - loss: 0.1846 - accuracy: 0.9307 - binary_crossentropy: 0.1846 - val_loss: 0.4143 - val_accuracy: 0.8418 - val_binary_crossentropy: 0.4143 Epoch 11/20 49/49 - 0s - loss: 0.1787 - accuracy: 0.9322 - binary_crossentropy: 0.1787 - val_loss: 0.4300 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.4300 Epoch 12/20 49/49 - 0s - loss: 0.1739 - accuracy: 0.9329 - binary_crossentropy: 0.1739 - val_loss: 0.4402 - val_accuracy: 0.8372 - val_binary_crossentropy: 0.4402 Epoch 13/20 49/49 - 0s - loss: 0.1663 - accuracy: 0.9373 - binary_crossentropy: 0.1663 - val_loss: 0.4508 - val_accuracy: 0.8358 - val_binary_crossentropy: 0.4508 Epoch 14/20 49/49 - 0s - loss: 0.1613 - accuracy: 0.9396 - binary_crossentropy: 0.1613 - val_loss: 0.4584 - val_accuracy: 0.8364 - val_binary_crossentropy: 0.4584 Epoch 15/20 49/49 - 0s - loss: 0.1581 - accuracy: 0.9400 - binary_crossentropy: 0.1581 - val_loss: 0.4805 - val_accuracy: 0.8356 - val_binary_crossentropy: 0.4805 Epoch 16/20 49/49 - 0s - loss: 0.1534 - accuracy: 0.9419 - binary_crossentropy: 0.1534 - val_loss: 0.4836 - val_accuracy: 0.8343 - val_binary_crossentropy: 0.4836 Epoch 17/20 49/49 - 0s - loss: 0.1477 - accuracy: 0.9454 - binary_crossentropy: 0.1477 - val_loss: 0.5082 - val_accuracy: 0.8330 - val_binary_crossentropy: 0.5082 Epoch 18/20 49/49 - 0s - loss: 0.1440 - accuracy: 0.9458 - binary_crossentropy: 0.1440 - val_loss: 0.5069 - val_accuracy: 0.8342 - val_binary_crossentropy: 0.5069 Epoch 19/20 49/49 - 0s - loss: 0.1382 - accuracy: 0.9489 - binary_crossentropy: 0.1382 - val_loss: 0.5187 - val_accuracy: 0.8323 - val_binary_crossentropy: 0.5187 Epoch 20/20 49/49 - 0s - loss: 0.1339 - accuracy: 0.9520 - binary_crossentropy: 0.1339 - val_loss: 0.5385 - val_accuracy: 0.8310 - val_binary_crossentropy: 0.5385 작은 모델을 만들어보자1234567891011small_model = keras.Sequential([ keras.layers.Dense(6, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(6, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])small_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])small_model.summary() Model: &quot;sequential_4&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 6) 6006 _________________________________________________________________ dense_19 (Dense) (None, 6) 42 _________________________________________________________________ dense_20 (Dense) (None, 1) 7 ================================================================= Total params: 6,055 Trainable params: 6,055 Non-trainable params: 0 _________________________________________________________________12small_history = small_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.2994 - accuracy: 0.8785 - binary_crossentropy: 0.2994 - val_loss: 0.3305 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3305 Epoch 2/20 49/49 - 0s - loss: 0.2972 - accuracy: 0.8790 - binary_crossentropy: 0.2972 - val_loss: 0.3306 - val_accuracy: 0.8599 - val_binary_crossentropy: 0.3306 Epoch 3/20 49/49 - 0s - loss: 0.2970 - accuracy: 0.8782 - binary_crossentropy: 0.2970 - val_loss: 0.3343 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3343 Epoch 4/20 49/49 - 0s - loss: 0.2965 - accuracy: 0.8777 - binary_crossentropy: 0.2965 - val_loss: 0.3312 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3312 Epoch 5/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8794 - binary_crossentropy: 0.2960 - val_loss: 0.3314 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3314 Epoch 6/20 49/49 - 0s - loss: 0.2957 - accuracy: 0.8783 - binary_crossentropy: 0.2957 - val_loss: 0.3320 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3320 Epoch 7/20 49/49 - 0s - loss: 0.2968 - accuracy: 0.8768 - binary_crossentropy: 0.2968 - val_loss: 0.3321 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3321 Epoch 8/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8790 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8594 - val_binary_crossentropy: 0.3323 Epoch 9/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8787 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8582 - val_binary_crossentropy: 0.3323 Epoch 10/20 49/49 - 0s - loss: 0.2959 - accuracy: 0.8784 - binary_crossentropy: 0.2959 - val_loss: 0.3327 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3327 Epoch 11/20 49/49 - 0s - loss: 0.2953 - accuracy: 0.8789 - binary_crossentropy: 0.2953 - val_loss: 0.3334 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3334 Epoch 12/20 49/49 - 0s - loss: 0.2970 - accuracy: 0.8775 - binary_crossentropy: 0.2970 - val_loss: 0.3334 - val_accuracy: 0.8578 - val_binary_crossentropy: 0.3334 Epoch 13/20 49/49 - 0s - loss: 0.2951 - accuracy: 0.8798 - binary_crossentropy: 0.2951 - val_loss: 0.3341 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3341 Epoch 14/20 49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3323 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3323 Epoch 15/20 49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3324 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3324 Epoch 16/20 49/49 - 0s - loss: 0.2949 - accuracy: 0.8790 - binary_crossentropy: 0.2949 - val_loss: 0.3330 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3330 Epoch 17/20 49/49 - 0s - loss: 0.2946 - accuracy: 0.8784 - binary_crossentropy: 0.2946 - val_loss: 0.3324 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3324 Epoch 18/20 49/49 - 0s - loss: 0.2952 - accuracy: 0.8784 - binary_crossentropy: 0.2952 - val_loss: 0.3329 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3329 Epoch 19/20 49/49 - 0s - loss: 0.2943 - accuracy: 0.8794 - binary_crossentropy: 0.2943 - val_loss: 0.3330 - val_accuracy: 0.8588 - val_binary_crossentropy: 0.3330 Epoch 20/20 49/49 - 0s - loss: 0.2949 - accuracy: 0.8789 - binary_crossentropy: 0.2949 - val_loss: 0.3329 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3329 큰 모델 만들기1234567891011big_model = keras.Sequential([ keras.layers.Dense(128, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])big_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])big_model.summary() Model: &quot;sequential_5&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_21 (Dense) (None, 128) 128128 _________________________________________________________________ dense_22 (Dense) (None, 128) 16512 _________________________________________________________________ dense_23 (Dense) (None, 1) 129 ================================================================= Total params: 144,769 Trainable params: 144,769 Non-trainable params: 0 _________________________________________________________________12big_history = big_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.0047 - accuracy: 0.9999 - binary_crossentropy: 0.0047 - val_loss: 0.6867 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.6867 Epoch 2/20 49/49 - 0s - loss: 0.0029 - accuracy: 1.0000 - binary_crossentropy: 0.0029 - val_loss: 0.7205 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.7205 Epoch 3/20 49/49 - 0s - loss: 0.0019 - accuracy: 1.0000 - binary_crossentropy: 0.0019 - val_loss: 0.7533 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.7533 Epoch 4/20 49/49 - 0s - loss: 0.0014 - accuracy: 1.0000 - binary_crossentropy: 0.0014 - val_loss: 0.7802 - val_accuracy: 0.8383 - val_binary_crossentropy: 0.7802 Epoch 5/20 49/49 - 0s - loss: 0.0010 - accuracy: 1.0000 - binary_crossentropy: 0.0010 - val_loss: 0.8079 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8079 Epoch 6/20 49/49 - 0s - loss: 8.0437e-04 - accuracy: 1.0000 - binary_crossentropy: 8.0437e-04 - val_loss: 0.8324 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8324 Epoch 7/20 49/49 - 0s - loss: 6.4169e-04 - accuracy: 1.0000 - binary_crossentropy: 6.4169e-04 - val_loss: 0.8510 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8510 Epoch 8/20 49/49 - 0s - loss: 5.2259e-04 - accuracy: 1.0000 - binary_crossentropy: 5.2259e-04 - val_loss: 0.8707 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8707 Epoch 9/20 49/49 - 0s - loss: 4.3499e-04 - accuracy: 1.0000 - binary_crossentropy: 4.3499e-04 - val_loss: 0.8885 - val_accuracy: 0.8395 - val_binary_crossentropy: 0.8885 Epoch 10/20 49/49 - 0s - loss: 3.6612e-04 - accuracy: 1.0000 - binary_crossentropy: 3.6612e-04 - val_loss: 0.9055 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.9055 Epoch 11/20 49/49 - 0s - loss: 3.1179e-04 - accuracy: 1.0000 - binary_crossentropy: 3.1179e-04 - val_loss: 0.9202 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9202 Epoch 12/20 49/49 - 0s - loss: 2.6851e-04 - accuracy: 1.0000 - binary_crossentropy: 2.6851e-04 - val_loss: 0.9358 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9358 Epoch 13/20 49/49 - 0s - loss: 2.3418e-04 - accuracy: 1.0000 - binary_crossentropy: 2.3418e-04 - val_loss: 0.9482 - val_accuracy: 0.8399 - val_binary_crossentropy: 0.9482 Epoch 14/20 49/49 - 0s - loss: 2.0480e-04 - accuracy: 1.0000 - binary_crossentropy: 2.0480e-04 - val_loss: 0.9615 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9615 Epoch 15/20 49/49 - 0s - loss: 1.8099e-04 - accuracy: 1.0000 - binary_crossentropy: 1.8099e-04 - val_loss: 0.9732 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9732 Epoch 16/20 49/49 - 0s - loss: 1.6065e-04 - accuracy: 1.0000 - binary_crossentropy: 1.6065e-04 - val_loss: 0.9851 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9851 Epoch 17/20 49/49 - 0s - loss: 1.4336e-04 - accuracy: 1.0000 - binary_crossentropy: 1.4336e-04 - val_loss: 0.9966 - val_accuracy: 0.8401 - val_binary_crossentropy: 0.9966 Epoch 18/20 49/49 - 0s - loss: 1.2880e-04 - accuracy: 1.0000 - binary_crossentropy: 1.2880e-04 - val_loss: 1.0070 - val_accuracy: 0.8399 - val_binary_crossentropy: 1.0070 Epoch 19/20 49/49 - 0s - loss: 1.1636e-04 - accuracy: 1.0000 - binary_crossentropy: 1.1636e-04 - val_loss: 1.0171 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0171 Epoch 20/20 49/49 - 0s - loss: 1.0553e-04 - accuracy: 1.0000 - binary_crossentropy: 1.0553e-04 - val_loss: 1.0270 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0270training dataset의 loss(손실)값과 test dataset의 loss(손실)값 시각화 1234567891011121314151617def plot_history(histories, key='binary_crossentropy'): plt.figure(figsize=(16,6)) for name, history in histories: val = plt.plot(history.epoch, history.history['val_' + key], '--', label=name.title()+' Val') plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+'Train') plt.xlabel('Epochs') plt.ylabel(key.replace('-', ' ').title()) plt.legend() plt.xlim([0, max(history.epoch)])plot_history([('base', base_history), ('smaller', small_history), ('bigger', big_history)]) big model의 경우 에포크가 시작하자마자 과대적합(Overfitting)이 일어나는 것을 알 수 있고 생각보다 심하게 이뤄집니다. 모델 네트워크의 용량이 많을수록 과대적합이 될 확률이 커집니다.(훈련 loss값과 검증 loss값 사이에 큰 차이가 발생) 과대적합(Overfitting)을 방지하기 위한 전략 - 가중치 규제하기 1. 훈련 데이터와 네트워크 구조가 주어졌을 때, 데이터를 설명할 수 있는 가중치의 조합을 간단하게! 2. 모델 파라미터의 분포를 봤을 때 엔트로피가 작은 모델(적은 파라미터를 가지는 모델), 즉 과대적합을 완화시키는 일반적인 방법은 가중치가 작은 값을 가지도록 네트워크의 복잡도에 제약을 가하는 것이라고 할 수 있습니다. &apos;가중치 규제(Weight regularization) * L1 규제는 가중치의 절댓값에 비례하는 비용이 추가 * L2 규제는 가중치의 제곱에 비례하는 비용이 추가, 신경망에서는 L2규제를 가중치 감쇠(weight decay)라고도 합니다.1234567891011121314l2_model = keras.Sequential([ keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'), keras.layers.Dense(1, activation='sigmoid')])l2_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])l2_history = l2_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 1s - loss: 0.6362 - accuracy: 0.6929 - binary_crossentropy: 0.5927 - val_loss: 0.4927 - val_accuracy: 0.8113 - val_binary_crossentropy: 0.4513 Epoch 2/20 49/49 - 0s - loss: 0.4164 - accuracy: 0.8462 - binary_crossentropy: 0.3749 - val_loss: 0.3873 - val_accuracy: 0.8545 - val_binary_crossentropy: 0.3460 Epoch 3/20 49/49 - 0s - loss: 0.3636 - accuracy: 0.8669 - binary_crossentropy: 0.3230 - val_loss: 0.3708 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3312 Epoch 4/20 49/49 - 0s - loss: 0.3498 - accuracy: 0.8721 - binary_crossentropy: 0.3113 - val_loss: 0.3687 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3312 Epoch 5/20 49/49 - 0s - loss: 0.3440 - accuracy: 0.8726 - binary_crossentropy: 0.3073 - val_loss: 0.3640 - val_accuracy: 0.8602 - val_binary_crossentropy: 0.3283 Epoch 6/20 49/49 - 0s - loss: 0.3393 - accuracy: 0.8760 - binary_crossentropy: 0.3044 - val_loss: 0.3622 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3281 Epoch 7/20 49/49 - 0s - loss: 0.3369 - accuracy: 0.8749 - binary_crossentropy: 0.3034 - val_loss: 0.3604 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3276 Epoch 8/20 49/49 - 0s - loss: 0.3349 - accuracy: 0.8754 - binary_crossentropy: 0.3027 - val_loss: 0.3595 - val_accuracy: 0.8595 - val_binary_crossentropy: 0.3281 Epoch 9/20 49/49 - 0s - loss: 0.3325 - accuracy: 0.8746 - binary_crossentropy: 0.3015 - val_loss: 0.3608 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3304 Epoch 10/20 49/49 - 0s - loss: 0.3332 - accuracy: 0.8744 - binary_crossentropy: 0.3031 - val_loss: 0.3599 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3304 Epoch 11/20 49/49 - 0s - loss: 0.3305 - accuracy: 0.8750 - binary_crossentropy: 0.3012 - val_loss: 0.3563 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3274 Epoch 12/20 49/49 - 0s - loss: 0.3290 - accuracy: 0.8748 - binary_crossentropy: 0.3004 - val_loss: 0.3554 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3272 Epoch 13/20 49/49 - 0s - loss: 0.3272 - accuracy: 0.8752 - binary_crossentropy: 0.2991 - val_loss: 0.3526 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247 Epoch 14/20 49/49 - 0s - loss: 0.3251 - accuracy: 0.8760 - binary_crossentropy: 0.2972 - val_loss: 0.3522 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3243 Epoch 15/20 49/49 - 0s - loss: 0.3232 - accuracy: 0.8759 - binary_crossentropy: 0.2953 - val_loss: 0.3547 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3268 Epoch 16/20 49/49 - 0s - loss: 0.3214 - accuracy: 0.8770 - binary_crossentropy: 0.2936 - val_loss: 0.3522 - val_accuracy: 0.8601 - val_binary_crossentropy: 0.3246 Epoch 17/20 49/49 - 0s - loss: 0.3201 - accuracy: 0.8781 - binary_crossentropy: 0.2926 - val_loss: 0.3512 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3238 Epoch 18/20 49/49 - 0s - loss: 0.3194 - accuracy: 0.8766 - binary_crossentropy: 0.2921 - val_loss: 0.3544 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3271 Epoch 19/20 49/49 - 0s - loss: 0.3180 - accuracy: 0.8772 - binary_crossentropy: 0.2908 - val_loss: 0.3509 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3238 Epoch 20/20 49/49 - 0s - loss: 0.3167 - accuracy: 0.8768 - binary_crossentropy: 0.2896 - val_loss: 0.3491 - val_accuracy: 0.8608 - val_binary_crossentropy: 0.3221123plot_history([('base', base_history), ('L2', l2_history) ]) 결과에서 보듯이 모델 파라미터의 개수는 똑같지만 L2규제를 적용한 모델이 base model보다 과대적합에 훨씬 잘 견디고 있는 것을 볼 수 있습니다. - dropout 추가하기 * 신경망에서 쓰이는 가장 효과적이고 널리 사용하는 규제 기법중 하나입니다. * dropout은 층을 이용해 네트워크에 추가할 수 있습니다. 두 개의 층에 dropout 규제를 추가하여 과대적합이 얼마나 감소하는지 알아 보겠습니다.1234567891011121314dpt_model = keras.Sequential([ keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dropout(0.5), keras.layers.Dense(16, activation='relu'), keras.layers.Dropout(0.5), keras.layers.Dense(1, activation='sigmoid')])dpt_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])dpt_history = dpt_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 1s - loss: 0.6841 - accuracy: 0.5583 - binary_crossentropy: 0.6841 - val_loss: 0.6280 - val_accuracy: 0.7269 - val_binary_crossentropy: 0.6280 Epoch 2/20 49/49 - 0s - loss: 0.5848 - accuracy: 0.6974 - binary_crossentropy: 0.5848 - val_loss: 0.4655 - val_accuracy: 0.8180 - val_binary_crossentropy: 0.4655 Epoch 3/20 49/49 - 0s - loss: 0.4784 - accuracy: 0.7861 - binary_crossentropy: 0.4784 - val_loss: 0.3797 - val_accuracy: 0.8453 - val_binary_crossentropy: 0.3797 Epoch 4/20 49/49 - 0s - loss: 0.4250 - accuracy: 0.8195 - binary_crossentropy: 0.4250 - val_loss: 0.3453 - val_accuracy: 0.8510 - val_binary_crossentropy: 0.3453 Epoch 5/20 49/49 - 0s - loss: 0.3931 - accuracy: 0.8381 - binary_crossentropy: 0.3931 - val_loss: 0.3338 - val_accuracy: 0.8548 - val_binary_crossentropy: 0.3338 Epoch 6/20 49/49 - 0s - loss: 0.3758 - accuracy: 0.8480 - binary_crossentropy: 0.3758 - val_loss: 0.3299 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3299 Epoch 7/20 49/49 - 0s - loss: 0.3600 - accuracy: 0.8544 - binary_crossentropy: 0.3600 - val_loss: 0.3224 - val_accuracy: 0.8612 - val_binary_crossentropy: 0.3224 Epoch 8/20 49/49 - 0s - loss: 0.3493 - accuracy: 0.8607 - binary_crossentropy: 0.3493 - val_loss: 0.3227 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3227 Epoch 9/20 49/49 - 0s - loss: 0.3442 - accuracy: 0.8605 - binary_crossentropy: 0.3442 - val_loss: 0.3226 - val_accuracy: 0.8618 - val_binary_crossentropy: 0.3226 Epoch 10/20 49/49 - 0s - loss: 0.3317 - accuracy: 0.8674 - binary_crossentropy: 0.3317 - val_loss: 0.3230 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3230 Epoch 11/20 49/49 - 0s - loss: 0.3267 - accuracy: 0.8691 - binary_crossentropy: 0.3267 - val_loss: 0.3247 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247 Epoch 12/20 49/49 - 0s - loss: 0.3242 - accuracy: 0.8695 - binary_crossentropy: 0.3242 - val_loss: 0.3261 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3261 Epoch 13/20 49/49 - 0s - loss: 0.3153 - accuracy: 0.8721 - binary_crossentropy: 0.3153 - val_loss: 0.3289 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3289 Epoch 14/20 49/49 - 0s - loss: 0.3092 - accuracy: 0.8742 - binary_crossentropy: 0.3092 - val_loss: 0.3294 - val_accuracy: 0.8573 - val_binary_crossentropy: 0.3294 Epoch 15/20 49/49 - 0s - loss: 0.3103 - accuracy: 0.8772 - binary_crossentropy: 0.3103 - val_loss: 0.3312 - val_accuracy: 0.8576 - val_binary_crossentropy: 0.3312 Epoch 16/20 49/49 - 0s - loss: 0.3010 - accuracy: 0.8815 - binary_crossentropy: 0.3010 - val_loss: 0.3363 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3363 Epoch 17/20 49/49 - 0s - loss: 0.3010 - accuracy: 0.8788 - binary_crossentropy: 0.3010 - val_loss: 0.3338 - val_accuracy: 0.8570 - val_binary_crossentropy: 0.3338 Epoch 18/20 49/49 - 0s - loss: 0.2975 - accuracy: 0.8824 - binary_crossentropy: 0.2975 - val_loss: 0.3343 - val_accuracy: 0.8564 - val_binary_crossentropy: 0.3343 Epoch 19/20 49/49 - 0s - loss: 0.2923 - accuracy: 0.8823 - binary_crossentropy: 0.2923 - val_loss: 0.3417 - val_accuracy: 0.8556 - val_binary_crossentropy: 0.3417 Epoch 20/20 49/49 - 0s - loss: 0.2910 - accuracy: 0.8830 - binary_crossentropy: 0.2910 - val_loss: 0.3452 - val_accuracy: 0.8560 - val_binary_crossentropy: 0.3452검증 고고 123plot_history([('base', base_history), ('dropout', dpt_history) ]) 1234plot_history([('base', base_history), ('dropout', dpt_history), ('L2', l2_history) ]) 과대적합을 방지하기 위한 결론1. 더 많은 훈련 데이터를 학습시킨다. 2. 네트워크의 용량을 줄인다. (ex. Dense(16 ..) 3. 가중치 규제를 추가한다. (L2) 4. 드롭아웃을 추가한다.12","link":"/2020/08/25/%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9-Overfitting-%EA%B3%BC-%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9-Underfitting/"},{"title":"데이터프레임 병합하기(merge, concat)","text":"복수의 데이터프레임을 하나로 만드는 과정을 만져보자 12df1 = pd.DataFrame({'국어':[87, 69], '수학':[77,96]}, index=['홍길동', '임꺽정'])df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 홍길동 87 77 임꺽정 69 96 12df2 = pd.DataFrame({'국어':[82,81], '영어':[86,90]}, index=['전봉준','장길산'])df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 영어 전봉준 82 86 장길산 81 90 12df3 = pd.DataFrame({'국어':[82,81], '영어':[86,90]}, index=['전봉준','장길산'])df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 영어 전봉준 82 86 장길산 81 90 concat함수세로로 병합하기(그냥 아래로 붙이는거) 데이터프레임끼리 컬럼이 달라도 된다. (없는 컬럼엔 NaN값으로 알아서 채워짐) 근데, 데이터프레임간에 컬럼이 서로 다르면 sort=False를 넣어줘야 경고가 발생하지 않아 [sort 파라미터는 컬럼 이름을 정렬해주는 옵션]12df_concat1 = pd.concat([df1, df2, df3], sort=False)df_concat1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 영어 홍길동 87 77.0 NaN 임꺽정 69 96.0 NaN 전봉준 82 NaN 86.0 장길산 81 NaN 90.0 전봉준 82 NaN 86.0 장길산 81 NaN 90.0 가로로 병합하기 (위에 언급했듯이 axis=1 옵션만 주면 되)12df_concat2 = pd.concat([df1, df2, df3], sort=False, axis=1)df_concat2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 국어 영어 국어 영어 홍길동 87.0 77.0 NaN NaN NaN NaN 임꺽정 69.0 96.0 NaN NaN NaN NaN 전봉준 NaN NaN 82.0 86.0 82.0 86.0 장길산 NaN NaN 81.0 90.0 81.0 90.0 쉽지?? 넘어가도록 할게 12 merge함수공통 컬럼을 기준으로 병합하기 가능123pd.merge(df1, df2)df1이랑 df2는 서로 겹치는 게 없어서 컬럼만 나오네 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 영어 1234pd.merge(df1, df2, how='outer')서로 겹치지 않는 부분은 그냥 NaN으로 채우면서 서로 병합하는거야방법은 how = 'outer' 파라미터를 쓰면 돼 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 영어 0 87 77.0 NaN 1 69 96.0 NaN 2 82 NaN 86.0 3 81 NaN 90.0 how 파라미터는 outer, inner만 있는게 아니야. left, right도 있어1234pd.merge(df1, df2, how='left')왼쪽꺼를 기준으로 놓고, 오른쪽의 데이터를 가져와서 합병시키되, 오른쪽에서 가져올 데이터가 없는 자리에는 NaN값으로 채워 넣어줘 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 영어 0 87 77 NaN 1 69 96 NaN 123pd.merge(df1, df2, how='right')마찬가지로 이거는 오른쪽을 기준으로 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 국어 수학 영어 0 82 NaN 86 1 81 NaN 90 merge 함수와 비슷한 기능의 join 이라는 함수도 있어근데 join함수는 merge랑 완전 같은 기능이라 따로 정리는 안할거야이러한 기능이 필요할 땐 merge함수를 쓰면 되지 12 12 중복되는 데이터가 존재하는 경우의 열단위 병합(merge) 어떻게든 조합을 만들어내니까 너무 걱정마 12df_1 = pd.DataFrame({'아이디':['a','b','c','d'], '결제금액':[1000, 1200, 1700, 3200]})df_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 아이디 결제금액 0 a 1000 1 b 1200 2 c 1700 3 d 3200 12df_2 = pd.DataFrame({'아이디':['a','b','c','d'], '적립금':[120, 1700, 200, 320]})df_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 아이디 적립금 0 a 120 1 b 1700 2 c 200 3 d 320 1pd.merge(df_1, df_2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 아이디 결제금액 적립금 0 a 1000 120 1 b 1200 1700 2 c 1700 200 3 d 3200 320 일단 기본 merge를 시켜서 모든 경우의수를 다 봐그리고 나서 수정을 하든말든 오키? 겹치는 컬럼이 2개 이상인 경우에는?간단해. 이럴때는 ‘on’ 이라는 파라미터를 사용하면 on값을 기준으로 merge가 되 1234df_a = pd.DataFrame({'고객명':['길동','길산'], '데이터':['2000','1700'], '날짜':['2020-05-08', '2020-06-08']})df_a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터 날짜 0 길동 2000 2020-05-08 1 길산 1700 2020-06-08 1234df_b = pd.DataFrame({'고객명':['길동','길산'], '데이터':['21세','17세'], })df_b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터 0 길동 21세 1 길산 17세 1pd.merge(df_a, df_b) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터 날짜 merge가 되지 않아. 왜냐믄 공통되는 컬럼이 2개가 되니깐 둘 다 참조를 하게되서.. 두 컬럼 사이에 교집합이 없으니 merge를 해도 소용이 없는거야 1pd.merge(df_a, df_b, on='고객명') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터_x 날짜 데이터_y 0 길동 2000 2020-05-08 21세 1 길산 1700 2020-06-08 17세 자 on 파라미터로 기준을 고객명으로 줬더니 출력이 됐어.이거를 보면 알 수 있듯이, 기준열은 아니면서 이름이 같은 열인 경우에는 _x 와 _y가 붙어 12 _x 랑 _y가 좀 그렇지? 그러면 컬럼명을 변경을 하자 123xx = pd.merge(df_a, df_b, on='고객명')xx = xx.rename(columns={'데이터_x': '금액', '데이터_y':'나이'})xx .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 금액 날짜 나이 0 길동 2000 2020-05-08 21세 1 길산 1700 2020-06-08 17세 Good Job! 12 공통 컬럼은 존재하지 않지만, 우리가 이름이 다른 두 컬럼을 지정해서 merge하라고 명령 할수도 있어1df_a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터 날짜 0 길동 2000 2020-05-08 1 길산 1700 2020-06-08 12df_b = df_b.rename(columns={'고객명':'이름'})df_b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 이름 데이터 0 길동 21세 1 길산 17세 자 해보자 12 12merged = pd.merge(df_a, df_b, left_on='고객명', right_on='이름')merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터_x 날짜 이름 데이터_y 0 길동 2000 2020-05-08 길동 21세 1 길산 1700 2020-06-08 길산 17세 왼쪽의 고객명과 오른쪽의 이름이 같은 데이터를 병합하라는 의미네. ㅇㅋ 이렇게 되는구나. 그럼 이제 겹치는 컬럼을 지우자12merged.drop('이름',axis=1, inplace=True)merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 고객명 데이터_x 날짜 데이터_y 0 길동 2000 2020-05-08 21세 1 길산 1700 2020-06-08 17세 12 ??? : merge는 항상 컬럼 기준이야?? 오~ 너 참 똑똑이구나? 12 Index를 기준으로 당연히 merge가 되 left_index, right_index 파라미터를 이용하면되는데,, 일단 해보자고 12 12역사 = pd.DataFrame({'역사':[90,82]}, index=['필구','봉구'])역사 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 필구 90 봉구 82 12수학 = pd.DataFrame({'수학':[81,92]}, index=['필구','맹구'])수학 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 수학 필구 81 맹구 92 12역사수학 = pd.merge(역사, 수학, left_index=True, right_index=True)역사수학 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 수학 필구 90 81 두 데이터에서 겹치는 index인 필구만 잡아서 추가해주네 ㅇㅋ? 12역사수학 = pd.merge(역사, 수학, left_index=True, right_index=True, how='outer')역사수학 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 수학 맹구 NaN 92.0 봉구 82.0 NaN 필구 90.0 81.0 12 그러면 이런 경우에는 어떻게 해야하는 거야? 12역사 = pd.DataFrame({'역사':[90,82]}, index=['필구','봉구'])역사 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 필구 90 봉구 82 12수학 = pd.DataFrame({'수학':[81,92],'이름':['필구','봉구']})수학 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 수학 이름 0 81 필구 1 92 봉구 이 두개를 merge하고 싶은데,서로 참조시킬 것이 하나는 index에 있고, 다른 하나는 column으로 갖고 있잖아.. 이럴때 바로 left/right_index와 left/right_on을 응용해서 쉐킷쉐킷! 12 12역사수학_ = pd.merge(역사,수학, left_index=True, right_on='이름')역사수학_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 수학 이름 0 90 81 필구 1 82 92 봉구 정리만 해주고 끝내자 힘들다!123역사수학_.index = 역사수학_['이름'].values역사수학_.drop('이름', axis=1, inplace=True)역사수학_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 역사 수학 필구 90 81 봉구 82 92","link":"/2020/05/08/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%EB%B3%91%ED%95%A9%ED%95%98%EA%B8%B0-merge-concat/"},{"title":"파이썬에서 설치되어있는 라이브러리 버전 체크하기","text":"파이썬을 사용하다 보면 사용하고자 하는 라이브러리 버전에 따라 같은 라이브러리임에도 불구하고 명령어가 다른 경우가 있어. 그럴 경우 설치된 라이브러리의 버전을 알아야 거기에 맞게 진행을 할 수 있겠지? 이때, 설치한 라이브러리들의 버전이 무엇인지부터 먼저 확인해보자! 1pip freeze anaconda-client==1.7.2 anaconda-navigator==1.9.12 appnope==0.1.0 asn1crypto==1.3.0 attrs==19.3.0 autopep8==1.5.2 autopy==4.0.0 backcall==0.1.0 backports.functools-lru-cache==1.6.1 backports.tempfile==1.0 backports.weakref==1.0.post1 beautifulsoup4==4.6.0 bleach==3.1.0 boto==2.49.0 boto3==1.13.12 botocore==1.16.12 branca==0.4.0 bs4==0.0.1 cachetools==4.0.0 certifi==2020.4.5.1 cffi==1.14.0 chardet==3.0.4 chart-studio==1.0.0 click==7.1.1 cloudpickle==1.3.0 clyent==1.2.2 colorama==0.4.3 colorlover==0.3.0 conda==4.8.3 conda-build==3.18.9 conda-package-handling==1.6.0 conda-verify==3.4.2 configobj==5.0.6 cryptography==2.8 cufflinks==0.17.3 cvxpy==1.0.31 cycler==0.10.0 cytoolz==0.10.1 dask==2.14.0 decorator==4.4.2 defusedxml==0.6.0 dill==0.3.1.1 docutils==0.15.2 dpkt==1.9.2 ecos==2.0.7.post1 entrypoints==0.3 et-xmlfile==1.0.1 filelock==3.0.12 finance-datareader==0.9.6 folium==0.10.1 funcy==1.14 future==0.18.2 gensim==3.8.3 glob2==0.7 google-api-core==1.16.0 google-auth==1.12.0 google-auth-oauthlib==0.4.1 google-cloud-bigquery==1.24.0 google-cloud-core==1.3.0 google-resumable-media==0.5.0 googleapis-common-protos==1.51.0 idna==2.9 imageio==2.8.0 importlib-metadata==1.5.0 inflect==4.1.0 ipykernel==5.1.4 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jaraco.itertools==5.0.0 jdcal==1.4.1 jedi==0.16.0 Jinja2==2.11.1 jmespath==0.10.0 joblib==0.14.1 JPype1==0.7.5 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-core==4.6.3 jupyterthemes==0.20.0 kiwisolver==1.2.0 konlpy==0.5.2 lesscpy==0.14.0 libarchive-c==2.8 lief==0.9.0 lxml==4.5.0 MarkupSafe==1.1.1 matplotlib==3.2.1 missingno==0.4.2 mistune==0.8.4 mkl-fft==1.0.15 mkl-random==1.1.0 mkl-service==2.3.0 more-itertools==8.2.0 mpmath==1.1.0 multiprocess==0.70.9 navigator-updater==0.2.1 nbconvert==5.6.1 nbformat==5.0.4 netifaces==0.10.9 networkx==2.4 nltk==3.5 notebook==6.0.3 numexpr==2.7.1 numpy==1.18.1 oauthlib==3.1.0 olefile==0.46 opencv-python==4.2.0.34 openpyxl==3.0.3 osqp==0.6.1 packaging==20.3 pandas==1.0.3 pandas-gbq==0.13.1 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pgmpy==0.1.10 picklable-itertools==0.1.1 pickleshare==0.7.5 Pillow==7.0.0 pkginfo==1.5.0.1 plotly==4.5.0 pluggy==0.13.1 ply==3.11 prometheus-client==0.7.1 prompt-toolkit==3.0.4 protobuf==3.11.3 psutil==5.7.0 ptyprocess==0.6.0 py==1.8.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycodestyle==2.5.0 pycosat==0.6.3 pycparser==2.20 pydata-google-auth==0.3.0 Pygments==2.6.1 pyLDAvis==2.1.2 PyMySQL==0.9.3 pyOpenSSL==19.1.0 pyparsing==2.4.7 pyrsistent==0.16.0 PySocks==1.7.1 pytest==5.4.2 python-dateutil==2.8.1 python-xlib==0.27 pytz==2019.3 PyWavelets==1.1.1 PyYAML==5.3.1 pyzmq==18.1.1 QtPy==1.9.0 regex==2020.5.14 requests==2.23.0 requests-file==1.4.3 requests-oauthlib==1.3.0 retrying==1.3.3 rsa==4.0 ruamel-yaml==0.15.87 s3transfer==0.3.3 schedule==0.6.0 scikit-image==0.16.2 scikit-learn==0.22.2.post1 scipy==1.4.1 scs==2.1.2 seaborn==0.10.0 selenium==3.141.0 Send2Trash==1.5.0 six==1.14.0 sklearn==0.0 smart-open==2.0.0 soupsieve==2.0 soynlp==0.0.493 SQLAlchemy==1.3.16 statsmodels==0.11.0 sympy==1.5.1 terminado==0.8.3 testpath==0.4.4 toolz==0.10.0 torch==1.5.0 tornado==6.0.4 tqdm==4.44.1 traitlets==4.3.3 tweepy==3.8.0 urllib3==1.25.8 wcwidth==0.1.9 webencodings==0.5.1 wget==3.2 widgetsnbextension==3.5.1 wordcloud==1.7.0 xlrd==1.2.0 xmltodict==0.12.0 zipp==2.2.0 Note: you may need to restart the kernel to use updated packages.12 12 12 12 12 12 어때? 쉽지? 물론 특정 라이브러리를 잡아서 하는 방법도 있지만, 여기서 소개하는 방법은 설치된 모든 라이브러리의 버전을 확인할 수 있는 방법이야. 파이썬 라이브러리 종류가 매우 많은데 이 명령어 하나로 모든 설치된 라이브러리의 버전을 확인할 수 있으니까 상당히 편리한 것 같네.","link":"/2020/05/19/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-%EC%84%A4%EC%B9%98%EB%90%98%EC%96%B4%EC%9E%88%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%B2%84%EC%A0%84-%EC%B2%B4%ED%81%AC%ED%95%98%EA%B8%B0/"},{"title":"데이터 전처리(Data Scaling with sklearn)","text":"데이터 스케일링이란 데이터 전처리 과정의 하나입니다. 데이터 스케일링을 해주는 이유는 데이터의 값이 너무 크거나 혹은 작은 경우에 모델 알고리즘 학습과정에서 0으로 수렴하거나 무한으로 발산해버릴 수 있기 때문입니다. 따라서, scaling은 데이터 전처리 과정에서 굉장히 중요한 과정입니다. 가볍게 살펴보도록 하겠습니다1. scale이란?1!pip install mglearn Collecting mglearn Downloading mglearn-0.1.9.tar.gz (540 kB) \u001b[K |████████████████████████████████| 540 kB 506 kB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.18.1) Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (3.2.1) Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.22.2.post1) Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.0.3) Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (7.0.0) Requirement already satisfied: cycler in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.10.0) Requirement already satisfied: imageio in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (2.8.0) Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.14.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (1.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.4.7) Requirement already satisfied: scipy&gt;=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;mglearn) (1.4.1) Requirement already satisfied: pytz&gt;=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;mglearn) (2019.3) Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from cycler-&gt;mglearn) (1.14.0) Building wheels for collected packages: mglearn Building wheel for mglearn (setup.py) ... \u001b[?25ldone \u001b[?25h Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=6bf4e55bc798dd18a2ce5a5d67e6f134dfc35d54bb51cd8020f85b838a7173b1 Stored in directory: /Users/wglee/Library/Caches/pip/wheels/f1/17/e1/1720d6dcd70187b6b6c3750cb3508798f2b1d57c9d3214b08b Successfully built mglearn Installing collected packages: mglearn Successfully installed mglearn-0.1.912import mglearnmglearn.plots.plot_scaling() (1) StandardScaler 각 feature의 평균을 0, 분산을 1로 변경합니다. 모든 특성들이 같은 스케일을 갖게 됩니다. (2) RobustScaler 모든 특성들이 같은 크기를 갖는다는 점에서 StandardScaler와 비슷하지만, 평균과 분산 대신 median과 quartile을 사용합니다. RobustScaler는 이상치에 영향을 받지 않습니다. (3) MinMaxScaler 모든 feature가 0과 1사이에 위치하게 만듭니다. 데이터가 2차원 셋일 경우, 모든 데이터는 x축의 0과 1 사이에, y축의 0과 1사이에 위치하게 됩니다. (4) Normalizer StandardScaler, RobustScaler, MinMaxScaler가 각 columns의 통계치를 이용한다면 Normalizer는 row마다 각각 정규화됩니다. Normalizer는 유클리드 거리가 1이 되도록 데이터를 조정합니다. (유클리드 거리는 두 점 사이의 거리를 계산할 때 쓰는 방법)2. Codescikit-learn에 있는 아이리스 데이터셋으로 데이터 스케일링을 해보겠습니다.12345from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitiris = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0) 데이터를 학습용과 테스트용으로 분할했습니다. scaler를 사용하기 이전에 주의해야될 점을 먼저 살펴보겠습니다. scaler는 fit과 transform 메서드를 지니고 있습니다. fit 메서드로 데이터 변환을 학습하고, transform 메서드로 실제 데이터의 스케일을 조정합니다. 이때, fit 메서드는 학습용 데이터에만 적용해야 합니다. 그 후, transform 메서드를 학습용 데이터와 테스트 데이터에 적용합니다. scaler는 fit_transform()이란 단축 메서드를 제공합니다. 학습용 데이터에는 fit_transform()메서드를 적용하고, 테스트 데이터에는 transform()메서드를 적용합니다.(1) StandardScaler code1234567from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train_scale = scaler.fit_transform(X_train)print('스케일 조정 전 features Min value : {}'.format(X_train.min(axis=0)))print('스케일 조정 전 features Max value : {}'.format(X_train.max(axis=0)))print('스케일 조정 후 features Min value : {}'.format(X_train_scale.min(axis=0)))print('스케일 조정 후 features Max value : {}'.format(X_train_scale.max(axis=0))) 스케일 조정 전 features Min value : [4.3 2.2 1.1 0.1] 스케일 조정 전 features Max value : [7.9 4.4 6.9 2.5] 스케일 조정 후 features Min value : [-1.73905934 -2.11220356 -1.37231262 -1.32054283] 스케일 조정 후 features Max value : [2.32920943 3.06374081 1.74766642 1.68511841](2) RobustScaler code1234567from sklearn.preprocessing import RobustScalerscaler = RobustScaler()X_train_scale = scaler.fit_transform(X_train)print('스케일 조정 전 features Min value : {}'.format(X_train.min(axis=0)))print('스케일 조정 전 features Max value : {}'.format(X_train.max(axis=0)))print('스케일 조정 후 features Min value : {}'.format(X_train_scale.min(axis=0)))print('스케일 조정 후 features Max value : {}'.format(X_train_scale.max(axis=0))) 스케일 조정 전 features Min value : [4.3 2.2 1.1 0.1] 스케일 조정 전 features Max value : [7.9 4.4 6.9 2.5] 스케일 조정 후 features Min value : [-1.01818182 -1.47826087 -0.82993197 -0.70588235] 스케일 조정 후 features Max value : [1.6 2.34782609 0.74829932 0.70588235](3) MinMaxScaler code1234567from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)print('스케일 조정 전 features Min value : {}'.format(X_train.min(axis=0)))print('스케일 조정 전 features Max value : {}'.format(X_train.max(axis=0)))print('스케일 조정 후 features Min value : {}'.format(X_train_scale.min(axis=0)))print('스케일 조정 후 features Max value : {}'.format(X_train_scale.max(axis=0))) 스케일 조정 전 features Min value : [4.3 2.2 1.1 0.1] 스케일 조정 전 features Max value : [7.9 4.4 6.9 2.5] 스케일 조정 후 features Min value : [0. 0. 0. 0.] 스케일 조정 후 features Max value : [1. 1. 1. 1.](4) Normalizer code1234567from sklearn.preprocessing import Normalizer scaler = Normalizer()X_train_scale = scaler.fit_transform(X_train)print('스케일 조정 전 features Min value : {}'.format(X_train.min(axis=0)))print('스케일 조정 전 features Max value : {}'.format(X_train.max(axis=0)))print('스케일 조정 후 features Min value : {}'.format(X_train_scale.min(axis=0)))print('스케일 조정 후 features Max value : {}'.format(X_train_scale.max(axis=0))) 스케일 조정 전 features Min value : [4.3 2.2 1.1 0.1] 스케일 조정 전 features Max value : [7.9 4.4 6.9 2.5] 스케일 조정 후 features Min value : [0.67017484 0.2383917 0.16783627 0.0147266 ] 스케일 조정 후 features Max value : [0.86093857 0.60379053 0.63265489 0.2553047 ]3. 적용해보기의사결정나무(decisionTree)로 breat_cancer 데이터셋을 학습해보겠습니다. 먼저, 데이터 스케일링을 적용하지 않은 채 진행하겠습니다.12345678from sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_breast_cancercancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)tree = DecisionTreeClassifier(criterion='entropy', max_depth=1)tree.fit(X_train, y_train)print('test accuracy : %3f' %tree.score(X_test, y_test)) test accuracy : 0.881119 다음은 데이터를 MinMaxScaler로 스케일을 조정하고 의사결정나무 모델로 학습시켜보겠습니다.1234567from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)X_test_scale = scaler.fit_transform(X_test)tree.fit(X_train_scale, y_train)print('test accuracy : %3f' %tree.score(X_test_scale, y_test)) test accuracy : 0.860140 비슷하게 나온 것을 알 수 있습니다.12 12","link":"/2020/06/13/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-Data-Scaling-with-sklearn/"}],"tags":[{"name":"git","slug":"git","link":"/tags/git/"},{"name":"fastcampus","slug":"fastcampus","link":"/tags/fastcampus/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[]}