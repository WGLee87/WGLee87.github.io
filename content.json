{"pages":[],"posts":[{"title":"200420-ê³µë¶€ë‚´ìš©","text":"íŒŒì´ì¬(Python) í•™ìŠµ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì½”ë”©í…ŒìŠ¤íŠ¸ ì—°ìŠµ startswith() startswithë€ ì•ì˜ ë³€ìˆ˜ì™€ valueê°’ì´ ë’¤ì˜ ë³€ìˆ˜ì˜ valueì¤‘ ê°™ì€ ë‹¨ì–´ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒì´ ìˆìœ¼ë©´ True or Falseë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ kë¦¬ê·¸1 ë°ì´í„° ì „ì²˜ë¦¬ì‘ì—… ì„ ìˆ˜ ë³„ í¼í¬ë¨¼ìŠ¤ ë°ì´í„°ì˜ ëˆ„ì í•© ì—°ìŠµ reduce, ë°˜ë³µë¬¸ì„ í†µí•´ ì½”ë”©í•¨ (ì‹¤íŒ¨) numpy ë‚˜ pandas dataframe ì˜ functionì¤‘ cumsum()ì´ë¼ëŠ” ëˆ„ì í•© ê¸°ëŠ¥ì´ ìˆìŒ cumsum(). ì¢‹ì€ ê²ƒì„ ë˜ í•˜ë‚˜ ì°¾ì•˜ê³ , ê³µë¶€í–ˆë‹¤. def í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ apply ê¸°ëŠ¥ì„ í†µí•´ ì ìš©ì‹œí‚¤ëŠ” ë°©ë²•ì€ ì¡°ê¸ˆ ë” ì—°ìŠµì´ í•„ìš”í• ë“¯ í•˜ë‹¤. ê°€ìƒí™˜ê²½ì—ì„œì˜ ì£¼í”¼í„° ì‹¤í–‰ ê³„ì†ì ìœ¼ë¡œ ì‹¤íŒ¨ì¤‘ í¬íŠ¸ ë“±ë¡ê¹Œì§€ í•˜ê³  ì—´ì—ˆëŠ”ë° ì—´ë¦¬ì§€ ì•ŠìŒ ê³„ì†ì ìœ¼ë¡œ. ê°•ì‚¬ë‹˜ì˜ ë„ì›€ì´ ì ˆì‹¤í•¨ ìˆ˜í•™(math) í•™ìŠµ í–‰ë ¬ì˜ ì„±ì§ˆ í–‰ë ¬ì˜ í¬ê¸° ë° ë¶€í”¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 3ê°€ì§€ ì„±ì§ˆ(ë†ˆ(norm), ëŒ€ê°í•©(trace), í–‰ë ¬ì‹(determinants) ë†ˆ(norm)ì€ í•­ìƒ 0ë³´ë‹¤ ê°™ê±°ë‚˜ í¬ë‹¤. ë²¡í„°ì˜ ë†ˆ(norm)ì˜ ì„±ì§ˆì€ ë²¡í„°ì˜ ë†ˆì˜ ì œê³±ì´ ë²¡í„°ì˜ ì œê³±í•©ê³¼ ê°™ë‹¤. ëŒ€ê°í•©(trace)ì€ ì •ë°©í•¸ë ¬ì— ëŒ€í•´ì„œë§Œ ì •ì˜ ë¨. ì–‘ì˜ ì •ë¶€í˜¸ì™€ ì–‘ì˜ ì¤€ì •ë¶€í˜¸ ëŒ€ì¹­í–‰ë ¬ì´ë€ Aë¼ëŠ” í–‰ë ¬ì˜ ì „ì¹˜í–‰ë ¬ê³¼ ì›ë˜ì˜ Aê°€ ê°™ì€ í–‰ë ¬. ì¢Œí‘œì™€ ë³€í™˜ ì¢Œí‘œë²¡í„°ì˜ ê¸¸ì´ëŠ” ë†ˆ(norm)ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìŒ. ìŠ¤ì¹¼ë¼ì™€ ì¢Œí‘œë²¡í„°ë¥¼ ê³±í•˜ë©´ ë°©í–¥ì€ ê·¸ëŒ€ë¡œ, ì‹¤ìˆ˜(ìŠ¤ì¹¼ë¼)ì˜ í¬ê¸° ë§Œí¼ ê¸¸ì´ê°€ ê¸¸ì–´ì§. ì—¬ëŸ¬ê°œì˜ ë²¡í„°ë¥¼ ìŠ¤ì¹¼ë¼ê°’ì„ ê³±í•œ í›„ ë”í•œ ê²ƒ - ì„ í˜•ì¡°í•©. ì¢Œí‘œì—ì„œ ë²¡í„°ì˜ ì°¨ëŠ” ì¢Œí‘œì˜ aì™€ bì„ ë¹¼ë©´ ë¨ (a-b) ìœ í´ë¦¬ë“œ ê±°ë¦¬ sin@ì€ @ê°€ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§€ê³ , @ê°€ 90ë„ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ 1ì— ê°€ê¹Œì›Œì§„ë‹¤. cos@ì€ @ê°€ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ 1ì— ê°€ê¹Œì›Œì§€ê³ , @ê°€ 90ë„ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ 0ì— ê°€ê¹Œì›Œì§„ë‹¤. aì™€ 90ë„ ë§Œë‚˜ëŠ” bì˜ ì¢Œí‘œë¥¼ ì§êµë¼ê³  í•œë‹¤. ex) 1,0 ê³¼ 0,1 ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë‘ ë²¡í„°ì˜ ë°©í–¥ì´ ë¹„ìŠ·í• ìˆ˜ë¡ ë²¡í„°ê°€ ë¹„ìŠ·í•˜ë‹¤ê³  ê°„ì£¼ &gt; ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ 1ì´ë©´ ê°€ì¥ ìœ ì‚¬í•˜ë‹¤â€™ë¼ê³  ë´„. ê³ ìœ³ê°’ [ì •ë¦¬] ì¤‘ë³µëœ ê³ ìœ³ê°’ì„ ê°ê° ë³„ê°œë¡œ ìƒˆã…ã…‡ê°í•˜ê³  ë³µì†Œìˆ˜ì¸ ê³ ìœ³ê°’ë„ ê³ ë ¤í•œë‹¤ë©´ nì°¨ì› ì •ë°©í–‰ë ¬ì˜ ê³ ìœ³ê°’ì€ í•­ìƒ nê°œì´ë‹¤. ì´ìƒ. (ì „í˜€ ì´í•´ê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤. í°ì¼ì´ë‹¤)title: 200420-ê³µë¶€ë‚´ìš©date: 2020-04-20 18:58:01tags:","link":"/2020/04/20/200420-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"200418-ê³µë¶€ë‚´ìš©","text":"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì½”ë”©í…ŒìŠ¤íŠ¸ ë¬¸ì œí’€ì´ kë¦¬ê·¸1 ë°ì´í„°ë¡œ pandas ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬ ì‘ì—… ì—°ìŠµ ê°€ìƒì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ì—°ê²°ì´ ì•ˆë¨) ìˆ˜í•™ ì˜¤ì „ì— ê°•ì‚¬ë‹˜ì˜ ê°•ì˜ ë„ì €íˆ ì´í•´ê°€ ë˜ì§ˆ ì•Šì•„ì„œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì°¨ê·¼ì°¨ê·¼ ê³µë¶€ì‹œì‘ ìˆ˜ì—´ê³¼ ìŠ¤ì¹¼ë¼, ë²¡í„°, í–‰ë ¬ ë“± ê³µë¶€ (ì´ì •ë„ëŠ” ì´í•´) ë‚´ì¼ë„ ìˆ˜í•™ ê³µë¶€ ì˜ˆì • (í°ì¼ë‚¨) gitê³¼ ë¸”ë¡œê·¸ì— ì˜¤ëŠ˜ì˜ ê³µë¶€ë‚´ìš© ì—…ë¡œë“œ title: 200418-ê³µë¶€ë‚´ìš©date: 2020-04-18 21:10:47tags:","link":"/2020/04/18/200418-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"200422-ê³µë¶€ë‚´ìš©","text":"ìˆ˜í•™(math) ë°ì´í„°ë¶„ì„ì—ì„œì˜ ë¯¸,ì ë¶„ ìµœì í™”(optimize) git 2ë²ˆì§¸ ê°•ì˜(branch ê°•ì˜) ì²«ë²ˆì§¸ í˜‘ì—… ì—°ìŠµ(fork and merge) title: 200422-ê³µë¶€ë‚´ìš©date: 2020-04-22 21:00:53tags:","link":"/2020/04/22/200422-%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"DataFrame Functions","text":"ë°ì´í„° ë¶„í¬ ë³€í™˜ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ë³€ìˆ˜ê°€ íŠ¹ì • ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì„ í˜• ëª¨ë¸ì˜ ê²½ìš°, ì¢…ì†ë³€ìˆ˜ê°€ ì •ê·œë¶„í¬ì™€ ìœ ì‚¬í•  ê²½ìš° ì„±ëŠ¥ì´ ë†’ì•„ì§€ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ìì£¼ ì“°ì´ëŠ” ë°©ë²•ì€ Log, Exp, Sqrt ë“± í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë°ì´í„° ë¶„í¬ë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì´ë‹¤.123456789import mathfrom sklearn import preprocessing# íŠ¹ì • ë³€ìˆ˜ì—ë§Œ í•¨ìˆ˜ ì ìš©df['X_log'] = preprocessing.scale(np.log(df['X']+1)) # ë¡œê·¸df['X_sqrt'] = preprocessing.scale(np.sqrt(df['X']+1)) # ì œê³±ê·¼# ë°ì´í„° í”„ë ˆì„ ì „ì²´ì— í•¨ìˆ˜ ì ìš© (ë‹¨, ìˆ«ìí˜• ë³€ìˆ˜ë§Œ ìˆì–´ì•¼ í•¨)df_log = df.apply(lambda x: np.log(x+1)) ì¤‘ë³µëœ í–‰ ì œê±°ìœ„, ì•„ë˜ í–‰ì´ ëª¨ë‘ ê°™ì€ ì„±ë¶„ì„ ê°€ì§€ëŠ” í–‰ì´ ì—¬ëŸ¬ê°œ ìˆì„ë•Œ, í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ê¸°(ë°ì´í„°í”„ë ˆì„) ì¤‘ë³µëœ í–‰ì´ ì œê±°ë˜ê³  uniqueí•œ ê°’ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.123df.drop_duplicates()df.drop_duplicated() ëŠ” booleanê°’ìœ¼ë¡œ ë°˜í™˜! 12df['name'] = df['name'].apply(lambda e: e.split()[0])df['email'].str.get(i=0) ë°ì´í„°í”„ë ˆì„ì´ë‚˜ ì‹œë¦¬ì¦ˆí˜•ì‹ì—ì„œ ë¬¸ìë¥¼ ë‚˜ëˆ„ê³  [0]ë²ˆì§¸ ë¬¸ìë§Œ ê°€ì ¸ì˜¤ê¸° ë°ì´í„° í”„ë ˆì„ ëª¨ë“  ì—´ì— íŠ¹ì • ìŠ¤ì¹¼ë¼ê°’ or íŠ¹ì • ì»¬ëŸ¼.value ì—°ì‚°1df[ì»¬ëŸ¼ëª…] *= (ìŠ¤ì¹¼ë¼ê°’) / í•´ë‹¹ ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ì´ ì™€ë„ ë¨ 1df[ì»¬ëŸ¼ëª…] = df[ì»¬ëŸ¼ëª…].div(ìŠ¤ì¹¼ë¼ê°’ or ì»¬ëŸ¼, axis=0) 12 a=10, b=20, c=3 Operator Description Example + ë”í•˜ê¸° a + b 30 - ë¹¼ê¸° a - b -10 * ê³±í•˜ê¸° a * b 200 / ë‚˜ëˆ„ê¸° b / a 2.0 % ë‚˜ë¨¸ì§€ b % a 0 ** ì œê³± a ** c 1000 // ëª« a // c 3","link":"/2020/06/04/DataFrame-Functions/"},{"title":"Linear Regression with scale, categorical regression, and partial regression","text":"12345678910111213141516import matplotlibfrom matplotlib import font_manager, rcimport platformtry : if platform.system() == 'windows': # windowsì˜ ê²½ìš° font_name = font_manager.FomntProperties(fname=\"c:/Windows/Font\") rc('font', family = font_name) else: # macì˜ ê²½ìš° rc('font', family = 'AppleGothic')except : passmatplotlib.rcParams['axes.unicode_minus'] = False ìŠ¤ì¼€ì¼ë§ 12 ì´ summary reportëŠ” ì–´ì œ ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì„ í˜•íšŒê·€ë¶„ì„ì„ í•œ ê²°ê³¼ê°’ì´ì•¼. ì•„ë«ë¶€ë¶„ì˜ Cond. No. ë¼ê³  ì“°ì—¬ì ¸ ìˆëŠ” ë¶€ë¶„ì¸ë° ì¡°ê±´ìˆ˜(conditional number)ëŠ” ê°€ì¥ í° ê³ ìœ ì¹˜ì™€ ê°€ì¥ ì‘ì€ ê³ ìœ ì¹˜ì˜ ë¹„ìœ¨ì„ ëœ»í•´.12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows Ã— 14 columns ì´ê²ƒì€ ì¼ë¶€ëŸ¬ TAXë³€ìˆ˜ë¥¼ í¬ê²Œ ë§Œë“¤ì–´ ì¡°ê±´ìˆ˜ë¥¼ ì¦í­ì‹œì¼œë³¸ ë°ì´í„°ì•¼12345678import statsmodels.api as smdfX = sm.add_constant(dfx)dfX2 = dfX.copy()dfX2[\"TAX\"] *= 1e13df2 = pd.concat([dfX2, dfy], axis=1)model2 = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df2)result2 = model2.fit()print(result2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.333 Model: OLS Adj. R-squared: 0.329 Method: Least Squares F-statistic: 83.39 Date: Wed, 13 May 2020 Prob (F-statistic): 8.62e-44 Time: 16:03:11 Log-Likelihood: -1737.9 No. Observations: 506 AIC: 3484. Df Residuals: 502 BIC: 3501. Df Model: 3 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept -0.0038 0.000 -8.543 0.000 -0.005 -0.003 CRIM -0.1567 0.046 -3.376 0.001 -0.248 -0.066 ZN 0.1273 0.016 7.752 0.000 0.095 0.160 INDUS -0.1971 0.019 -10.433 0.000 -0.234 -0.160 CHAS 0.0034 0.000 12.430 0.000 0.003 0.004 NOX -0.0023 0.000 -9.285 0.000 -0.003 -0.002 RM 0.0267 0.002 14.132 0.000 0.023 0.030 AGE 0.1410 0.017 8.443 0.000 0.108 0.174 DIS -0.0286 0.004 -7.531 0.000 -0.036 -0.021 RAD 0.1094 0.018 6.163 0.000 0.075 0.144 TAX 1.077e-15 2.66e-16 4.051 0.000 5.55e-16 1.6e-15 PTRATIO -0.1124 0.011 -10.390 0.000 -0.134 -0.091 B 0.0516 0.003 19.916 0.000 0.046 0.057 LSTAT -0.6569 0.056 -11.790 0.000 -0.766 -0.547 ============================================================================== Omnibus: 39.447 Durbin-Watson: 0.863 Prob(Omnibus): 0.000 Jarque-Bera (JB): 46.611 Skew: 0.704 Prob(JB): 7.56e-11 Kurtosis: 3.479 Cond. No. 1.19e+17 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.19e+17. This might indicate that there are strong multicollinearity or other numerical problems. ì¡°ê±´ìˆ˜(Conditional No.)ê°€ 1000ì¡° ìˆ˜ì¤€ìœ¼ë¡œ ì¦ê°€í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆì§€? ì˜¤ë¥¸ìª½ ì œì¼ ìƒë‹¨ì— ë³´ì´ëŠ” R-squaredë¼ëŠ” ê°’ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ì„±ëŠ¥ì§€í‘œë„ í¬ê²Œ ê°ì†Œí•œê²ƒì„ ë³¼ ìˆ˜ ìˆì–´. R-squared ëŠ” ì´ ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•´ ëª‡ì ì¸ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ê¸°ëŠ¥ì´ë¼ê³  ë³´ë©´ ë˜(0.333ìœ¼ë¡œ ë‚˜ì™”ìœ¼ë‹ˆ 100ì  ë§Œì ì— 33.3ì ì´ë¼ëŠ” ì†Œë¦¬ì•¼) statsmodelsì—ì„œëŠ” scale() ì´ë¼ëŠ” ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¼€ì¼ë§ì„ í•  ìˆ˜ ìˆëŠ”ë°, ì´ ë°©ì‹ìœ¼ë¡œ ìŠ¤ì¼€ì¼ì„ í•˜ë©´ ìŠ¤ì¼€ì¼ë§ì— ì‚¬ìš©ëœ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì €ì¥í•˜ì˜€ë‹¤ê°€ ë‚˜ì¤‘ì— predict() ë¼ëŠ” ëª…ë ¹ì„ ì‚¬ìš©í•  ë•Œë„ ê°™ì€ ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í¸ë¦¬í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì–´. ë‹¤ë§Œ! ìŠ¤ì¼€ì¼ë§ì„ í•  ë•Œì—ëŠ” ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜, ì¦‰ ë²”ì£¼í˜• ë°ì´í„°ëŠ” ìŠ¤ì¼€ì¼ë§ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì— ì£¼ì˜ í•´ì£¼ë©´ ë˜.123456feature_names = list(boston.feature_names)feature_names.remove(\"CHAS\") feature_names = ['scale({})'.format(name) for name in feature_names] + ['CHAS']model3 = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(feature_names), data=df2)result3 = model3.fit()print(result3.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.741 Model: OLS Adj. R-squared: 0.734 Method: Least Squares F-statistic: 108.1 Date: Wed, 13 May 2020 Prob (F-statistic): 6.72e-135 Time: 16:11:05 Log-Likelihood: -1498.8 No. Observations: 506 AIC: 3026. Df Residuals: 492 BIC: 3085. Df Model: 13 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 22.3470 0.219 101.943 0.000 21.916 22.778 scale(CRIM) -0.9281 0.282 -3.287 0.001 -1.483 -0.373 scale(ZN) 1.0816 0.320 3.382 0.001 0.453 1.710 scale(INDUS) 0.1409 0.421 0.334 0.738 -0.687 0.969 scale(NOX) -2.0567 0.442 -4.651 0.000 -2.926 -1.188 scale(RM) 2.6742 0.293 9.116 0.000 2.098 3.251 scale(AGE) 0.0195 0.371 0.052 0.958 -0.710 0.749 scale(DIS) -3.1040 0.420 -7.398 0.000 -3.928 -2.280 scale(RAD) 2.6622 0.577 4.613 0.000 1.528 3.796 scale(TAX) -2.0768 0.633 -3.280 0.001 -3.321 -0.833 scale(PTRATIO) -2.0606 0.283 -7.283 0.000 -2.617 -1.505 scale(B) 0.8493 0.245 3.467 0.001 0.368 1.331 scale(LSTAT) -3.7436 0.362 -10.347 0.000 -4.454 -3.033 CHAS 2.6867 0.862 3.118 0.002 0.994 4.380 ============================================================================== Omnibus: 178.041 Durbin-Watson: 1.078 Prob(Omnibus): 0.000 Jarque-Bera (JB): 783.126 Skew: 1.521 Prob(JB): 8.84e-171 Kurtosis: 8.281 Cond. No. 10.6 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ë…ë¦½ë³€ìˆ˜ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§í•œê²ƒë§Œìœ¼ë¡œ ì¡°ê±´ìˆ˜ì˜ ìˆ˜ì¹˜ê°€ í™• ë‚´ë ¤ê°„ ê²ƒì„ ë³¼ ìˆ˜ ìˆì–´. ì–´ë•Œ? ì‰½ì§€? ì¡°ê±´ìˆ˜ë¥¼ ë  ìˆ˜ ìˆìœ¼ë©´ ë‚®ì¶°ì£¼ëŠ”ê²Œ ê° ë…ë¦½ë³€ìˆ˜ì˜ ì˜¤ì°¨ë²”ìœ„ë¥¼ ì¤„ì—¬ì¤„ ìˆ˜ ìˆë‹¤ê³  í•´. ê·¸ë˜ì„œ í° ê°’ì˜ ë°ì´í„°ë“¤ì€ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì—¬ì£¼ëŠ” ê±°ì§€.ë²”ì£¼í˜• ë…ë¦½ë³€ìˆ˜ì˜ íšŒê·€ë¶„ì„ì´ë²ˆì—ëŠ” ì—°ì†í˜• ë°ì´í„°ê°€ ì•„ë‹Œ ë²”ì£¼í˜• ë°ì´í„°ì˜ íšŒê·€ë¶„ì„ì„ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì! ë²”ì£¼í˜• ë°ì´í„°ëŠ” ì¸¡ì • ê²°ê³¼ê°€ ëª‡ ê°œì˜ ë²”ì£¼ ë˜ëŠ” í–¥ëª©ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìë£Œë¥¼ ë§í•˜ëŠ”ë° ê·¸ê²ƒì„ ìˆ«ìë¡œ í‘œí˜„í•œ ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆì–´. ì˜ˆë¥¼ ë“¤ë©´ ë‚¨ìëŠ” 1 ì—¬ìëŠ” 0 ì´ëŸ°ì‹ì´ì§€! ì•„ë¬´íŠ¼.. ì—¬ê¸°ì„œ ë‹¤ë£° í•™ìŠµì€ ê·¸ëŸ¬í•œ ë²”ì£¼í˜• ë…ë¦½ë³€ìˆ˜(ë°ì´í„°)ì˜ íšŒê·€ë¶„ì„ ëª¨ë¸ë§ ì‹œ ì—ëŠ” ì•ì„œ ë°°ìš´ ë”ë¯¸ë³€ìˆ˜í™”ê°€ í•„ìˆ˜ë¼ëŠ” ê±°ì•¼. í’€ë­í¬(full-rank) ë°©ì‹ê³¼ ì¶•ì†Œë­í¬(reduced-rank) ë°©ì‹ì´ ìˆëŠ”ë° í’€ë­í¬ë°©ì‹ì—ì„œëŠ” ë”ë¯¸ë³€ìˆ˜ì˜ ê°’ì„ ì›í•«ì¸ì½”ë”©(one-hot-encoding) ë°©ì‹ìœ¼ë¡œ ì§€ì •ì„ í•˜ëŠ”ê±°ì•¼ ì˜ˆë¥¼ ë“¤ì–´ì„œ.. ë‚¨ìëŠ” 1 ì´ê³  ì—¬ìëŠ” 0 ì´ë©´ ë‚¨ì : d1=1, d2=0 ì—¬ì : d1=0, d2=1 ì´ëŸ°ì‹ìœ¼ë¡œ ì“´ë‹¤ëŠ” ê±°ì§€ ì¶•ì†Œë­í¬ ë°©ì‹ì—ì„œëŠ” íŠ¹ì •í•œ í•˜ë‚˜ì˜ ë²”ì£¼ê°’ì„ ê¸°ì¤€ê°’(reference, baseline)ìœ¼ë¡œ í•˜ê³  ê¸°ì¤€ê°’ì— ëŒ€ì‘í•˜ëŠ” ë”ë¯¸ë³€ìˆ˜ì˜ ê°€ì¤‘ì¹˜ëŠ” í•­ìƒ 1ìœ¼ë¡œ ë†“ì•„ ê³„ì‚° í•˜ëŠ” ë°©ë²•ì´ì§€ ë¬´ìŠ¨ ë§ì¸ì§€ ì–´ë µì§€? ê·¸ëŸ¼ ì‹¤ ë°ì´í„°ë¡œ ì˜ˆë¥¼ ë“¤ì–´ë³´ë„ë¡ í• ê²Œ. ì•„ë˜ì˜ ë°ì´í„°ëŠ” 1920ë…„ë¶€í„° 1939ë…„ê¹Œì§€ ì˜êµ­ ë…¸íŒ…í—˜ ì§€ì—­ì˜ ê¸°ì˜¨ì„ ë‚˜íƒ€ë‚¸ ë°ì´í„°ì•¼. ì´ ë°ì´í„°ì—ì„œ ë…ë¦½ ë³€ìˆ˜ëŠ” ì›”(monath)ì´ë©° ë²”ì£¼ê°’ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ í• ê±°ì•¼ ê·¸ë¦¬ê³  valueë¡œ í‘œê¸°ëœ ê°’ì´ ì¢…ì†ë³€ìˆ˜ì¸ í•´ë‹¹ ì›”ì˜ í‰ê·  ê¸°ì˜¨ì´ë¼ê³  í•  ìˆ˜ ìˆì§€. ë¶„ì„ì˜ ëª©ì ì€ ë…ë¦½ë³€ìˆ˜ì¸ ì›” ê°’ì„ ì´ìš©í•˜ì—¬ ì¢…ì†ë³€ìˆ˜ì¸ ì›” í‰ê·  ê¸°ì˜¨ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ì•¼.123456789101112131415161718import datetimefrom calendar import isleapdef convert_partial_year(number): #ì—° ë‹¨ìœ„ ìˆ«ìì—ì„œ ë‚ ì§œë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œ year = int(number) d = datetime.timedelta(days=(number - year) * (365 + isleap(year))) day_one = datetime.datetime(year, 1, 1) date = d + day_one return datedf_nottem = sm.datasets.get_rdataset(\"nottem\").datadf_nottem[\"date0\"] = df_nottem[[\"time\"]].applymap(convert_partial_year)df_nottem[\"date\"] = pd.DatetimeIndex(df_nottem[\"date0\"]).round('60min') + datetime.timedelta(seconds=3600*24)df_nottem[\"month\"] = df_nottem[\"date\"].dt.strftime(\"%m\").astype('category')del df_nottem[\"date0\"], df_nottem[\"date\"]df_nottem .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } time value month 0 1920.000000 40.6 01 1 1920.083333 40.8 02 2 1920.166667 44.4 03 3 1920.250000 46.7 04 4 1920.333333 54.1 05 ... ... ... ... 235 1939.583333 61.8 08 236 1939.666667 58.2 09 237 1939.750000 46.7 10 238 1939.833333 46.6 11 239 1939.916667 37.8 12 240 rows Ã— 3 columns ë°•ìŠ¤í”Œë¡¯ì„ í†µí•´ í•´ë‹¹ ì›”ì— ì¢…ì†ë³€ìˆ˜ë°ì´í„°ê°€ ì–´ë””ì— ì£¼ë¡œ ëª¨ì—¬ìˆëŠ”ì§€ í™•ì¸ì„ í•´ë³´ëŠ”ê±°ì•¼12df_nottem.boxplot(\"value\", \"month\")plt.show() %% ë²”ì£¼í˜• ë…ë¦½ë³€ìˆ˜ì˜ ê²½ìš° ì•ì„œ ì‹œí–‰í•œ íšŒê·€ë¶„ì„ì—ì„œ ì¶”ê°€í•´ì¤€ ìƒìˆ˜í•­(add_constant)ì„ ì¶”ê°€í•˜ì§€ ì•Šì•„.123456# í’€ë­í¬ ë°©ì‹model = sm.OLS.from_formula(\"value ~ C(month) + 0\", df_nottem)result = model.fit()print(result.summary())# C()ë¡œ ì¹´í…Œê³ ë¦¬ë¡œ ì²˜ë¦¬ OLS Regression Results ============================================================================== Dep. Variable: value R-squared: 0.930 Model: OLS Adj. R-squared: 0.927 Method: Least Squares F-statistic: 277.3 Date: Wed, 13 May 2020 Prob (F-statistic): 2.96e-125 Time: 16:28:22 Log-Likelihood: -535.82 No. Observations: 240 AIC: 1096. Df Residuals: 228 BIC: 1137. Df Model: 11 Covariance Type: nonrobust ================================================================================ coef std err t P&gt;|t| [0.025 0.975] -------------------------------------------------------------------------------- C(month)[01] 39.6950 0.518 76.691 0.000 38.675 40.715 C(month)[02] 39.1900 0.518 75.716 0.000 38.170 40.210 C(month)[03] 42.1950 0.518 81.521 0.000 41.175 43.215 C(month)[04] 46.2900 0.518 89.433 0.000 45.270 47.310 C(month)[05] 52.5600 0.518 101.547 0.000 51.540 53.580 C(month)[06] 58.0400 0.518 112.134 0.000 57.020 59.060 C(month)[07] 61.9000 0.518 119.592 0.000 60.880 62.920 C(month)[08] 60.5200 0.518 116.926 0.000 59.500 61.540 C(month)[09] 56.4800 0.518 109.120 0.000 55.460 57.500 C(month)[10] 49.4950 0.518 95.625 0.000 48.475 50.515 C(month)[11] 42.5800 0.518 82.265 0.000 41.560 43.600 C(month)[12] 39.5300 0.518 76.373 0.000 38.510 40.550 ============================================================================== Omnibus: 5.430 Durbin-Watson: 1.529 Prob(Omnibus): 0.066 Jarque-Bera (JB): 5.299 Skew: -0.281 Prob(JB): 0.0707 Kurtosis: 3.463 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.123model = sm.OLS.from_formula(\"value ~ C(month)\", df_nottem)result = model.fit()print(result.summary()) OLS Regression Results ============================================================================== Dep. Variable: value R-squared: 0.930 Model: OLS Adj. R-squared: 0.927 Method: Least Squares F-statistic: 277.3 Date: Wed, 13 May 2020 Prob (F-statistic): 2.96e-125 Time: 16:32:20 Log-Likelihood: -535.82 No. Observations: 240 AIC: 1096. Df Residuals: 228 BIC: 1137. Df Model: 11 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 39.6950 0.518 76.691 0.000 38.675 40.715 C(month)[T.02] -0.5050 0.732 -0.690 0.491 -1.947 0.937 C(month)[T.03] 2.5000 0.732 3.415 0.001 1.058 3.942 C(month)[T.04] 6.5950 0.732 9.010 0.000 5.153 8.037 C(month)[T.05] 12.8650 0.732 17.575 0.000 11.423 14.307 C(month)[T.06] 18.3450 0.732 25.062 0.000 16.903 19.787 C(month)[T.07] 22.2050 0.732 30.335 0.000 20.763 23.647 C(month)[T.08] 20.8250 0.732 28.450 0.000 19.383 22.267 C(month)[T.09] 16.7850 0.732 22.931 0.000 15.343 18.227 C(month)[T.10] 9.8000 0.732 13.388 0.000 8.358 11.242 C(month)[T.11] 2.8850 0.732 3.941 0.000 1.443 4.327 C(month)[T.12] -0.1650 0.732 -0.225 0.822 -1.607 1.277 ============================================================================== Omnibus: 5.430 Durbin-Watson: 1.529 Prob(Omnibus): 0.066 Jarque-Bera (JB): 5.299 Skew: -0.281 Prob(JB): 0.0707 Kurtosis: 3.463 Cond. No. 12.9 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ì¶•ì†Œë­í¬ ë°©ì‹ ( +0 ì œê±°) - &apos;ê¸°ì¤€ì´ ë˜ëŠ” ë°ì´í„° ê°’ì—ì„œ ë‹¤ë¥¸ ë²”ì£¼í˜• ë°ì´í„° ê°’ì´ ì–¼ë§ˆë‚˜ ë‹¤ë¥´ëƒ&apos; ì˜ ì˜ë¯¸ë¡œ ë³´ë©´ ë¨, ì¦‰ 1ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ 2ì›”ì—ëŠ” ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ìˆëŠëƒ ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ê±°ì•¼. í’€ë­í¬ ë°©ì‹ê³¼ ì¶•ì†Œë­í¬ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì¡°ê¸ˆì€ ì•Œ ìˆ˜ ìˆê²Œëœê±°ê°™ì•„. ì´ ì´ìƒì€ ë‚´ê°€ ì´í•´ë¥¼ ëª»í–ˆê¸° ë•Œë¬¸ì— ë„˜ì–´ê°€ë„ë¡ í• ê²Œ.ë¶€ë¶„íšŒê·€ë§Œì•½ íšŒê·€ë¶„ì„ì„ í•œ í›„ì— ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ ë‹¤ì‹œ íšŒê·€ë¶„ì„ì„ í•œë‹¤ë©´ ê·¸ ì „ì— íšŒê·€ë¶„ì„ìœ¼ë¡œ êµ¬í–ˆë˜ ê°€ì¤‘ì¹˜ì˜ ê°’ì€ ë³€í• ê¹Œ ë³€í•˜ì§€ ì•Šì„ê¹Œ? ì˜ˆë¥¼ ë“¤ì–´ ğ‘¥1 ì´ë¼ëŠ” ë…ë¦½ë³€ìˆ˜ë§Œìœ¼ë¡œ íšŒê·€ë¶„ì„í•œ ê²°ê³¼ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•˜ì. ì´ ë•Œ ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜ ğ‘¥2 ë¥¼ ì¶”ê°€í•˜ì—¬ íšŒê·€ë¶„ì„ì„ í•˜ê²Œ ë˜ë©´ ì´ ë•Œ ë‚˜ì˜¤ëŠ” ğ‘¥1 ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ğ‘¤â€²1 ê°€ ì›ë˜ì˜ ğ‘¤1 ê³¼ ê°™ì„ê¹Œ ë‹¤ë¥¼ê¹Œ? ë‹µë¶€í„° ë§í•˜ìë©´ ì¼ë°˜ì ìœ¼ë¡œ ğ‘¤â€²1 ì˜ ê°’ì€ ì›ë˜ì˜ ğ‘¤1 ì˜ ê°’ê³¼ ë‹¤ë¥´ë‹¤.123456789101112from sklearn.datasets import load_bostonboston = load_boston()dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)dfX = sm.add_constant(dfX0)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfX, dfy], axis=1)model_boston = sm.OLS(dfy, dfX)result_boston = model_boston.fit()print(result_boston.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.741 Model: OLS Adj. R-squared: 0.734 Method: Least Squares F-statistic: 108.1 Date: Wed, 13 May 2020 Prob (F-statistic): 6.72e-135 Time: 18:01:08 Log-Likelihood: -1498.8 No. Observations: 506 AIC: 3026. Df Residuals: 492 BIC: 3085. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 36.4595 5.103 7.144 0.000 26.432 46.487 CRIM -0.1080 0.033 -3.287 0.001 -0.173 -0.043 ZN 0.0464 0.014 3.382 0.001 0.019 0.073 INDUS 0.0206 0.061 0.334 0.738 -0.100 0.141 CHAS 2.6867 0.862 3.118 0.002 0.994 4.380 NOX -17.7666 3.820 -4.651 0.000 -25.272 -10.262 RM 3.8099 0.418 9.116 0.000 2.989 4.631 AGE 0.0007 0.013 0.052 0.958 -0.025 0.027 DIS -1.4756 0.199 -7.398 0.000 -1.867 -1.084 RAD 0.3060 0.066 4.613 0.000 0.176 0.436 TAX -0.0123 0.004 -3.280 0.001 -0.020 -0.005 PTRATIO -0.9527 0.131 -7.283 0.000 -1.210 -0.696 B 0.0093 0.003 3.467 0.001 0.004 0.015 LSTAT -0.5248 0.051 -10.347 0.000 -0.624 -0.425 ============================================================================== Omnibus: 178.041 Durbin-Watson: 1.078 Prob(Omnibus): 0.000 Jarque-Bera (JB): 783.126 Skew: 1.521 Prob(JB): 8.84e-171 Kurtosis: 8.281 Cond. No. 1.51e+04 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.51e+04. This might indicate that there are strong multicollinearity or other numerical problems. ì´ë ‡ê²Œ ë³´ë©´ AGEëŠ” ì§‘ê°’ì„ ê²°ì •í•˜ëŠ”ë° ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤ê³  í•  ìˆ˜ ìˆì–´12sns.regplot(x=\"AGE\", y=\"MEDV\", data=df)plt.show() plot_partregress(endog, exog_i, exog_others, data=None, obs_labels=True, ret_coords=False) endog: ì¢…ì†ë³€ìˆ˜ ë¬¸ìì—´ exog_i: ë¶„ì„ ëŒ€ìƒì´ ë˜ëŠ” ë…ë¦½ë³€ìˆ˜ ë¬¸ìì—´ exog_others: ë‚˜ë¨¸ì§€ ë…ë¦½ë³€ìˆ˜ ë¬¸ìì—´ì˜ ë¦¬ìŠ¤íŠ¸ data: ëª¨ë“  ë°ì´í„°ê°€ ìˆëŠ” ë°ì´í„°í”„ë ˆì„ obs_labels: ë°ì´í„° ë¼ë²¨ë§ ì—¬ë¶€ ret_coords: ì”ì°¨ ë°ì´í„° ë°˜í™˜ ì—¬ë¶€ í•˜ì§€ë§Œ! ë‹¤ë¥¸ ë…ë¦½ë³€ìˆ˜ì— ì˜í–¥ì„ ë°›ì€ AGEê°€ ì§‘ê°’ì— ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ì— ëŒ€í•œ ë¶€ë¶„ì„ í™•ì¸í•´ë³´ë©´ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ì•„. AGE ë°ì´í„°ì— ëŒ€í•œ ë¶€ë¶„íšŒê·€ì¸ì…ˆì´ì§€.1234567others = list(set(df.columns).difference(set([\"MEDV\", \"AGE\"])))p, resids = sm.graphics.plot_partregress( \"MEDV\", \"AGE\", others, data=df, obs_labels=False, ret_coords=True)plt.show()# í¬ê²Œ ìƒê´€ì´ ì—†ë‹¤ëŠ” ê±°ë¡œ ë‚˜ì˜¤ê²Œ ë˜ sm.graphics.plot_partregress_grid ëª…ë ¹ì„ ì“°ë©´ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•œë²ˆì— ë¶€ë¶„íšŒê·€ í”Œë¡¯ì„ ê·¸ë¦´ ìˆ˜ ìˆì–´. plot_partregress_grid(result, fig) result: íšŒê·€ë¶„ì„ ê²°ê³¼ ê°ì²´ fig: plt.figure ê°ì²´1234fig = plt.figure(figsize=(8, 20))sm.graphics.plot_partregress_grid(result_boston, fig=fig)fig.suptitle(\"\")plt.show() CCPR í”Œë¡¯ CCPR(Component-Component plus Residual) í”Œë¡¯ë„ ë¶€ë¶„íšŒê·€ í”Œë¡¯ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ íŠ¹ì •í•œ í•˜ë‚˜ì˜ ë³€ìˆ˜ì˜ ì˜í–¥ì„ ì‚´í´ë³´ê¸° ìœ„í•œ ê²ƒì´ì•¼ ë¶€ë¶„íšŒê·€ë¶„ì„ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ì ì´ í•˜ë‚˜ ìˆëŠ”ë° ê·¸ê²ƒì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë‹¤ë¥¸ ë³€ìˆ˜ì— ì˜í–¥ì„ ë°›ì€ AGEê°€ ì•„ë‹ˆë¼ AGE ë°ì´í„° ê·¸ ìì²´ì™€ ì§‘ê°’ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ê¸°ìœ„í•œ ë°©ë²•ì´ë¼ê³  ë³´ë©´ ë˜12sm.graphics.plot_ccpr(result_boston, \"AGE\")plt.show() CCPR í”Œë¡¯ì—ì„œëŠ” ë¶€ë¶„íšŒê·€ í”Œë¡¯ê³¼ ë‹¬ë¦¬ ë…ë¦½ë³€ìˆ˜ê°€ ì›ë˜ì˜ ê°’ ê·¸ëŒ€ë¡œ ë‚˜íƒ€ë‚œë‹¤ëŠ” ì ì„ ë‹¤ì‹œ í•œë²ˆ ìƒê¸° ì‹œì¼œì¤„ê²Œ1234fig = plt.figure(figsize=(8, 15))sm.graphics.plot_ccpr_grid(result_boston, fig=fig)fig.suptitle(\"\")plt.show() plot_regress_exog(result, exog_idx) result: íšŒê·€ë¶„ì„ ê²°ê³¼ ê°ì²´ exog_idx: ë¶„ì„ ëŒ€ìƒì´ ë˜ëŠ” ë…ë¦½ë³€ìˆ˜ ë¬¸ìì—´123fig = sm.graphics.plot_regress_exog(result_boston, \"AGE\")plt.tight_layout(pad=4, h_pad=0.5, w_pad=0.5)plt.show()","link":"/2020/05/13/Linear-Regression-with-scale-categorical-regression-and-partial-regression/"},{"title":"Model sava and load in tensorflow","text":"ì„¤ì •í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  í…ì„œí”Œë¡œë¥¼ ì„í¬íŠ¸(import)í•©ë‹ˆë‹¤.1pip install -q pyyaml h5py # HDF5 í¬ë§·ìœ¼ë¡œ ëª¨ë¸ì„ ì €ì¥í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•©ë‹ˆë‹¤ Note: you may need to restart the kernel to use updated packages.123456import osimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersprint(tf.__version__) 2.4.0-dev20200724 ì˜ˆì œ ë°ì´í„°ì…‹ ë°›ê¸°1234567(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()train_labels = train_labels[:1000]test_labels = test_labels[:1000]train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0 ëª¨ë¸ë§ ì‘ì—…1234567891011121314151617181920# Sequential ëª¨ë¸ ì •ì˜def create_model(): model = tf.keras.models.Sequential([ keras.layers.Dense(512, activation='relu', input_shape=(784,)), keras.layers.Dropout(0.2), keras.layers.Dense(10) ]) model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) return model# ëª¨ë¸ ê°ì²´ ìƒì„±model = create_model()# ì¶œë ¥model.summary() Model: &quot;sequential_8&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_8 (Dropout) (None, 512) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________ í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸° í›ˆë ¨ ì¤‘ê°„ê³¼ í›ˆë ¨ ë§ˆì§€ë§‰ì— ì²´í¬í¬ì¸íŠ¸(checkpoint)ë¥¼ ìë™ìœ¼ë¡œ ì €ì¥í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‹¤ì‹œ í›ˆë ¨í•˜ì§€ ì•Šê³  ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ í›ˆë ¨ ê³¼ì •ì´ ì¤‘ì§€ëœ ê²½ìš° ì´ì–´ì„œ í›ˆë ¨ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tf.keras.callbacks.ModelCheckpointì€ ì´ëŸ° ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½œë°±(callback)ì…ë‹ˆë‹¤. ì´ ì½œë°±ì€ ì²´í¬í¬ì¸íŠ¸ ì‘ì—…ì„ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ ì—¬ëŸ¬ê°€ì§€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.1234567891011121314151617checkpoint_path = \"training_1/cp.ckpt\"checkpoint_dir = os.path.dirname(checkpoint_path)# ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ì½œë°± ë§Œë“¤ê¸°cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)# ìƒˆë¡œìš´ ì½œë°±ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨í•˜ê¸°model.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), callbacks=[cp_callback]) # ì½œë°±ì„ í›ˆë ¨ì— ì „ë‹¬í•©ë‹ˆë‹¤# ì˜µí‹°ë§ˆì´ì €ì˜ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ë˜ì–´ ê²½ê³ ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.# ì´ ê²½ê³ ëŠ” (ê·¸ë¦¬ê³  ì´ ë…¸íŠ¸ë¶ì˜ ë‹¤ë¥¸ ë¹„ìŠ·í•œ ê²½ê³ ëŠ”) ì´ì „ ì‚¬ìš© ë°©ì‹ì„ ê¶Œì¥í•˜ì§€ ì•Šê¸° ìœ„í•¨ì´ë©° ë¬´ì‹œí•´ë„ ì¢‹ìŠµë‹ˆë‹¤. WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback. Epoch 1/10 16/32 [==============&gt;...............] - ETA: 0s - loss: 1.8756 - accuracy: 0.3736 Epoch 00001: saving model to training_1/cp.ckpt 32/32 [==============================] - 1s 30ms/step - loss: 1.5677 - accuracy: 0.5056 - val_loss: 0.6899 - val_accuracy: 0.7870 Epoch 2/10 31/32 [============================&gt;.] - ETA: 0s - loss: 0.4283 - accuracy: 0.8845 Epoch 00002: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 8ms/step - loss: 0.4276 - accuracy: 0.8844 - val_loss: 0.5193 - val_accuracy: 0.8380 Epoch 3/10 20/32 [=================&gt;............] - ETA: 0s - loss: 0.2892 - accuracy: 0.9208 Epoch 00003: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.2828 - accuracy: 0.9232 - val_loss: 0.4733 - val_accuracy: 0.8510 Epoch 4/10 19/32 [================&gt;.............] - ETA: 0s - loss: 0.1721 - accuracy: 0.9687 Epoch 00004: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9622 - val_loss: 0.4489 - val_accuracy: 0.8490 Epoch 5/10 17/32 [==============&gt;...............] - ETA: 0s - loss: 0.1666 - accuracy: 0.9582 Epoch 00005: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.9605 - val_loss: 0.4112 - val_accuracy: 0.8580 Epoch 6/10 30/32 [===========================&gt;..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9851 Epoch 00006: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 0.9846 - val_loss: 0.4088 - val_accuracy: 0.8650 Epoch 7/10 17/32 [==============&gt;...............] - ETA: 0s - loss: 0.0798 - accuracy: 0.9883 Epoch 00007: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9870 - val_loss: 0.4074 - val_accuracy: 0.8680 Epoch 8/10 32/32 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9899 Epoch 00008: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9900 - val_loss: 0.4204 - val_accuracy: 0.8590 Epoch 9/10 28/32 [=========================&gt;....] - ETA: 0s - loss: 0.0588 - accuracy: 0.9915 Epoch 00009: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 7ms/step - loss: 0.0574 - accuracy: 0.9920 - val_loss: 0.4110 - val_accuracy: 0.8640 Epoch 10/10 31/32 [============================&gt;.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9978 Epoch 00010: saving model to training_1/cp.ckpt 32/32 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 0.9978 - val_loss: 0.3962 - val_accuracy: 0.8660 &lt;tensorflow.python.keras.callbacks.History at 0x7fd0f59b3f10&gt; ì´ ì½”ë“œëŠ” tensorflow ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ë§Œë“¤ê³  ì—í¬í¬ê°€ ì¢…ë£Œë  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:1ls {checkpoint_dir} checkpoint cp.ckpt.index cp.ckpt.data-00000-of-0000112345# ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„±model = create_model()loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 2.3409 - accuracy: 0.1250 í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸ì˜ ì •í™•ë„: 12.50% ì €ì¥í–ˆë˜ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ë‹¤ì‹œ í‰ê°€í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.123456# ê°€ì¤‘ì¹˜ ë¡œë“œmodel.load_weights(checkpoint_path)# ëª¨ë¸ ì¬í‰ê°€loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 0.3962 - accuracy: 0.8660 ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: 86.60% ì²´í¬í¬ì¸íŠ¸ ì½œë°± ë§¤ê°œë³€ìˆ˜123456789101112131415161718192021222324# íŒŒì¼ ì´ë¦„ì— ì—í¬í¬ ë²ˆí˜¸ë¥¼ í¬í•¨ì‹œí‚µë‹ˆë‹¤(`str.format` í¬ë§·)checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"checkpoint_dir = os.path.dirname(checkpoint_path)# ë‹¤ì„¯ ë²ˆì§¸ ì—í¬í¬ë§ˆë‹¤ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ì½œë°±ì„ ë§Œë“­ë‹ˆë‹¤cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5)# ìƒˆë¡œìš´ ëª¨ë¸ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤model = create_model()# `checkpoint_path` í¬ë§·ì„ ì‚¬ìš©í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤model.save_weights(checkpoint_path.format(epoch=0))# ìƒˆë¡œìš´ ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤model.fit(train_images, train_labels, epochs=50, callbacks=[cp_callback], validation_data=(test_images,test_labels), verbose=0) WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen. WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback. Epoch 00005: saving model to training_2/cp-0005.ckpt Epoch 00010: saving model to training_2/cp-0010.ckpt Epoch 00015: saving model to training_2/cp-0015.ckpt Epoch 00020: saving model to training_2/cp-0020.ckpt Epoch 00025: saving model to training_2/cp-0025.ckpt Epoch 00030: saving model to training_2/cp-0030.ckpt Epoch 00035: saving model to training_2/cp-0035.ckpt Epoch 00040: saving model to training_2/cp-0040.ckpt Epoch 00045: saving model to training_2/cp-0045.ckpt Epoch 00050: saving model to training_2/cp-0050.ckpt &lt;tensorflow.python.keras.callbacks.History at 0x7fd0f1d481d0&gt;1ls {checkpoint_dir} checkpoint cp-0025.ckpt.index cp-0000.ckpt.data-00000-of-00001 cp-0030.ckpt.data-00000-of-00001 cp-0000.ckpt.index cp-0030.ckpt.index cp-0005.ckpt.data-00000-of-00001 cp-0035.ckpt.data-00000-of-00001 cp-0005.ckpt.index cp-0035.ckpt.index cp-0010.ckpt.data-00000-of-00001 cp-0040.ckpt.data-00000-of-00001 cp-0010.ckpt.index cp-0040.ckpt.index cp-0015.ckpt.data-00000-of-00001 cp-0045.ckpt.data-00000-of-00001 cp-0015.ckpt.index cp-0045.ckpt.index cp-0020.ckpt.data-00000-of-00001 cp-0050.ckpt.data-00000-of-00001 cp-0020.ckpt.index cp-0050.ckpt.index cp-0025.ckpt.data-00000-of-0000112latest = tf.train.latest_checkpoint(checkpoint_dir)latest &apos;training_2/cp-0050.ckpt&apos;123456789# ëª¨ë¸ ì´ˆê¸°í™” ë° ìƒì„±model = create_model()# ëª¨ë¸ ë¡œë“œmodel.load_weights(latest)# ëª¨ë¸ ë³µì›, í‰ê°€loss, acc = model.evaluate(test_images, test_labels, verbose=2)print(\"ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%\".format(100*acc)) WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. 32/32 - 0s - loss: 0.4795 - accuracy: 0.8720 ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: 87.20% ìˆ˜ë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì €ì¥í•˜ê¸°123456789101112# ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤model.save_weights('./checkpoints/my_checkpoint')# ìƒˆë¡œìš´ ëª¨ë¸ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤model = create_model()# ê°€ì¤‘ì¹˜ë¥¼ ë³µì›í•©ë‹ˆë‹¤model.load_weights('./checkpoints/my_checkpoint')# ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤loss,acc = model.evaluate(test_images, test_labels, verbose=2)print(\"ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%\".format(100*acc)) 32/32 - 0s - loss: 0.4795 - accuracy: 0.8720 ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: 87.20%ì „ì²´ ëª¨ë¸ ì €ì¥í•˜ê¸°1234567# ìƒˆë¡œìš´ ëª¨ë¸ ê°ì²´ë¥¼ ë§Œë“¤ê³  í›ˆë ¨í•©ë‹ˆë‹¤model = create_model()model.fit(train_images, train_labels, epochs=10)# SavedModelë¡œ ì „ì²´ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤!mkdir -p saved_modelmodel.save('saved_model/my_model') Epoch 1/10 32/32 [==============================] - 0s 15ms/step - loss: 1.6664 - accuracy: 0.4644 Epoch 2/10 32/32 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8490 Epoch 3/10 32/32 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.9225 Epoch 4/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9644 Epoch 5/10 32/32 [==============================] - 0s 4ms/step - loss: 0.1473 - accuracy: 0.9746 Epoch 6/10 32/32 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9736 Epoch 7/10 32/32 [==============================] - 0s 4ms/step - loss: 0.0863 - accuracy: 0.9785 Epoch 8/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9967 Epoch 9/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9974 Epoch 10/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0374 - accuracy: 0.9988 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2 WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details. INFO:tensorflow:Assets written to: saved_model/my_model/assets12345# my_model ë””ë ‰í† ë¦¬!ls saved_model# assests í´ë”, saved_model.pb, variables í´ë”!ls saved_model/my_model \u001b[34mmy_model\u001b[m\u001b[m \u001b[34massets\u001b[m\u001b[m saved_model.pb \u001b[34mvariables\u001b[m\u001b[m1234new_model = tf.keras.models.load_model('saved_model/my_model')# ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤new_model.summary() Model: &quot;sequential_23&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_46 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_23 (Dropout) (None, 512) 0 _________________________________________________________________ dense_47 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________12345# ë³µì›ëœ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)print('ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%'.format(100*acc))print(new_model.predict(test_images).shape) 32/32 - 0s - loss: 0.4205 - accuracy: 0.0880 ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: 8.80% (1000, 10)HDF5 íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°1234567# ìƒˆë¡œìš´ ëª¨ë¸ ê°ì²´ë¥¼ ë§Œë“¤ê³  í›ˆë ¨í•©ë‹ˆë‹¤model = create_model()model.fit(train_images, train_labels, epochs=10)# ì „ì²´ ëª¨ë¸ì„ HDF5 íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤# '.h5' í™•ì¥ìëŠ” ì´ ëª¨ë¸ì´ HDF5ë¡œ ì €ì¥ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤model.save('my_model.h5') Epoch 1/10 32/32 [==============================] - 0s 14ms/step - loss: 1.6326 - accuracy: 0.5135 Epoch 2/10 32/32 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8959 Epoch 3/10 32/32 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.9177 Epoch 4/10 32/32 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9320 Epoch 5/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9757 Epoch 6/10 32/32 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9879 Epoch 7/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9864 Epoch 8/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9946 Epoch 9/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9925 Epoch 10/10 32/32 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.999412345# ê°€ì¤‘ì¹˜ì™€ ì˜µí‹°ë§ˆì´ì €ë¥¼ í¬í•¨í•˜ì—¬ ì •í™•íˆ ë™ì¼í•œ ëª¨ë¸ì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤new_model = tf.keras.models.load_model('my_model.h5')# ëª¨ë¸ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤new_model.summary() Model: &quot;sequential_25&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_50 (Dense) (None, 512) 401920 _________________________________________________________________ dropout_25 (Dropout) (None, 512) 0 _________________________________________________________________ dense_51 (Dense) (None, 10) 5130 ================================================================= Total params: 407,050 Trainable params: 407,050 Non-trainable params: 0 _________________________________________________________________12loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)print('ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: {:5.2f}%'.format(100*acc)) 32/32 - 0s - loss: 0.4255 - accuracy: 0.0890 ë³µì›ëœ ëª¨ë¸ì˜ ì •í™•ë„: 8.90%12","link":"/2020/08/26/Model-sava-and-load-in-tensorflow/"},{"title":"Data Analytic on Football","text":"[ìˆ«ìëŠ” ê±°ì§“ë§í•˜ì§€ ì•ŠëŠ”ë‹¤: ì™œ ì¶•êµ¬ í´ëŸ½ë“¤ì´ ê·¸ë ‡ê²Œ ì• ë„ë¦¬í‹±ìŠ¤ë¥¼ ì¤‘ìš”í•˜ê²Œ ìƒê°í• ê¹Œ(The numbers donâ€™t lie: why football clubs place such importance on analytics)] ì›¨ìŠ¤íŠ¸ í–„ì˜ ë¡œë¦¬ ìº ë²¨ì€ ë¹… í´ëŸ½ì—ì„œ í•µì‹¬ ê²°ì •ì— ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ë– ì˜¤ë¥´ëŠ” ì¶•êµ¬ ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ ì¤‘ì˜ í•˜ë‚˜ì´ë‹¤. ì„ ìˆ˜ë“¤ì´ ë– ë‚œ í•œì°¸ í›„ì˜ ì›¨ìŠ¤íŠ¸ í–„ ìœ ë‚˜ì´í‹°ë“œì˜ ì±„ë“œì›° í—¬ìŠ¤ í›ˆë ¨ì¥ì˜ ì¡°ìš©í•œ êµ¬ì„ì—ì„œ ë¡œë¦¬ ìº ë²¨ì€ ì»´í“¨í„° í™”ë©´ì„ ì‘ì‹œí•˜ê³  ìˆë‹¤. ì´ê²ƒì´ ë³´ë¹„ ë¬´ì–´ì™€ ì œí”„ í—ˆìŠ¤íŠ¸ ê²½ì˜ ë‘ ë²ˆì§¸ ì§‘ì´ì—ˆë˜ ì´ë˜ë¡œ ê·¸ ì£¼ë³€ì˜ ê±°ì£¼ì§€ë“¤ì€ ì˜¤ì§ í”¼ìƒì ìœ¼ë¡œ ë³€í–ˆì„ ë¿ì´ë‚˜, í•„ë“œ ë°–ì˜ ì¤€ë¹„ì˜ ë³µì¡ì„±ì€ ì™„ì „íˆ ë°”ë€Œê³  ìˆë‹¤. ìº ë²¨ì€ ì›¨ìŠ¤íŠ¸ í–„ì˜ ê¸°ìˆ  ìŠ¤ì¹´ìš°íŠ¸ì´ì ë¶„ì„ê°€ì´ë‹¤. ì˜¥ìŠ¤í¬ë“œ ì¡¸ì—…ìƒì´ì, ì•Œë¼ìŠ¤í…Œì–´ ìº ë²¨ì˜ ì•„ë“¤ë“¤ ì¤‘ í•˜ë‚˜ì¸ ê·¸ëŠ” ì„±ê³µì ì¸ í¬ì»¤ ì„ ìˆ˜ì˜€ìœ¼ë©°, ì•½ê°„ì˜ ì„ ìˆ˜ ê²½í—˜ê³¼ ì½”ì¹­ ë°°ê²½ì„ ê°€ì¡Œë‹¤. ê·¸ì˜ ì´ˆì ì€ ì¶•êµ¬ì— ê´€í•œ ë¬´í•œí•œ í†µê³„ì ì´ ë°ì´í„°ë“¤ì„ ì´í•´í•˜ëŠ” ê²ƒì´ë©°, ê·¸ë˜ì„œ í´ëŸ½ì˜ í•µì‹¬ì ì¸ ì˜ì‚¬ ê²°ì •ìë“¤ì—ê²Œ ë¬´ì—‡ì´ ì •ë§ ì¤‘ìš”í•œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ë‹¤. ì€ê³¨ë¡œ ì¹¸í…ŒëŠ” ì˜¬ì‹œì¦Œì˜ ì˜ì…ì´ê³  ì¶•êµ¬ ì• ë„ë¦¬í‹±ìŠ¤ì˜ ìŠ¹ë¦¬ì´ë‹¤. ë”êµ¬ë‚˜, ì¶•êµ¬ì˜ ê°€ì¥ íš¨ê³¼ì ì¸ ë¶„ì„ ì‘ì—…ë“¤ ì¤‘ ëª‡ëª‡ê³¼ ë‹¨ìˆœíˆ êµ¬ë§¤ë ¥ì„ ê±°ì˜ ë°˜ì˜í•˜ì§€ ì•Šê³  ìˆëŠ” í”„ë¦¬ë¯¸ì–´ ë¦¬ê·¸ í…Œì´ë¸” ì‚¬ì´ì˜ ì ì¬ì ì¸ ìƒê´€ ê´€ê³„ëŠ” ë¶„ëª…í•˜ë‹¤. ë ˆìŠ¤í„° ì‹œí‹°ì™€ ì›¨ìŠ¤íŠ¸ í–„ì€ ì˜¤ëŠ˜ ë§Œë‚˜ ê²½ê¸°ë¥¼ ê°–ì§€ë§Œ, ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–»ê²Œ ê·¸ë“¤ì´, ë§Œì²´ìŠ¤í„° ìœ ë‚˜ì´í‹°ë“œê°€ ë§ˆë£¨ì•™ í ë¼ì´ë‹ˆ, ì•ˆë” ì—ë ˆë¼ ê·¸ë¦¬ê³  ë°”ìŠ¤í‹°ì•ˆ ìŠˆë°”ì¸ìŠˆíƒ€ì´ê±°ì— 7ì²œë§Œ íŒŒìš´ë“œë¥¼ ìŸì•„ë¶€ì„ ë•Œ, ì€ê³¨ë¡œ ì¹¸í…Œ, ë””ë¯¸íŠ¸ë¦¬ íŒŒì˜ˆ ê·¸ë¦¬ê³  ë¦¬ì•¼ë“œ ë§ˆë ˆì¦ˆë¥¼ ì²œ 6ë°±ë§Œ íŒŒìš´ë“œì— ë°œê²¬í•´ëƒˆì„ê¹Œ? ê·¸ë¦¬ê³  ë¬´ì—‡ì´ í† íŠ¼í–„ í•«ìŠ¤í¼ê°€ ë¸ë¦¬ ì•Œë¦¬ì™€ ì—ë¦­ ë‹¤ì´ì–´ë¡œ ì´ëŒì—ˆëŠ”ì§€ í˜¹ì€ ì‚¬ìš°ìŠ¤í–„íŠ¼ì„ ì‚¬ë””ì˜¤ ë§ˆë„¤ì™€ ë²„ì§ˆ ë°˜ ë‹¤ì´í¬ë¡œ ì´ëŒì—ˆì„ê¹Œ? ì™œ íŒ€ë“¤ì´ ì „ì— ì—†ì´ ì ì€ í¬ë¡œìŠ¤ë¥¼ í•˜ê³  ìˆì„ê¹Œ? ë¬´ì—‡ì´ ë ˆìŠ¤í„°ì˜ ë…íŠ¹í•œ íŠ¹ì§•ì¼ê¹Œ? ì™œ í© ê³¼ë¥´ë””ì˜¬ë¼ì™€ ê°™ì€ ê°ë…ë“¤ì´ ë¨¼ ê±°ë¦¬ì—ì„œì˜ ìŠˆíŒ…ì„ ì¥ë ¤í•˜ì§€ ì•Šì„ê¹Œ? ê·¸ë¦¬ê³  ëª¨ë“  ì‹œì¦Œë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†€ë¼ìš´, í…Œì´ë¸”ì´ ê±°ì§“ë§ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì€ ì •ë§ ì‚¬ì‹¤ì¼ê¹Œ? ìº ë²¨ì€ ì›¨ìŠ¤íŠ¸ í–„ì˜ ê¸°ìˆ  ìŠ¤ì¹´ìš°íŠ¸ì´ì ì• ë„ë¦¬ìŠ¤íŠ¸ì´ë‹¤. ì• ë„ë¦¬í‹±ìŠ¤ëŠ” ìµœì†Œí•œ ë¶€ë¶„ì ì¸ ë‹µì„ ì œê³µí•œë‹¤. ë¹„ë¡ ìº ë²¨ì´ ë¶„ì„ì˜ ê°€ì¹˜ê°€ ì—¬ì „íˆ ì˜ì‚¬ê²°ì •ì˜ ê¸°ì €ë¥¼ êµ¬ì„±í•˜ê³  ìˆëŠ” ê²½í—˜, ì§ê´€, ë³¸ì§ˆì ì¸ ì§€ì‹ê³¼ ì ‘ì´‰ë“¤ì„ ëŒ€ì²´í•œë‹¤ê¸° ë³´ë‹¤ëŠ” ë³´ì¡°í•˜ëŠ” ê²ƒì´ë¼ê³  í™•ì‹ í•˜ì§€ë§Œ ë§ì´ë‹¤. ê·¸ëŠ” â€œë¹„íš¨ìœ¨ì ì¸ ì–´ë–¤ ì‹œì¥ë„ ê¸°íšŒì…ë‹ˆë‹¤.â€ë¼ê³  ë§í•œë‹¤. â€œì¶•êµ¬ê°€ ì¬ëŠ¥ì„ ê°€ì¹˜ í‰ê°€í•˜ëŠ” ì„¸íŠ¸ë‚˜ ë™ì˜ëœ ë°©ì‹ì„ ê°–ê³  ìˆì§€ ì•Šê³  ë„ˆë¬´ë‚˜ ì„ì˜ì ì´ë¼ëŠ” ì‚¬ì‹¤ì€ ê¸°íšŒì…ë‹ˆë‹¤. í†µê³„ì™€ ì• ë„ë¦¬í‹±ìŠ¤ëŠ” ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. í†µê³„ëŠ” ë‹¹ì‹ ì—ê²Œ ì¼ì–´ë‚œ ì‚¬ê±´ë“¤ì— ëŒ€í•´ì„œ ë§í•´ì¤ë‹ˆë‹¤. ê·¸ë“¤ì€ ë§¥ë½ì—†ì´ëŠ” ì•„ë¬´ ê²ƒë„ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë ˆìŠ¤í„°ì˜ ì§ì ‘ì ì¸ ìŠ¤íƒ€ì¼ì€ ê·¸ë“¤ì„ ëšœë ·í•œ í”„ë¦¬ë¯¸ì–´ ë¦¬ê·¸ ì•„ì›ƒë¼ì´ì–´ë¡œ ë§Œë“¤ì—ˆë‹¤. â€œì• ë„ë¦¬í‹±ìŠ¤ëŠ” ê·¸ëŸ¬í•œ í†µê³„ë“¤ì„ ë¯¸ë˜ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ í•´ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ëª¨ë“  ê²ƒì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë ¤ìš´ ê²ƒì€ ë¬´ì—‡ì´ ì¤‘ìš”í•œì§€ë¥¼ ì•Œì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤. í•œê°€ì§€ ì¢‹ì€ ê²ƒì€ ì¶•êµ¬ëŠ” ê½¤ ë‹¨ìˆœí•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë“  ê²ƒì€ ê²°êµ­ì—ëŠ” ì–´ë–»ê²Œë“  ê³¨ê³¼ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤, ê·¸ê²ƒì´ ìš°ë¦¬ì˜ ë“ì  ê¸°íšŒë¥¼ ëŠ˜ë ¤ì£¼ê±°ë‚˜ ì•„ë‹ˆë©´ ì‹¤ì ì„ ë§‰ì•„ì£¼ê±°ë‚˜ìš”. ê·¸ê²ƒì€ ë˜í•œ ê°ë…ì´ íŒ€ì´ ì–´ë–»ê²Œ í”Œë ˆì´í•˜ê¸°ë¥¼ ì›í•˜ëŠ” í‹€ ì•ˆì—ì„œ í†µí•©ë‹ˆë‹¤.â€ ë³´ë‹¤ ë” ë§ì€ í†µì°°ë“¤ì´ ì‚¬ìš°ìŠ¤í–„íŠ¼ì˜ í›ˆë ¨ì‹œì„¤ì—ì„œ ë°œê²¬ë  ìˆ˜ ìˆë‹¤. ê°€ì¥ ë†€ë¼ìš´ ê³³ì€ í´ëŸ½ì˜ 34ì„¸ì˜ ìŠ¤ì¹´ìš°íŒ…ê³¼ ì„ ìˆ˜ì„ ë°œ ì´ì‚¬ì¸ ë¡œìŠ¤ ìœŒìŠ¨ì´ ìë¦¬ì¡ê³  ìˆëŠ” ë°©ì´ë‹¤. ê·¸ì˜ ë°”ë¡œ ì•ì—ëŠ” 15ê°œì˜ ì¼ë ¨ì˜ í™”ë©´ ì•ì—ì„œ ì Šì€ ìŠ¤íƒœí”„ë“¤ì˜ íŒ€ì´ ì •ë³´ë“¤ì„ ì²˜ë¦¬í•˜ê³  ìˆë‹¤. ëª‡ëª‡ì€ ì¶•êµ¬ ë¶„ì„ì˜ íŠ¹ì • ë¶„ì•¼ì—ì„œì˜ í•™ìœ„ë¥¼ ë³´ìœ í•œ ì¸í„´ë“¤ì´ë‹¤. ìœŒìŠ¨ì˜ ì˜¤ë¥¸ìª½ì—ëŠ” ë³´ë‹¤ í¬ë—í¬ë—í•œ ë¨¸ë¦¬ë¥¼ ê°€ì§„ ê´€ê³„ìì¸ ë¡œë“œ ë£¨ë”•ê³¼ ê°™ì€ ì‚¬ëŒì´ ìˆë‹¤. ê·¸ëŠ” ë‰´í¬íŠ¸ì˜ í•„ë“œì—ì„œ ë›°ë˜ 8ì‚´ì˜ ê°€ë ˆìŠ¤ ë² ì¼ì„ ë°œê²¬í•œ ìŠ¤ì¹´ìš°íŠ¸ì´ë‹¤. ìœŒìŠ¨ì˜ ì™¼ìª½ì—ëŠ” â€œë¸”ë™ ë°•ìŠ¤â€ë¼ëŠ” ë‹¨ì–´ê°€ ë¯¸ìŠ¤í…Œë¦¬í•˜ê²Œ ê±¸ë ¤ìˆëŠ” ë¬¸ì´ ìˆë‹¤. í´ ë¯¸ì²¼ì€ ì‚¬ìš°ìŠ¤í–„íŠ¼ì—ì„œ í† íŠ¼í–„ìœ¼ë¡œ ì´ë™í•´ì„œ ì„ ìˆ˜ì„ ë°œ ë° ë¶„ì„ íŒ€ì¥ì´ ë˜ì—ˆë‹¤. ì‚¬ìš°ìŠ¤í–„íŠ¼ì€ ì´ ë¯¸ë‹ˆ-ì‹œë„¤ë§ˆì— ì“°ì´ëŠ” ë§ì¶¤í˜• ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ê¾¸ì¤€íˆ ìˆ˜ì •í•˜ê³  ìˆë‹¤. ê·¸ ì†Œí”„íŠ¸ì›¨ì–´ëŠ” ëª‡ë²ˆì˜ í´ë¦­ë“¤ ë§Œìœ¼ë¡œ ì „ì„¸ê³„ì— ìˆëŠ” ì–´ë–¤ ì„ ìˆ˜ì— ëŒ€í•œ ê²ƒë„ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ í´ëŸ½ë“¤ì€ ë¹„ìŠ·í•œ ê¸°ìˆ ì„ ê°œë°œ ì¤‘ì´ë©°, ìˆ˜í•™ì ìœ¼ë¡œ ì¬ëŠ¥ë“¤ì„ ê³¨ë¼ë‚´ëŠ” ì‚¬ëŒë“¤ ì‚¬ì´ì—ì„œ, ì´ì  ì‹œì¥ì´ ë– ì˜¤ë¥´ê³  ìˆë‹¤. ì•„ìŠ¤ë‚ ì€ ì˜¬ í•´ ë²¤ ë¤¼ê¸€ì›ŒìŠ¤ë¥¼ ë ˆìŠ¤í„°ë¡œë¶€í„° ë¹¼ëƒˆê³ , ë¯¸êµ­ ê¸°ë°˜ì˜ ë¶„ì„ íšŒì‚¬ì¸ statDNAë¥¼ 2ë°±ë§Œ íŒŒìš´ë“œì— ì‚¬ë“¤ì˜€ë‹¤. ë§ˆìš°ë¦¬ì‹œì˜¤ í¬ì²´í‹°ë…¸ì™€ í•¨ê»˜, í† íŠ¼í–„ì€ ê·¸ë“¤ì˜ ì„ ìˆ˜ì„ ë°œê³¼ ë¶„ì„ íŒ€ì¥ì¸ í´ ë¯¸ì²¼ì„ ì‚¬ìš°ìŠ¤í–„íŠ¼ì—ì„œ ì„ ë°œí–ˆë‹¤. ë¶€ìƒìœ¼ë¡œ 27ì„¸ì— ì„ ìˆ˜ ì»¤ë¦¬ì–´ë¥¼ ëëƒˆë˜ ë¯¸ì²¼ì€ ì´ë ‡ê²Œ ë§í•œë‹¤. â€œì €ëŠ” í•œ ë²ˆ ì¢‹ì€ ê²½ê¸°ë¥¼ ê°€ì§ˆ ë•Œ ë‹¤ë¥¸ 80ê²½ê¸°ì—ì„œëŠ” ê·¸ë ‡ê²Œ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê°„ë‹¨í•œ ì´ë¡ ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.â€ í¬ì»¤ì—ì„œì²˜ëŸ¼, ìº ë²¨ì€ ì„ ìˆ˜ ì„ ë°œì„ â€œë‹¹ì‹ ì˜ ë² íŒ…ì˜ ê²½ì œì ì¸ ë¦¬ìŠ¤í¬ë¥¼ ê°€ìš©í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ê´€ë¦¬í•˜ëŠ” ê²ƒâ€ì´ë¼ê³  ë¶€ë¥¸ë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ì ì€ ê·¸ì˜ ì‚°ì—…ì˜ ë‹¤ë¥¸ ë©¤ë²„ë“¤ê³¼ì˜ ë¯¸íŒ…ì´ ë¶„ëª…íˆ í•µì‹¬ì ì´ë¼ëŠ” ê²ƒì„ ê°•ì¡°í•œë‹¤. ê³¼ê±°ì˜ ì¶•êµ¬ì—ì„œì˜ í˜ì‹ ì˜ ì‹œë„ê°€ ì£¼ì €ì•‰ì€ ê³³ì—ëŠ” ì¢…ì¢… ì˜ì‚¬ì†Œí†µì˜ ì‹¤ìˆ˜ë‚˜ ê°œì¸ê°„ì˜ ì¶©ëŒë“¤ì— ê¸°ë°˜í•œë‹¤. í´ë¼ì´ë¸Œ ìš°ë“œì™€ë“œ ê²½ì€ ë³´í†µ ìƒê°ë˜ëŠ” ê²ƒë³´ë‹¤ í•´ë¦¬ ë ˆë“œë‚©ê³¼ ë” ë‚˜ì€ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤. í•˜ì§€ë§Œ ê±°ê¸°ì— ë£¨í¼íŠ¸ ë¡œ, ë°ì´ë¸Œ ë°”ì…‹, ë°ë‹ˆìŠ¤ ì™€ì´ì¦ˆì™€ ì‚¬ì´ë¨¼ í´ë¦¬í¬ë“œë¥¼ ë”í•˜ë©´ ë‹¹ì‹ ì´ ê·¸ê²ƒì´ ì–´ë””ì„œ ì˜ëª»ë˜ì—ˆëŠ”ì§€ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ ì—ë¥´í˜ í¬ì™€ë“œì˜ ì¶”ë¦¬ë ¥ê¹Œì§€ í•„ìš”í•˜ì§€ë„ ì•Šë‹¤. ì• ë„ë¦¬í‹±ìŠ¤ê°€ ì°¨ì´ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê³³ì—ëŠ” ì£¼ë¡œ ë¬¸í™”ê°€ ì •ë¦½ëœë‹¤. â€œë‹¹ì‹ ì´ í”Œë ˆì´í•  í•„ìš”ê°€ ìˆëŠ” í´ëŸ½ë“¤ ì¤‘ì—ëŠ” ì „í†µì´ ê¹Šì´ ë°°ì–´ìˆëŠ” í´ëŸ½ë“¤ì´ ìˆì„ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš´ì´ ì¢‹ê²Œë„ ë‚´ê°€ ìˆì—ˆë˜ í´ëŸ½ë“¤ì€ ì‚¬ê³ ë°©ì‹ì´ ë§¤ìš° ì—´ë ¤ìˆì—ˆìŠµë‹ˆë‹¤.â€ ìœŒìŠ¨ì´ ë§í•œë‹¤. ìº ë²¨ì€ ì´ë ‡ê²Œ ë”í•œë‹¤: â€œì• ë„ë¦¬í‹±ìŠ¤ ì„¸ê³„ê°€ í˜ê²¨ì›Œí•˜ëŠ” ë¶€ë¶„ì€ ì „í†µì ì¸ ì¶•êµ¬ ì„¸ê³„ë¡œì˜ ë‹¤ë¦¬ë¥¼ ë†“ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •ë³´ë¥¼ ë” ì¹¨íˆ¬ì‹œí‚¤ê¸° ìœ„í•´ì„œìš”. ìˆ˜í•™ì ì¸ ì¸¡ë©´ì—ì„œëŠ” ì™„ë²½íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ì´ë˜ëŠ” ë§ì€ ì •ë³´ë“¤ì„ ì£¼ê³  ìˆ˜ì‹­ë…„ê°„ ë°œì „í•´ ì˜¨ ìŠ¤í¬ì¸ ê°€ ê·¸ê²ƒì„ í•˜ë£¨ ì•„ì¹¨ì— ë°›ì•„ë“¤ì´ê¸°ë¥¼ ê¸°ëŒ€í•˜ëŠ” ê²ƒì€ ì‚¬ì‹¤ ê½¤ ê±´ë°©ì§„ ê²ƒì´ë‹¤. ë‚˜ëŠ” ì´ê²ƒì´ ê°€ì¥ í° ë„ì „ìœ¼ë¡œ ë‚¨ì•„ìˆë‹¤ê³  ë§í•˜ê³  ì‹¶ë‹¤. ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆê¸° ìœ„í•´ì„œëŠ” ë‹¹ì‹ ì€ ì—­í•™ê´€ê³„ì™€ ë‹¹ì‹ ê³¼ í•¨ê»˜ ì¼í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì„±ê²©ë“¤ì„ ì´í•´í•´ì•¼ë§Œ í•œë‹¤. ë‚˜ëŠ” ê·¸ê²ƒì´ ì™œ ì• ë„ë¦¬í‹±ìŠ¤ê°€ ë‹¤ë¥¸ ë¹„ì§€ë‹ˆìŠ¤ì—ì„œ ê·¸ë¬ë˜ ê²ƒì²˜ëŸ¼ ì¶•êµ¬ì— ì¹¨íˆ¬í•˜ì§€ ëª»í•œ ì´ìœ ë¼ê³  ìƒê°í•œë‹¤.â€ ì•„ìŠ¤ë‚  ê°ë… ì•„ìŠ¨ ë²µê±°ëŠ” ë˜í•œ ì• ë„ë¦¬í‹±ìŠ¤ë¥¼ ìˆ˜ìš©í•´ì˜¤ê³  ìˆë‹¤. í•˜ì§€ë§Œ ì´ê²ƒì€ ë³€í•˜ê³  ìˆë‹¤. ìº ë²¨ì€ ê·¸ê°€ ìŠ¬ë˜ë¸ ë¹Œë¦¬ì¹˜ì™€ ì„ ìˆ˜ ì„ ë°œ ë””ë ‰í„°ì¸ í† ë‹ˆ í—¨ë¦¬ ì•„ë˜ì—ì„œ ì¼í•  ìˆ˜ ìˆì–´ì„œ ë§¤ìš° ìš´ì´ ì¢‹ì•˜ë‹¤ê³  ë§í•œë‹¤. ê·¸ë¦¬ê³  ëª¨ë‘ì˜ ì—­í• ì— ë¶„ëª…í•¨ì´ ìˆì—ˆë‹¤. ê·¸ë“¤ì´ ì›í•˜ëŠ” ê²ƒì€ ê°„ë‹¨íˆ ê·¸ë“¤ì˜ ê²°ì •ì„ ë„ì™€ì¤„ ì •ë³´ì— ëŒ€í•œ ë¯¿ì„ë§Œí•œ í‰ê°€ì˜€ë‹¤. ë‚˜ì´ ë§ì€ ê°ë…ë“¤ ì—­ì‹œ ì ‘ê·¼í•˜ê³  ìˆë‹¤. í´ë¼ìš°ë””ì˜¤ ë¼ë‹ˆì—ë¦¬ëŠ” í•œ ì˜ˆì´ë‹¤. ì•„ìŠ¨ ë²µê±°ëŠ” ì˜¬ ì‹œì¦Œ ê³µì‹ìë¦¬ì—ì„œ ì•„ìŠ¤ë‚ ì˜ â€œê¸°ëŒ€ ê³¨ ê°’ (xG)â€ì— ëŒ€í•´ ì–¸ê¸‰í•˜ë©´ì„œ ì¶©ê²©ì„ ì´‰ë°œì‹œì¼°ë‹¤. ê·¸ê²ƒì€ íŒ€ì´ í†µê³„ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ìì£¼ ë“ì í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ìŠ¤í¬ì¸  ë² íŒ…ê³¼ ì• ë„ë¦¬í‹±ìŠ¤ì—ì„œ í•µì‹¬ì ì¸ ì¸¡ì •ê°’ì´ë‹¤. ë³´ë£¨ì‹œì•„ ë„ë¥´íŠ¸ë¬¸íŠ¸ì˜ ì½”ì¹˜ í† ë§ˆìŠ¤ íˆ¬í—¬ì€ xGì— ëŒ€í•´ ë” ë°°ìš°ê¸° ìœ„í•´ ë§¤íŠœ ë² ë„˜ì„ ì°¾ì•„ê°”ë‹¤. ë² ë„˜ì€ ìŠ¤í¬ì¸  ë² íŒ…ì—ì„œ ìˆ˜ë°±ë§Œì„ ë²Œì—ˆê³  ê·¸ ì´í›„ë¡œ ë¸Œë ŒíŠ¸í¬ë“œì™€ FCë°‹ì¸Œë€ì„ ì‚¬ë“¤ì˜€ë‹¤. ì„ ìˆ˜ë¥¼ ê°œì¸ì ìœ¼ë¡œ ê´€ì°°í•˜ê¸° ìœ„í•´ í­ë„“ê²Œ ì›€ì§ì´ëŠ” ìº ë²¨ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ë² ë„˜ì€ ì¶•êµ¬ì²˜ëŸ¼ ë‚®ì€ ë“ì ì„ ê°€ì§„ ìŠ¤í¬ì¸ ì—ì„œì˜ ì–´ë–¤ ìˆ˜í•™ ëª¨ë¸ì˜ ëšœë ·í•œ ë³€ë™ì„±ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ â€œëˆˆìœ¼ë¡œ í•˜ëŠ” ìŠ¤ì¹´ìš°íŒ…â€ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•œë‹¤. í¬ì»¤ì²˜ëŸ¼, ëœë¤ê³¼ í†µì œë¶ˆê°€ëŠ¥í•œ ì‚¬ê±´ë“¤ì´ íŒë‹¨ë“¤ì„ í˜•ì„±í•˜ëŠ”ë° ì„œë‘ë¥´ëŠ” ê°€ìš´ë°ì— ì¢…ì¢… ê°„ê³¼í•˜ëŠ” ë¶€ë¶„ì ì¸ ì—­í• ì„ í•œë‹¤. ìº ë²¨ì€ ì´ë ‡ê²Œ ë§í•œë‹¤. â€œì´ê²ƒì€ ì¶•êµ¬ë¥¼ í¥ë¯¸ë¡­ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤ë§Œ ì˜ˆì¸¡ë¶ˆê°€ëŠ¥ì€ í•­ìƒ ì‹¬ê°í•œ ë¹„íš¨ìœ¨ì„ ë™ë°˜í•©ë‹ˆë‹¤. ìš´ì„ ë¶ˆí‰í•˜ëŠ” í”„ë¡œ í¬ì»¤ ì„ ìˆ˜ë“¤ì€ í¸í˜‘í•œ ê²ƒì…ë‹ˆë‹¤. ìš´ì´ë¼ëŠ” ê²ƒì€ ê·¸ë“¤ì´ ì‚¬ëŠ” ê²ƒì„ ê°€ëŠ¥ì¼€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§Œì•½ ë‚´ê°€ ì •ë§ ë‚˜ìœ ì„ ìˆ˜ì™€ í¬ì»¤ë¥¼ ì¹œë‹¤ë©´, ê·¸ëŠ” 100ë²ˆ ì¤‘ì— 40ë²ˆì„ ì´ê¸¸ ê²ƒì…ë‹ˆë‹¤. ë§Œì•½ ë‚´ê°€ ê°œë¦¬ ì¹´ìŠ¤íŒŒë¡œí”„ì™€ ì²´ìŠ¤ë¥¼ ë‘”ë‹¤ë©´ ê·¸ê°€ 100ë²ˆì„ ì´ê¸°ê² ì£ . ì²´ìŠ¤ ì„ ìˆ˜ë“¤ì€ ë² íŒ…ìœ¼ë¡œ ëˆì„ ë²Œì§€ ì•ŠìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì•„ë¬´ë„ ê·¸ë“¤ì—ê²Œ ëˆì„ ê±¸ì§€ ì•Šìœ¼ë‹ˆê¹Œìš”.â€ ë ˆìŠ¤í„°ì˜ ë™í™”ê°™ì€ ì‹œì¦Œì€ í†µê³„ì ì¸ ëª¨ë¸ë“¤ì´ í‹€ë ¸ìŒì„ ì…ì¦í•´ì˜¤ê³  ìˆë‹¤. ì• ë„ë¦¬í‹±ìŠ¤ëŠ” ì ì  ë” ë§ì€ ì˜ê²¬ë“¤ë¡œ ê°€ë“ì°¬ í˜„ì¬ ì§€í˜• ì•ˆì—ì„œ ëª©ì†Œë¦¬ë¥¼ ë‚´ê¸° ìœ„í•´ ì‹¸ìš°ê³  ìˆë‹¤. ìº ë²¨ì€ ì´ë ‡ê²Œ ë§í•œë‹¤. â€œì•„ìŠ¨ ë²µê±°ëŠ” ìš°ë¦¬ê°€ ìˆ˜ì§ì ì—ì„œ ìˆ˜í‰ì ì¸ ì‚¬íšŒë¡œ ì›€ì§ì´ê³  ìˆë‹¤ê³  ë§í–ˆìŠµë‹ˆë‹¤. ìˆ˜ì§ì ì¸ ê²ƒì€ ê¼­ëŒ€ê¸°ì— ë¦¬ë”ê°€ ìˆê³  ëª¨ë‘ê°€ ë”°ë¥´ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ˜í‰ì ì¸ ê²ƒì€ ì •ë³´ì™€ ì˜ê²¬ë“¤ì— í­ê²©ì„ ë‹¹í•˜ëŠ” ë¦¬ë”ë¥¼ ê°€ì§„ ê²ƒì…ë‹ˆë‹¤. ê·¸ê²ƒì´ ë¦¬ë”ê°€ ì–´ë–¤ ê²ƒì´ ì¤‘ìš”í•˜ê³  ì–´ë–¤ ê²ƒì´ ë…¸ì´ì¦ˆì¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì •ë§ ì¤‘ìš”í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.â€ ê·¸ëŸ¬ë©´ ì²˜ìŒ ì§ˆë¬¸ìœ¼ë¡œ ëŒì•„ê°€ ë³´ì. ì¹¸í…Œ, ë§ˆë„¤ ê·¸ë¦¬ê³  íŒŒì˜ˆëŠ” ê¶ê·¹ì ìœ¼ë¡œ ê¸°ë¯¼í•œ ì¶•êµ¬ì ì¸ ê²°ì •ë“¤ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ì• ë„ë¦¬í‹±ìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì˜ ê¸°ì €ì— ìˆëŠ” í¼í¬ë¨¼ìŠ¤ ì§€í‘œë“¤ì˜ ìŠ¹ë¦¬ì˜€ë‹¤. ë¶„ëª…í•œ í†µê³„ì ì¸ ì¦ê±°ëŠ” í¬ë¡œìŠ¤ë“¤ì´ ìŠ¤ë£¨ ë³¼ ë³´ë‹¤ ì ì€ í™•ë¥ ì˜ ì „ìˆ ì´ë¼ëŠ” ê²ƒì´ê³  ë¨¼ ê±°ë¦¬ì—ì„œì˜ ìŠˆíŒ…ì€ ë” ë‚˜ì€ í¬ì§€ì…˜ìœ¼ë¡œ íŒ¨ìŠ¤í•˜ëŠ” ê²ƒ ë³´ë‹¤ ì ì€ ê³¨ì„ ìƒì‚°í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìˆ˜í•™ êµìˆ˜ì´ì ì‚¬ì»¤ë§¤í‹±ìŠ¤ì˜ ì €ì ë°ì´ë¹— ì„¬í„°ì— ë”°ë¥´ë©´, ë ˆìŠ¤í„°ì™€ ë¦¬ê·¸ì˜ ë‹¤ë¥¸ íŒ€ë“¤ê³¼ì˜ ì¶©ê²©ì ì¸ ì°¨ì´ëŠ” ì–´ë–»ê²Œ ê·¸ë“¤ì´ ìƒëŒ€ì ìœ¼ë¡œ ê¸¸ê³  ì§ì„ ì ì¸ íŒ¨ìŠ¤ë“¤ë¡œ ë³¼ì„ ì „ë°©ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì›€ì§ì´ëŠ”ì§€ì´ë‹¤. ê·¸ëŸ¬ë©´ í…Œì´ë¸”ì€ ì ˆëŒ€ ê±°ì§“ë§í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” í´ë¦¬ì…°ëŠ” ê±°ì§“ì¼ê¹Œ? ê¸€ì„, ì•„ë§ˆë„ ê·¸ê²ƒì€ ì§„ì‹¤ ì „ë¶€ë¥¼ ë§í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ê±°ì˜ ëª¨ë“  xGëª¨ë¸ì€ ì•„ìŠ¤ë‚ ì´ ì‚¬ì‹¤ ì—„ì²­ë‚œ ê¸°íšŒë¥¼ ë†“ì³¤ê³  ì„ ë‘ì— ìˆì—ˆì–´ì•¼ í•œë‹¤ê³  ë§í•˜ê³  ìˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ë§Œì•½ ì´ë²ˆ ì‹œì¦Œì´ ë¬´í•œí•œ ê²½ê¸° ìˆ˜ë¥¼ ê°€ì¡Œì„ ë•Œ ë ˆìŠ¤í„°ê°€ 4ìœ„ì—ì„œ 8ìœ„ ì‚¬ì´ì— ë†“ê³  ìˆë‹¤. ë¶„ì‚°ê³¼ ìš´ì€, 38ê²½ê¸° í”„ë¡œê·¸ë¨ì—ì„œ ìƒë‹¹í•œ ì–‘ìœ¼ë¡œ ë‚¨ì•„ìˆë‹¤. ì—¬ì „íˆ ê·¸ ì°¨ì´ë“¤ì€ ì¢ê³ , ë§Œì•½ ì§€ë‚œ í•´ê°€ ì–´ë–¤ ê²ƒë„ ìƒˆë¡œ ì¦ëª…í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ë©´, ì—´ì‹¬íˆ ì¼í•˜ëŠ” ê²ƒê³¼ ìŠ¤ë§ˆíŠ¸í•¨ì´ í´ëŸ½ì˜ ì€í–‰ ê³„ì¢Œì˜ ì‚¬ì´ì¦ˆ ë³´ë‹¤ ë” ì¤‘ìš”í•  ìˆ˜ ìˆë‹¤. ì¶œ ì²˜ : http://www.telegraph.co.uk/football/2016/04/16/the-numbers-dont-lie-why-football-clubs-place-such-importance-on/","link":"/2020/05/15/Data-Analytics-on-Football/"},{"title":"[MySQL] Storage Engine (InnoDB vs MyISAM)","text":"ìƒê°ì—†ì´ Engineì„ InnoDBë§Œ ì‚¬ìš©í–ˆì§€ ì™œ ì´ê²ƒì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ê³ ë¯¼í•´ë³¸ ì ì´ ì—†ì—ˆë‹¤. í•˜ì§€ë§Œ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì ì¬í•˜ë©´ ë°ì´í„°ì˜ ìˆ˜ê°€ ë§¤ìš° ë§ê±°ë‚˜ columnì˜ ê°¯ìˆ˜ê°€ ë§ì•„ì§€ë©´ ê³µê°„ë¶€ì¡± í˜„ìƒì´ ë‚˜íƒ€ë‚˜ê²Œ ë˜ê³ , ê·¸ë¡œ ì¸í•´ ì—”ì§„ì— ëŒ€í•´ ê³ ë¯¼í•˜ê¸° ì‹œì‘í•˜ì˜€ë‹¤. ëŒ€ëµ 500ê°œì— ë‹¬í•˜ëŠ” ì¹¼ëŸ¼ì´ í•„ìš”í•œ ìƒí™©ì´ì˜€ë‹¤. ê·¸ë˜ì„œ ì–´ë–»ê²Œ í…Œì´ë¸”ì— ì ì¬í•´ì•¼ íš¨ìœ¨ì ì¸ì§€ ê³ ë¯¼ì´ í•„ìš”í–ˆë‹¤. ë˜í•œ InnoDB í…Œì´ë¸”ì— ë§ì€ ì¹¼ëŸ¼ì„ ì¶”ê°€í•˜ë‹ˆ Row size too large. ë¼ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•´ì„œ Engineì„ ë³€ê²½í•˜ëŠ” ë°©ë²•ì„ ìƒê°í•˜ê²Œ ë˜ì—ˆë‹¤. ìš°ì„  ê° Stroage Engineì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ë‹¤. Mysql Storage Engineì€ ë¬¼ë¦¬ì  ì €ì¥ì¥ì¹˜ì—ì„œ ë°ì´í„°ë¥¼ ì–´ë–¤ ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ê³  ì½ì–´ì˜¬ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ 8ê°€ì§€ì˜ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì´ íƒ‘ì¬ë˜ì–´ ìˆìœ¼ë©° CREATE TABLEë¬¸ì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸”ì„ ìƒì„±í•  ë•Œ ì—”ì§„ ì´ë¦„ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ê°„ë‹¨í•˜ê²Œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ê·¸ ì¤‘ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ì—”ì§„ì€ InnoDB, MyISAM, Archive 3ê°€ì§€ì´ë‹¤. InnoDBí…Œì´ë¸” ìƒì„± ì‹œ, ë”°ë¡œ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì„ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ defaultë¡œ ì„¤ì •ë˜ëŠ” ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì´ë‹¤. InnoDBëŠ” íŠ¸ëœì­ì…˜(tranjection)ì„ ì§€ì›í•˜ê³ , ì»¤ë°‹(commit)ê³¼ ë¡¤ë°±(roll-back) ê·¸ë¦¬ê³  ë°ì´í„° ë³µêµ¬ ê¸°ëŠ¥ì„ ì œê³µí•˜ë¯€ë¡œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. InnoDBëŠ” ê¸°ë³¸ì ìœ¼ë¡œ row-level lockingì„ ì œê³µí•˜ë©°, ë˜í•œ ë°ì´í„°ë¥¼ clustered indexì— ì €ì¥í•˜ì—¬ PKê¸°ë°˜ì˜ queryì˜ ë¹„ìš©ì„ ì¤„ì¸ë‹¤. ë˜í•œ, PK ì œì•½ì„ ì œê³µí•˜ì—¬ ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•œë‹¤. ì—¬ê¸°ì„œ clustered indexì— ì €ì¥í•œë‹¤ëŠ” ê²ƒì€ ë°ì´í„°ë¥¼ PKìˆœì„œì— ë§ê²Œ ì €ì¥í•œë‹¤ëŠ” ëœ»ì´ë¯€ë¡œ order by ë“± ì¿¼ë¦¬ì— ìœ ë¦¬í•  ìˆ˜ ìˆë‹¤. ë˜í•œ row-level lockingì„ ì œê³µí•œë‹¤ëŠ” ëœ»ì€ í…Œì´ë¸”ì— CRUDí•  ë•Œ, ë¡œìš°ë³„ë¡œ ë½ì„ ì¡ê¸° ë•Œë¬¸ì— multi-threadì— ë³´ë‹¤ íš¨ìœ¨ì ì´ë¼ëŠ” ë§ì´ë‹¤. ë¬¼ë¡  ì¥ì ë§Œ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. InnoDBëŠ” ë”ìš± ë§ì€ ë©”ëª¨ë¦¬ì™€ ë””ìŠ¤í¬ë¥¼ ì‚¬ìš©í•œë‹¤. ë˜í•œ ë°ì´í„°ê°€ ê¹¨ì¡Œì„ ë•Œ ë‹¨ìˆœ íŒŒì¼ ë°±ì—…/ë³µêµ¬ë§Œìœ¼ë¡œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•œ MyISAMê³¼ ë‹¬ë¦¬ InnoDBì˜ ê²½ìš° ë³µêµ¬ ë°©ë²•ì´ ì–´ë µë‹¤. MyISAMì´ë‚˜ Memory ë°©ì‹ì´ ì§€ì›í•˜ì§€ ì•ŠëŠ” FKì˜ ê²½ìš°ë„ í…Œì´ë¸” ê°„ ë°ì´í„° ì²´í¬ë¡œ ì¸í•œ lock, íŠ¹íˆ dead lockì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. InnoDBì˜ ìµœëŒ€ í–‰ ì €ì¥ê³µê°„Mysql í…Œì´ë¸”ì˜ í–‰ í¬ê¸°ëŠ” ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì— ì œì•½ì´ ì—†ë‹¤ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ìµœëŒ€ 65535ë°”ì´íŠ¸ì´ë‹¤. í•˜ì§€ë§Œ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì— ë”°ë¼ ì œì•½ì´ ì¶”ê°€ë˜ì–´ í–‰ í¬ê¸°ëŠ” ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤. BLOB, TEXT ì»¬ëŸ¼ì˜ ë‚´ìš©ì€ í–‰ì˜ ë‚¨ì€ ë¶€ë¶„ì´ ì•„ë‹Œ ë³„ë„ì˜ ê³µê°„ì— ì €ì¥ë˜ê¸° ë•Œë¬¸ì— ê°ê° 912ë°”ì´íŠ¸ë§Œ ì˜í–¥ì„ ì¤€ë‹¤. (í¬ì¸íŠ¸ ì €ì¥ ê³µê°„ì´ 912ë°”ì´íŠ¸) ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ ìœ„ì˜ ë‚´ìš©ì€ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì— ìƒê´€ì—†ì´ ê¸°ë³¸ì ìœ¼ë¡œ ì´ë ‡ë‹¤ëŠ” ê²ƒì´ë‹¤. MyISAMíŠ¸ëœì­ì…˜(tranjection)ì„ ì§€ì›í•˜ì§€ ì•Šê³  table-level lockingì„ ì œê³µí•œë‹¤. ë”°ë¼ì„œ 1ê°œì˜ ROWì„ READí•˜ë”ë¼ë„ í…Œì´ë¸” ì „ì²´ì— ë½ì„ ì¡ê¸° ë•Œë¬¸ì— multi-thread í™˜ê²½ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ InnoDBì— ë¹„í•´ ê¸°ëŠ¥ì ìœ¼ë¡œ ë‹¨ìˆœí•˜ë¯€ë¡œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì€ InnoDBë³´ë‹¤ ì†ë„ë©´ì—ì„œ ìš°ì›”í•˜ë‹¤. ë‹¨ìˆœí•œ ì¡°íšŒì˜ ê²½ìš° MyISAMì´ InnoDBë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ, Order Byë“± ì •ë ¬ë“¤ì˜ êµ¬ë¬¸ì´ ë“¤ì–´ê°„ë‹¤ë©´ InnoDBë³´ë‹¤ ëŠë¦¬ë‹¤. ì™œëƒí•˜ë©´ InnoDBëŠ” í´ëŸ¬ìŠ¤í„°ë§ ì¸ë±ìŠ¤ì— ì €ì¥í•˜ê¸° ë•Œë¬¸ì— PKì— ë”°ë¼ ë°ì´í„° íŒŒì¼ì´ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ, MyISAMì€ ê·¸ë ‡ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. Full text searchingì„ ì§€ì›í•œë‹¤. MyISAM ì—”ì§„ì˜ ê²½ìš° ìµœëŒ€ í–‰ í¬ê¸°ê°€ ê¸°ë³¸ MySQL ì œì•½ì„ ë”°ë¥´ë¯€ë¡œ ìµœëŒ€ í–‰ í¬ê¸°ëŠ” 65535ë°”ì´íŠ¸ê°€ ë  ê²ƒì´ë‹¤. Archiveë¡œê·¸ ìˆ˜ì§‘ì— ì í•©í•œ ì—”ì§„ì´ë‹¤. ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ìƒì—ì„œ ì••ì¶•ë˜ê³  ì••ì¶•ëœ ìƒíƒœë¡œ ë””ìŠ¤í¬ì— ì €ì¥ë˜ê¸° ë•Œë¬¸ì— row-level lockingì´ ê°€ëŠ¥í•˜ë‹¤. ë‹¤ë§Œ, í•œë²ˆ INSERTëœ ë°ì´í„°ëŠ” UPDATE, DELETEë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©° ì¸ë±ìŠ¤ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ê±°ì˜ ê°€ê³µí•˜ì§€ ì•Šì„ ë°ì´í„°ì— ëŒ€í•´ì„œ ê´€ë¦¬í•˜ëŠ”ë°ì— íš¨ìœ¨ì ì¼ ìˆ˜ ìˆê³ , í…Œì´ë¸” íŒŒí‹°ì…”ë‹ë„ ì§€ì›í•œë‹¤. ë‹¤ë§Œ íŠ¸ëœì­ì…˜ì€ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.","link":"/2020/07/22/MySQL-Storage-Engine-InnoDB-vs-MyISAM/"},{"title":"[MySQL] Ubuntuì—ì„œ MySQL ì™„ì „ ì‚­ì œí•˜ê¸°","text":"MySQL Workbenchë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ í™˜ê²½ì„ ì„¤ì •í•˜ê³  ì‘ì—… ê³¼ì •ì—ì„œ ì‹œìŠ¤í…œ ê³„ì •ì„ ì‚­ì œí•˜ëŠ”(?) ì•„ì£¼ í° ë¬¸ì œê°€ ìƒê²¨ì„œ ì‚¬ìš©ì ìƒì„± ë“± ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•´ë´¤ì§€ë§Œ ë­”ê°€ ê¼¬ì¸ê²ƒ ê°™ì€ ëŠë‚Œì´ ë“¤ì—ˆë‹¤. Mysqlì„ ì‚­ì œí•˜ê³  ì¬ì„¤ì¹˜ê°€ í•„ìš”í• ë“¯ í•˜ì—¬ ì¬ì„¤ì¹˜ ë°©ë²•ì„ í¬ìŠ¤íŒ… í•˜ê³ ì í•œë‹¤. ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ì°¸ê³ í•˜ì.[Mysql]sudo apt-get purge mysql-server sudo apt-get purge mysql-common sudo rm -rf /var/log/mysql sudo rm -rf /var/log/mysql.* sudo rm -rf /var/lib/mysql sudo rm -rf /etc/mysql","link":"/2020/07/24/MySQL-Ubuntu%EC%97%90%EC%84%9C-MySQL-%EC%99%84%EC%A0%84-%EC%82%AD%EC%A0%9C%ED%95%98%EA%B8%B0/"},{"title":"Hyper-Parameter(Tuner)","text":"12345import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersimport IPython Keras Tunerë¥¼ ì„¤ì¹˜í•˜ê³  ê°€ì ¸ì˜µë‹ˆë‹¤. 12!pip install -q -U keras-tunerimport kerastuner as kt ë°ì´í„° ì„¸íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„ - ì´ ììŠµì„œì—ì„œëŠ” Keras Tunerë¥¼ ì‚¬ìš©í•˜ì—¬ Fashion MNIST ë°ì´í„° ì„¸íŠ¸ ì—ì„œ ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì— ê°€ì¥ ì í•©í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤. 123456# ë°ì´í„° ë¡œë“œ(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()# Normalize pixel values between 0 and 1img_train = img_train.astype('float32') / 255.0img_test = img_test.astype('float32') / 255.0 1img_train.shape, label_train.shape ((60000, 28, 28), (60000,))ëª¨ë¸ ì •ì˜ 12345678910111213141516171819def model_builder(hp): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) # Tune the number of units in the first Dense layer # Choose an optimal value between 32-512 hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32) model.add(keras.layers.Dense(units = hp_units, activation = 'relu')) model.add(keras.layers.Dense(10)) # Tune the learning rate for the optimizer # Choose an optimal value from 0.01, 0.001, or 0.0001 hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate), loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy']) return model íŠœë„ˆë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  í•˜ì´í¼ íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ íŠœë„ˆë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì—¬ í•˜ì´í¼ íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Hyperband Tunerì—ëŠ” RandomSearch , Hyperband , BayesianOptimization ë° Sklearn ë„¤ ê°€ì§€ íŠœë„ˆê°€ ìˆìŠµë‹ˆë‹¤. ì´ ììŠµì„œì—ì„œëŠ” í•˜ì´í¼ ë°´ë“œ íŠœë„ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì´í¼ ë°´ë“œ íŠœë„ˆë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ë ¤ë©´ í•˜ì´í¼ ëª¨ë¸, ìµœì í™” í•  objective ë° í›ˆë ¨ í•  ìµœëŒ€ max_epochs ( max_epochs )ë¥¼ ì§€ì •í•´ì•¼í•©ë‹ˆë‹¤.123456tuner = kt.Hyperband(model_builder, objective = 'val_accuracy', max_epochs = 10, factor = 3, directory = 'my_dir', project_name = 'intro_to_kt') INFO:tensorflow:Reloading Oracle from existing project my_dir/intro_to_kt/oracle.json INFO:tensorflow:Reloading Tuner from my_dir/intro_to_kt/tuner0.json123class ClearTrainingOutput(tf.keras.callbacks.Callback): def on_train_end(*args, **kwargs): IPython.display.clear_output(wait = True) 12345678910tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])# Get the optimal hyperparametersbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connectedlayer is {best_hps.get('units')} and the optimal learning rate for the optimizeris {best_hps.get('learning_rate')}.\"\"\") Trial complete Trial summary |-Trial ID: a07676c4549fc425444c7c101819cb0a |-Score: 0.8574000000953674 |-Best step: 0 Hyperparameters: |-learning_rate: 0.0001 |-tuner/bracket: 0 |-tuner/epochs: 10 |-tuner/initial_epoch: 0 |-tuner/round: 0 |-units: 64 INFO:tensorflow:Oracle triggered exit The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is 384 and the optimal learning rate for the optimizer is 0.001.123# Build the model with the optimal hyperparameters and train it on the datamodel = tuner.hypermodel.build(best_hps)model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test)) Epoch 1/10 1875/1875 [==============================] - ETA: 8:42 - loss: 2.1883 - accuracy: 0.18 - ETA: 20s - loss: 1.5613 - accuracy: 0.4721 - ETA: 11s - loss: 1.3144 - accuracy: 0.550 - ETA: 9s - loss: 1.1853 - accuracy: 0.591 - ETA: 7s - loss: 1.1046 - accuracy: 0.61 - ETA: 6s - loss: 1.0539 - accuracy: 0.63 - ETA: 6s - loss: 1.0195 - accuracy: 0.64 - ETA: 6s - loss: 0.9893 - accuracy: 0.65 - ETA: 5s - loss: 0.9689 - accuracy: 0.66 - ETA: 5s - loss: 0.9544 - accuracy: 0.67 - ETA: 6s - loss: 0.9429 - accuracy: 0.67 - ETA: 5s - loss: 0.9283 - accuracy: 0.68 - ETA: 5s - loss: 0.9130 - accuracy: 0.68 - ETA: 5s - loss: 0.9015 - accuracy: 0.68 - ETA: 5s - loss: 0.8923 - accuracy: 0.69 - ETA: 5s - loss: 0.8822 - accuracy: 0.69 - ETA: 5s - loss: 0.8716 - accuracy: 0.69 - ETA: 5s - loss: 0.8618 - accuracy: 0.70 - ETA: 5s - loss: 0.8483 - accuracy: 0.70 - ETA: 5s - loss: 0.8360 - accuracy: 0.71 - ETA: 4s - loss: 0.8242 - accuracy: 0.71 - ETA: 4s - loss: 0.8147 - accuracy: 0.71 - ETA: 4s - loss: 0.8061 - accuracy: 0.72 - ETA: 4s - loss: 0.8001 - accuracy: 0.72 - ETA: 4s - loss: 0.7961 - accuracy: 0.72 - ETA: 4s - loss: 0.7900 - accuracy: 0.72 - ETA: 4s - loss: 0.7842 - accuracy: 0.72 - ETA: 4s - loss: 0.7790 - accuracy: 0.73 - ETA: 4s - loss: 0.7746 - accuracy: 0.73 - ETA: 4s - loss: 0.7703 - accuracy: 0.73 - ETA: 4s - loss: 0.7662 - accuracy: 0.73 - ETA: 4s - loss: 0.7617 - accuracy: 0.73 - ETA: 4s - loss: 0.7573 - accuracy: 0.73 - ETA: 4s - loss: 0.7536 - accuracy: 0.73 - ETA: 4s - loss: 0.7493 - accuracy: 0.73 - ETA: 4s - loss: 0.7453 - accuracy: 0.74 - ETA: 3s - loss: 0.7407 - accuracy: 0.74 - ETA: 3s - loss: 0.7365 - accuracy: 0.74 - ETA: 3s - loss: 0.7324 - accuracy: 0.74 - ETA: 3s - loss: 0.7288 - accuracy: 0.74 - ETA: 3s - loss: 0.7252 - accuracy: 0.74 - ETA: 3s - loss: 0.7212 - accuracy: 0.74 - ETA: 3s - loss: 0.7171 - accuracy: 0.75 - ETA: 3s - loss: 0.7132 - accuracy: 0.75 - ETA: 3s - loss: 0.7094 - accuracy: 0.75 - ETA: 3s - loss: 0.7059 - accuracy: 0.75 - ETA: 3s - loss: 0.7021 - accuracy: 0.75 - ETA: 3s - loss: 0.6986 - accuracy: 0.75 - ETA: 3s - loss: 0.6962 - accuracy: 0.75 - ETA: 3s - loss: 0.6929 - accuracy: 0.75 - ETA: 2s - loss: 0.6897 - accuracy: 0.75 - ETA: 2s - loss: 0.6866 - accuracy: 0.76 - ETA: 2s - loss: 0.6839 - accuracy: 0.76 - ETA: 2s - loss: 0.6810 - accuracy: 0.76 - ETA: 2s - loss: 0.6781 - accuracy: 0.76 - ETA: 2s - loss: 0.6753 - accuracy: 0.76 - ETA: 2s - loss: 0.6728 - accuracy: 0.76 - ETA: 2s - loss: 0.6704 - accuracy: 0.76 - ETA: 2s - loss: 0.6682 - accuracy: 0.76 - ETA: 2s - loss: 0.6657 - accuracy: 0.76 - ETA: 2s - loss: 0.6633 - accuracy: 0.76 - ETA: 2s - loss: 0.6607 - accuracy: 0.76 - ETA: 2s - loss: 0.6584 - accuracy: 0.76 - ETA: 2s - loss: 0.6561 - accuracy: 0.77 - ETA: 2s - loss: 0.6541 - accuracy: 0.77 - ETA: 2s - loss: 0.6521 - accuracy: 0.77 - ETA: 1s - loss: 0.6500 - accuracy: 0.77 - ETA: 1s - loss: 0.6479 - accuracy: 0.77 - ETA: 1s - loss: 0.6457 - accuracy: 0.77 - ETA: 1s - loss: 0.6433 - accuracy: 0.77 - ETA: 1s - loss: 0.6410 - accuracy: 0.77 - ETA: 1s - loss: 0.6388 - accuracy: 0.77 - ETA: 1s - loss: 0.6366 - accuracy: 0.77 - ETA: 1s - loss: 0.6347 - accuracy: 0.77 - ETA: 1s - loss: 0.6324 - accuracy: 0.77 - ETA: 1s - loss: 0.6302 - accuracy: 0.77 - ETA: 1s - loss: 0.6281 - accuracy: 0.77 - ETA: 1s - loss: 0.6260 - accuracy: 0.78 - ETA: 1s - loss: 0.6240 - accuracy: 0.78 - ETA: 1s - loss: 0.6221 - accuracy: 0.78 - ETA: 1s - loss: 0.6202 - accuracy: 0.78 - ETA: 0s - loss: 0.6182 - accuracy: 0.78 - ETA: 0s - loss: 0.6163 - accuracy: 0.78 - ETA: 0s - loss: 0.6144 - accuracy: 0.78 - ETA: 0s - loss: 0.6126 - accuracy: 0.78 - ETA: 0s - loss: 0.6108 - accuracy: 0.78 - ETA: 0s - loss: 0.6092 - accuracy: 0.78 - ETA: 0s - loss: 0.6077 - accuracy: 0.78 - ETA: 0s - loss: 0.6061 - accuracy: 0.78 - ETA: 0s - loss: 0.6043 - accuracy: 0.78 - ETA: 0s - loss: 0.6026 - accuracy: 0.78 - ETA: 0s - loss: 0.6010 - accuracy: 0.78 - ETA: 0s - loss: 0.5995 - accuracy: 0.78 - ETA: 0s - loss: 0.5981 - accuracy: 0.78 - ETA: 0s - loss: 0.5965 - accuracy: 0.79 - 6s 3ms/step - loss: 0.5951 - accuracy: 0.7907 - val_loss: 0.4127 - val_accuracy: 0.8468 Epoch 2/10 1875/1875 [==============================] - ETA: 8s - loss: 0.4197 - accuracy: 0.84 - ETA: 3s - loss: 0.4062 - accuracy: 0.85 - ETA: 3s - loss: 0.3977 - accuracy: 0.85 - ETA: 3s - loss: 0.3848 - accuracy: 0.86 - ETA: 3s - loss: 0.3777 - accuracy: 0.86 - ETA: 3s - loss: 0.3730 - accuracy: 0.86 - ETA: 3s - loss: 0.3704 - accuracy: 0.86 - ETA: 3s - loss: 0.3683 - accuracy: 0.86 - ETA: 3s - loss: 0.3674 - accuracy: 0.86 - ETA: 3s - loss: 0.3670 - accuracy: 0.86 - ETA: 3s - loss: 0.3664 - accuracy: 0.86 - ETA: 3s - loss: 0.3659 - accuracy: 0.86 - ETA: 3s - loss: 0.3653 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3650 - accuracy: 0.86 - ETA: 3s - loss: 0.3651 - accuracy: 0.86 - ETA: 3s - loss: 0.3652 - accuracy: 0.86 - ETA: 3s - loss: 0.3654 - accuracy: 0.86 - ETA: 3s - loss: 0.3655 - accuracy: 0.86 - ETA: 2s - loss: 0.3657 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3661 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3660 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3659 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 2s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3659 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3658 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 1s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3657 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3656 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3655 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3654 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3653 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3652 - accuracy: 0.86 - ETA: 0s - loss: 0.3651 - accuracy: 0.86 - 4s 2ms/step - loss: 0.3651 - accuracy: 0.8674 - val_loss: 0.3735 - val_accuracy: 0.8663 Epoch 3/10 1875/1875 [==============================] - ETA: 7s - loss: 0.2668 - accuracy: 0.87 - ETA: 4s - loss: 0.2867 - accuracy: 0.88 - ETA: 4s - loss: 0.2901 - accuracy: 0.89 - ETA: 4s - loss: 0.2983 - accuracy: 0.89 - ETA: 3s - loss: 0.3031 - accuracy: 0.89 - ETA: 3s - loss: 0.3054 - accuracy: 0.89 - ETA: 3s - loss: 0.3067 - accuracy: 0.89 - ETA: 3s - loss: 0.3087 - accuracy: 0.88 - ETA: 3s - loss: 0.3110 - accuracy: 0.88 - ETA: 3s - loss: 0.3125 - accuracy: 0.88 - ETA: 3s - loss: 0.3140 - accuracy: 0.88 - ETA: 3s - loss: 0.3150 - accuracy: 0.88 - ETA: 3s - loss: 0.3159 - accuracy: 0.88 - ETA: 3s - loss: 0.3167 - accuracy: 0.88 - ETA: 3s - loss: 0.3174 - accuracy: 0.88 - ETA: 3s - loss: 0.3180 - accuracy: 0.88 - ETA: 3s - loss: 0.3185 - accuracy: 0.88 - ETA: 3s - loss: 0.3189 - accuracy: 0.88 - ETA: 3s - loss: 0.3194 - accuracy: 0.88 - ETA: 3s - loss: 0.3199 - accuracy: 0.88 - ETA: 3s - loss: 0.3202 - accuracy: 0.88 - ETA: 3s - loss: 0.3205 - accuracy: 0.88 - ETA: 3s - loss: 0.3208 - accuracy: 0.88 - ETA: 3s - loss: 0.3212 - accuracy: 0.88 - ETA: 3s - loss: 0.3214 - accuracy: 0.88 - ETA: 3s - loss: 0.3215 - accuracy: 0.88 - ETA: 2s - loss: 0.3217 - accuracy: 0.88 - ETA: 2s - loss: 0.3219 - accuracy: 0.88 - ETA: 2s - loss: 0.3220 - accuracy: 0.88 - ETA: 2s - loss: 0.3221 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3223 - accuracy: 0.88 - ETA: 2s - loss: 0.3224 - accuracy: 0.88 - ETA: 2s - loss: 0.3225 - accuracy: 0.88 - ETA: 2s - loss: 0.3226 - accuracy: 0.88 - ETA: 2s - loss: 0.3227 - accuracy: 0.88 - ETA: 2s - loss: 0.3229 - accuracy: 0.88 - ETA: 2s - loss: 0.3230 - accuracy: 0.88 - ETA: 2s - loss: 0.3232 - accuracy: 0.88 - ETA: 2s - loss: 0.3233 - accuracy: 0.88 - ETA: 1s - loss: 0.3234 - accuracy: 0.88 - ETA: 1s - loss: 0.3235 - accuracy: 0.88 - ETA: 1s - loss: 0.3236 - accuracy: 0.88 - ETA: 1s - loss: 0.3237 - accuracy: 0.88 - ETA: 1s - loss: 0.3239 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3241 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3242 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3243 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3242 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3241 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3240 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3239 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - 4s 2ms/step - loss: 0.3238 - accuracy: 0.8810 - val_loss: 0.3695 - val_accuracy: 0.8646 Epoch 4/10 1875/1875 [==============================] - ETA: 7s - loss: 0.2077 - accuracy: 0.87 - ETA: 4s - loss: 0.3105 - accuracy: 0.85 - ETA: 4s - loss: 0.3010 - accuracy: 0.86 - ETA: 3s - loss: 0.2976 - accuracy: 0.87 - ETA: 3s - loss: 0.2957 - accuracy: 0.87 - ETA: 3s - loss: 0.2952 - accuracy: 0.88 - ETA: 3s - loss: 0.2951 - accuracy: 0.88 - ETA: 3s - loss: 0.2948 - accuracy: 0.88 - ETA: 3s - loss: 0.2947 - accuracy: 0.88 - ETA: 3s - loss: 0.2943 - accuracy: 0.88 - ETA: 3s - loss: 0.2940 - accuracy: 0.88 - ETA: 3s - loss: 0.2937 - accuracy: 0.88 - ETA: 3s - loss: 0.2934 - accuracy: 0.88 - ETA: 3s - loss: 0.2931 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 3s - loss: 0.2925 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2923 - accuracy: 0.88 - ETA: 3s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2924 - accuracy: 0.88 - ETA: 2s - loss: 0.2925 - accuracy: 0.88 - ETA: 2s - loss: 0.2926 - accuracy: 0.88 - ETA: 3s - loss: 0.2927 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2928 - accuracy: 0.88 - ETA: 2s - loss: 0.2929 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2930 - accuracy: 0.88 - ETA: 2s - loss: 0.2931 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2932 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2933 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 2s - loss: 0.2934 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2935 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2936 - accuracy: 0.88 - ETA: 1s - loss: 0.2937 - accuracy: 0.88 - ETA: 1s - loss: 0.2938 - accuracy: 0.88 - ETA: 1s - loss: 0.2939 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2940 - accuracy: 0.89 - ETA: 1s - loss: 0.2941 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2942 - accuracy: 0.89 - ETA: 1s - loss: 0.2943 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2944 - accuracy: 0.88 - ETA: 1s - loss: 0.2945 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2946 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2947 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2948 - accuracy: 0.88 - ETA: 0s - loss: 0.2949 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2950 - accuracy: 0.88 - ETA: 0s - loss: 0.2951 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2952 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2953 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2954 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2955 - accuracy: 0.88 - ETA: 0s - loss: 0.2956 - accuracy: 0.88 - 4s 2ms/step - loss: 0.2956 - accuracy: 0.8898 - val_loss: 0.3532 - val_accuracy: 0.8745 Epoch 5/10 1875/1875 [==============================] - ETA: 7s - loss: 0.3435 - accuracy: 0.87 - ETA: 4s - loss: 0.3009 - accuracy: 0.89 - ETA: 3s - loss: 0.2893 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2894 - accuracy: 0.89 - ETA: 3s - loss: 0.2882 - accuracy: 0.89 - ETA: 3s - loss: 0.2872 - accuracy: 0.89 - ETA: 3s - loss: 0.2868 - accuracy: 0.89 - ETA: 3s - loss: 0.2870 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 3s - loss: 0.2885 - accuracy: 0.89 - ETA: 3s - loss: 0.2888 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2889 - accuracy: 0.89 - ETA: 3s - loss: 0.2887 - accuracy: 0.89 - ETA: 3s - loss: 0.2883 - accuracy: 0.89 - ETA: 3s - loss: 0.2881 - accuracy: 0.89 - ETA: 3s - loss: 0.2880 - accuracy: 0.89 - ETA: 3s - loss: 0.2879 - accuracy: 0.89 - ETA: 3s - loss: 0.2877 - accuracy: 0.89 - ETA: 2s - loss: 0.2875 - accuracy: 0.89 - ETA: 2s - loss: 0.2872 - accuracy: 0.89 - ETA: 2s - loss: 0.2870 - accuracy: 0.89 - ETA: 2s - loss: 0.2867 - accuracy: 0.89 - ETA: 2s - loss: 0.2865 - accuracy: 0.89 - ETA: 2s - loss: 0.2863 - accuracy: 0.89 - ETA: 2s - loss: 0.2861 - accuracy: 0.89 - ETA: 2s - loss: 0.2859 - accuracy: 0.89 - ETA: 2s - loss: 0.2858 - accuracy: 0.89 - ETA: 2s - loss: 0.2856 - accuracy: 0.89 - ETA: 2s - loss: 0.2854 - accuracy: 0.89 - ETA: 2s - loss: 0.2852 - accuracy: 0.89 - ETA: 2s - loss: 0.2851 - accuracy: 0.89 - ETA: 2s - loss: 0.2850 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2849 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 2s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2846 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2843 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2842 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2841 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2840 - accuracy: 0.89 - ETA: 0s - loss: 0.2839 - accuracy: 0.89 - 5s 2ms/step - loss: 0.2839 - accuracy: 0.8954 - val_loss: 0.3473 - val_accuracy: 0.8731 Epoch 6/10 1875/1875 [==============================] - ETA: 8s - loss: 0.1747 - accuracy: 0.93 - ETA: 4s - loss: 0.2120 - accuracy: 0.92 - ETA: 4s - loss: 0.2342 - accuracy: 0.91 - ETA: 4s - loss: 0.2445 - accuracy: 0.91 - ETA: 4s - loss: 0.2483 - accuracy: 0.91 - ETA: 4s - loss: 0.2509 - accuracy: 0.91 - ETA: 3s - loss: 0.2532 - accuracy: 0.90 - ETA: 3s - loss: 0.2548 - accuracy: 0.90 - ETA: 3s - loss: 0.2559 - accuracy: 0.90 - ETA: 3s - loss: 0.2568 - accuracy: 0.90 - ETA: 3s - loss: 0.2574 - accuracy: 0.90 - ETA: 3s - loss: 0.2578 - accuracy: 0.90 - ETA: 3s - loss: 0.2582 - accuracy: 0.90 - ETA: 3s - loss: 0.2586 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2587 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 3s - loss: 0.2588 - accuracy: 0.90 - ETA: 2s - loss: 0.2589 - accuracy: 0.90 - ETA: 2s - loss: 0.2590 - accuracy: 0.90 - ETA: 2s - loss: 0.2591 - accuracy: 0.90 - ETA: 2s - loss: 0.2592 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2593 - accuracy: 0.90 - ETA: 2s - loss: 0.2594 - accuracy: 0.90 - ETA: 2s - loss: 0.2595 - accuracy: 0.90 - ETA: 2s - loss: 0.2596 - accuracy: 0.90 - ETA: 2s - loss: 0.2597 - accuracy: 0.90 - ETA: 2s - loss: 0.2599 - accuracy: 0.90 - ETA: 2s - loss: 0.2601 - accuracy: 0.90 - ETA: 2s - loss: 0.2602 - accuracy: 0.90 - ETA: 2s - loss: 0.2604 - accuracy: 0.90 - ETA: 2s - loss: 0.2605 - accuracy: 0.90 - ETA: 2s - loss: 0.2606 - accuracy: 0.90 - ETA: 2s - loss: 0.2608 - accuracy: 0.90 - ETA: 2s - loss: 0.2609 - accuracy: 0.90 - ETA: 1s - loss: 0.2610 - accuracy: 0.90 - ETA: 1s - loss: 0.2611 - accuracy: 0.90 - ETA: 1s - loss: 0.2612 - accuracy: 0.90 - ETA: 1s - loss: 0.2613 - accuracy: 0.90 - ETA: 1s - loss: 0.2614 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2615 - accuracy: 0.90 - ETA: 1s - loss: 0.2616 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2617 - accuracy: 0.90 - ETA: 1s - loss: 0.2618 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2619 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2620 - accuracy: 0.90 - ETA: 1s - loss: 0.2621 - accuracy: 0.90 - ETA: 1s - loss: 0.2622 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 1s - loss: 0.2623 - accuracy: 0.90 - ETA: 0s - loss: 0.2624 - accuracy: 0.90 - ETA: 0s - loss: 0.2625 - accuracy: 0.90 - ETA: 0s - loss: 0.2626 - accuracy: 0.90 - ETA: 0s - loss: 0.2627 - accuracy: 0.90 - ETA: 0s - loss: 0.2628 - accuracy: 0.90 - ETA: 0s - loss: 0.2629 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2630 - accuracy: 0.90 - ETA: 0s - loss: 0.2631 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2632 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2633 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2634 - accuracy: 0.90 - ETA: 0s - loss: 0.2635 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2635 - accuracy: 0.9027 - val_loss: 0.3392 - val_accuracy: 0.8760 Epoch 7/10 1875/1875 [==============================] - ETA: 8s - loss: 0.2374 - accuracy: 0.90 - ETA: 3s - loss: 0.2432 - accuracy: 0.91 - ETA: 3s - loss: 0.2501 - accuracy: 0.91 - ETA: 3s - loss: 0.2520 - accuracy: 0.91 - ETA: 3s - loss: 0.2517 - accuracy: 0.91 - ETA: 3s - loss: 0.2506 - accuracy: 0.91 - ETA: 3s - loss: 0.2494 - accuracy: 0.91 - ETA: 3s - loss: 0.2490 - accuracy: 0.91 - ETA: 3s - loss: 0.2492 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2493 - accuracy: 0.91 - ETA: 3s - loss: 0.2491 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2488 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2485 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2483 - accuracy: 0.91 - ETA: 2s - loss: 0.2484 - accuracy: 0.91 - ETA: 2s - loss: 0.2486 - accuracy: 0.91 - ETA: 2s - loss: 0.2487 - accuracy: 0.91 - ETA: 2s - loss: 0.2489 - accuracy: 0.91 - ETA: 2s - loss: 0.2490 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2491 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2492 - accuracy: 0.91 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2493 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 1s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2495 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2496 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2497 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2498 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2499 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2500 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - ETA: 0s - loss: 0.2501 - accuracy: 0.90 - 4s 2ms/step - loss: 0.2501 - accuracy: 0.9087 - val_loss: 0.3413 - val_accuracy: 0.8810 Epoch 8/10 1875/1875 [==============================] - ETA: 11s - loss: 0.1224 - accuracy: 0.937 - ETA: 5s - loss: 0.2162 - accuracy: 0.927 - ETA: 5s - loss: 0.2236 - accuracy: 0.92 - ETA: 5s - loss: 0.2277 - accuracy: 0.91 - ETA: 4s - loss: 0.2284 - accuracy: 0.91 - ETA: 4s - loss: 0.2283 - accuracy: 0.91 - ETA: 3s - loss: 0.2286 - accuracy: 0.91 - ETA: 3s - loss: 0.2295 - accuracy: 0.91 - ETA: 3s - loss: 0.2307 - accuracy: 0.91 - ETA: 3s - loss: 0.2317 - accuracy: 0.91 - ETA: 3s - loss: 0.2323 - accuracy: 0.91 - ETA: 3s - loss: 0.2324 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2325 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 3s - loss: 0.2326 - accuracy: 0.91 - ETA: 2s - loss: 0.2327 - accuracy: 0.91 - ETA: 2s - loss: 0.2329 - accuracy: 0.91 - ETA: 2s - loss: 0.2332 - accuracy: 0.91 - ETA: 2s - loss: 0.2335 - accuracy: 0.91 - ETA: 2s - loss: 0.2338 - accuracy: 0.91 - ETA: 2s - loss: 0.2340 - accuracy: 0.91 - ETA: 2s - loss: 0.2342 - accuracy: 0.91 - ETA: 2s - loss: 0.2343 - accuracy: 0.91 - ETA: 2s - loss: 0.2345 - accuracy: 0.91 - ETA: 2s - loss: 0.2347 - accuracy: 0.91 - ETA: 2s - loss: 0.2349 - accuracy: 0.91 - ETA: 2s - loss: 0.2351 - accuracy: 0.91 - ETA: 2s - loss: 0.2353 - accuracy: 0.91 - ETA: 2s - loss: 0.2355 - accuracy: 0.91 - ETA: 2s - loss: 0.2356 - accuracy: 0.91 - ETA: 2s - loss: 0.2357 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2359 - accuracy: 0.91 - ETA: 1s - loss: 0.2360 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2361 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2362 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2363 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2364 - accuracy: 0.91 - ETA: 1s - loss: 0.2365 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2366 - accuracy: 0.91 - ETA: 1s - loss: 0.2367 - accuracy: 0.91 - ETA: 1s - loss: 0.2369 - accuracy: 0.91 - ETA: 1s - loss: 0.2370 - accuracy: 0.91 - ETA: 1s - loss: 0.2371 - accuracy: 0.91 - ETA: 1s - loss: 0.2372 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2373 - accuracy: 0.91 - ETA: 1s - loss: 0.2374 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2375 - accuracy: 0.91 - ETA: 0s - loss: 0.2376 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2377 - accuracy: 0.91 - ETA: 0s - loss: 0.2378 - accuracy: 0.91 - ETA: 0s - loss: 0.2379 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2380 - accuracy: 0.91 - ETA: 0s - loss: 0.2381 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2382 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2383 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2384 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2385 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - ETA: 0s - loss: 0.2386 - accuracy: 0.91 - 5s 2ms/step - loss: 0.2386 - accuracy: 0.9109 - val_loss: 0.3364 - val_accuracy: 0.8836 Epoch 9/10 1875/1875 [==============================] - ETA: 11s - loss: 0.3092 - accuracy: 0.843 - ETA: 4s - loss: 0.2166 - accuracy: 0.911 - ETA: 4s - loss: 0.2219 - accuracy: 0.91 - ETA: 4s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2247 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2252 - accuracy: 0.91 - ETA: 3s - loss: 0.2254 - accuracy: 0.91 - ETA: 3s - loss: 0.2260 - accuracy: 0.91 - ETA: 3s - loss: 0.2264 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2271 - accuracy: 0.91 - ETA: 3s - loss: 0.2270 - accuracy: 0.91 - ETA: 3s - loss: 0.2268 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2265 - accuracy: 0.91 - ETA: 3s - loss: 0.2266 - accuracy: 0.91 - ETA: 3s - loss: 0.2267 - accuracy: 0.91 - ETA: 3s - loss: 0.2269 - accuracy: 0.91 - ETA: 2s - loss: 0.2271 - accuracy: 0.91 - ETA: 2s - loss: 0.2273 - accuracy: 0.91 - ETA: 2s - loss: 0.2275 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2276 - accuracy: 0.91 - ETA: 2s - loss: 0.2277 - accuracy: 0.91 - ETA: 2s - loss: 0.2278 - accuracy: 0.91 - ETA: 2s - loss: 0.2279 - accuracy: 0.91 - ETA: 2s - loss: 0.2280 - accuracy: 0.91 - ETA: 2s - loss: 0.2282 - accuracy: 0.91 - ETA: 2s - loss: 0.2283 - accuracy: 0.91 - ETA: 2s - loss: 0.2284 - accuracy: 0.91 - ETA: 2s - loss: 0.2286 - accuracy: 0.91 - ETA: 2s - loss: 0.2287 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2288 - accuracy: 0.91 - ETA: 2s - loss: 0.2289 - accuracy: 0.91 - ETA: 1s - loss: 0.2290 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2291 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2293 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2294 - accuracy: 0.91 - ETA: 1s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2295 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2296 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2297 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2298 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - ETA: 0s - loss: 0.2299 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2299 - accuracy: 0.9138 - val_loss: 0.3266 - val_accuracy: 0.8846 Epoch 10/10 1875/1875 [==============================] - ETA: 6s - loss: 0.1648 - accuracy: 0.93 - ETA: 3s - loss: 0.1805 - accuracy: 0.93 - ETA: 3s - loss: 0.1840 - accuracy: 0.93 - ETA: 3s - loss: 0.1904 - accuracy: 0.92 - ETA: 3s - loss: 0.1970 - accuracy: 0.92 - ETA: 3s - loss: 0.2009 - accuracy: 0.92 - ETA: 3s - loss: 0.2040 - accuracy: 0.92 - ETA: 3s - loss: 0.2060 - accuracy: 0.92 - ETA: 3s - loss: 0.2075 - accuracy: 0.92 - ETA: 3s - loss: 0.2087 - accuracy: 0.92 - ETA: 3s - loss: 0.2099 - accuracy: 0.92 - ETA: 3s - loss: 0.2112 - accuracy: 0.92 - ETA: 3s - loss: 0.2121 - accuracy: 0.92 - ETA: 3s - loss: 0.2127 - accuracy: 0.92 - ETA: 3s - loss: 0.2132 - accuracy: 0.92 - ETA: 3s - loss: 0.2135 - accuracy: 0.92 - ETA: 3s - loss: 0.2138 - accuracy: 0.92 - ETA: 3s - loss: 0.2141 - accuracy: 0.92 - ETA: 3s - loss: 0.2144 - accuracy: 0.92 - ETA: 3s - loss: 0.2146 - accuracy: 0.91 - ETA: 3s - loss: 0.2148 - accuracy: 0.91 - ETA: 3s - loss: 0.2151 - accuracy: 0.91 - ETA: 3s - loss: 0.2154 - accuracy: 0.91 - ETA: 3s - loss: 0.2157 - accuracy: 0.91 - ETA: 3s - loss: 0.2159 - accuracy: 0.91 - ETA: 2s - loss: 0.2161 - accuracy: 0.91 - ETA: 2s - loss: 0.2163 - accuracy: 0.91 - ETA: 2s - loss: 0.2165 - accuracy: 0.91 - ETA: 2s - loss: 0.2167 - accuracy: 0.91 - ETA: 2s - loss: 0.2168 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2171 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2172 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2173 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2174 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2177 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2179 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2180 - accuracy: 0.91 - ETA: 1s - loss: 0.2181 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2182 - accuracy: 0.91 - ETA: 1s - loss: 0.2183 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2184 - accuracy: 0.91 - ETA: 1s - loss: 0.2185 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2186 - accuracy: 0.91 - ETA: 1s - loss: 0.2187 - accuracy: 0.91 - ETA: 1s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2188 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2189 - accuracy: 0.91 - ETA: 0s - loss: 0.2190 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2191 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2192 - accuracy: 0.91 - ETA: 0s - loss: 0.2193 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2194 - accuracy: 0.91 - ETA: 0s - loss: 0.2195 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2196 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2197 - accuracy: 0.91 - ETA: 0s - loss: 0.2198 - accuracy: 0.91 - 4s 2ms/step - loss: 0.2198 - accuracy: 0.9179 - val_loss: 0.3605 - val_accuracy: 0.8808 &lt;tensorflow.python.keras.callbacks.History at 0x7fdcd087f710&gt;12 12 12","link":"/2020/08/26/Hyper-Parameter-Tuner/"},{"title":"[mysql] workbenchì—ì„œ ERDíˆ´ ì‚¬ìš©í•˜ê¸° (Database Modeling)","text":"ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ë§ (Database Modeling)ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ë§(ë˜ëŠ” ë°ì´í„° ëª¨ë¸ë§)ì´ë€ í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì‘ì—…ì´ë‚˜ ì‚¬ë¬¼ë“¤ì„ DBMSì˜ ë°ì´í„°ë² ì´ìŠ¤ ê°œì²´ë¡œ ì˜®ê¸°ê¸° ìœ„í•œ ê³¼ì •ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ë§ì€ ëª¨ë¸ë§ì„ í•˜ëŠ” ì‚¬ëŒì´ ì–´ë–¤ ì‚¬ëŒì´ëƒì— ë”°ë¼ì„œ ê°ê¸° ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ë°–ì— ì—†ê³  â€˜ë§ì€ ì‹¤ë¬´ ê²½í—˜ê³¼ ì§€ì‹ì„ ê°€ì§„ ì‚¬ëŒì´ ë” ì¢‹ì€ ëª¨ë¸ë§ì„ í•œë‹¤â€™ ë¼ê³  í•©ë‹ˆë‹¤. 3ê°€ì§€ì˜ ëª¨ë¸ë§ ë°©ì‹ì´ ìˆë‹¤. - ê°œë…ì  ëª¨ë¸ë§ - ë…¼ë¦¬ì  ëª¨ë¸ë§ - ë¬¼ë¦¬ì  ëª¨ë¸ë§ ê°œë…ì  ëª¨ë¸ë§ì€ ì£¼ë¡œ ì—…ë¬´ ë¶„ì„ ë‹¨ê³„ì—ì„œ ì§„í–‰ë˜ë©° ë…¼ë¦¬ì  ëª¨ë¸ë§ì€ ì—…ë¬´ ë¶„ì„ì˜ í›„ë°˜ë¶€ì™€ ì‹œìŠ¤í…œ ì„¤ê³„ë¥¼ í•˜ëŠ” ë¶€ë¶„ì— ê±¸ì³ì„œ ì§„í–‰ëœë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë¬¼ë¦¬ì  ëª¨ë¸ë§ì€ ì‹œìŠ¤í…œ ì„¤ê³„ ë‹¨ê³„ì˜ í›„ë°˜ë¶€ì—ì„œ ì£¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë‘ ì ˆëŒ€ì ì¸ ê²ƒì€ ì•„ë‹ˆë©° ì‚¬ëŒì— ë”°ë¼ ì¡°ê¸ˆì”© ì°¨ì´ë¥¼ ë³´ì´ê¸°ë„ í•©ë‹ˆë‹¤. ì›Œí¬ë²¤ì¹˜ëŠ” ì´ ëª¨ë¸ë§ íˆ´ì„ ì œê³µí•´ì£¼ëŠ”ë° ê·¸ê²ƒì´ ë°”ë¡œ ERDíˆ´ì…ë‹ˆë‹¤. ERDë€?Entity Relationship Diagramì˜ ì•½ìë¡œ, ê°œì²´ê´€ê³„ë„ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤ ì¥ì  ë§Œë“¤ê³ ì í•˜ëŠ” ë°”ë¥¼ ë” ëª…í™•í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤. ì´í•´í•˜ê³  ì†Œí†µí•˜ê¸°ì— í¸ë¦¬í•˜ë‹¤. RDBMS ë°ì´í„° ì„¤ê³„ê°€ ì‰¬ì›Œì§„ë‹¤. ë°ì´í„°ë² ì´ìŠ¤ëª¨ë¸ë§ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë¥¼ Schemaë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì‹¤ìŠµì„ í•´ë³´ê¸° ì „ì— ERDì— ëŒ€í•´ ì†Œê°œë¥¼ í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹¤ìŠµì€ ì›Œí¬ë²¤ì¹˜ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ . MySQL workbenchëŠ” ì•½ 10ë…„ ì •ë„ ìƒì—…ìš©ìœ¼ë¡œ ê°œë°œë˜ì–´ íŒë§¤ë˜ë‹¤ê°€, MySQLì—ì„œ workbenchë¥¼ ì¸ìˆ˜í•˜ì—¬ ì˜¤í”ˆì†ŒìŠ¤ë¡œ í’€ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. ì‹¤ìŠµê³¼ì • FILE &gt; New Model í´ë¦­ Modelì˜ ì´ë¦„ ì„¤ì •(ì•ˆí•´ë„ ë¨) í…Œì´ë¸” ìƒì„± &gt; í° ë°”íƒ•ì— í´ë¦­ &gt; ì»¬ëŸ¼ ìƒì„± PKì™€ FK ì„¤ì • (ì„¤ì • ì‹œ place a relationship using existing columns ì„ íƒ &gt; ìì‹í…Œì´ë¸”ì˜ í•´ë‹¹ ì»¬ëŸ¼ ì—´ì„ ë¨¼ì € í´ë¦­ í›„ ë¶€ëª¨í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì—´ í´ë¦­) ê´€ê³„ ìƒì„± ì €ì¥ &gt; databaseì˜ forward engineer ì„ íƒ &gt; continue ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ë§ì´ ë˜ì–´ ìˆëŠ” dbì™€ table ìƒì„±(refresh all)","link":"/2020/07/23/MySQL-workbench%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-ERD%E1%84%90%E1%85%AE%E1%86%AF-%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5-Database-Modeling/"},{"title":"[PYTHON]ì¹¼ëŸ¼ ë‚´ íŠ¹ì • ê°’ì„ ê°€ì§„ rowë§Œ ê°€ì ¸ì˜¤ê¸°","text":"1# ë°ì´í„° í”„ë ˆì„ì—ì„œ íŠ¹ì • ì¹¼ëŸ¼ ë‚´ì—ì„œ íŠ¹ì • ê°’ì´ í¬í•¨ëœ rowë§Œ ê°€ì ¸ì˜¤ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°1234# seaborn ë°ì´í„° ê°€ì ¸ì˜¤ê¸°iris_df = sns.load_dataset('iris')iris_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 150 rows Ã— 5 columns ì—¬ëŸ¬ê°œë¥¼ í•  ê²½ìš° | ë¡œ êµ¬ë¶„í•˜ë©´ ë©ë‹ˆë‹¤ (ex: setosa | virginica) 12iris_filtered = iris_df[iris_df['species'].str.contains('setosa|virginica')]iris_filtered .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 100 rows Ã— 5 columns 12","link":"/2020/06/29/PYTHON-%EC%B9%BC%EB%9F%BC-%EB%82%B4-%ED%8A%B9%EC%A0%95-%EA%B0%92%EC%9D%84-%EA%B0%80%EC%A7%84-row%EB%A7%8C-%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0/"},{"title":"Python Pandas ì—ì„œ íŠ¹ì • ì»¬ëŸ¼ê°’ì˜ rowë¥¼ ì œê±°í•˜ê¸°","text":"íŒŒì´ì¬ì˜ Pandasë¥¼ ì‚¬ìš©í•˜ë©´ì„œ íŠ¹ì •ê°’ì˜ row ê°€ ì¡´ì¬í•  ë•Œ, ì´ rowë¥¼ ì œê±°í•˜ë ¤ë©´ ê·¸ ê°’ì´ ë“¤ì–´ê°€ëŠ” rowë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ê°’ë“¤ì„ ë‹¤ì‹œ dataframeìœ¼ë¡œ loadí•˜ë©´ ì†ì‰½ê²Œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì€ ëœë¤ìœ¼ë¡œ ìƒì„±í•œ ë°ì´í„°ì—ì„œ ëª‡ëª‡ ë¶€ë¶„ì˜ ë°ì´í„°ê°€ 0ì´ ì¡´ì¬í•˜ëŠ” ê·¸ë¦¼ì´ë‹¤. ì—¬ê¸°ì—ì„œ 0ê°’ì„ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ ì œê±°í•˜ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ê·¸ ê²°ê³¼ê°’ì´ë‹¤. nullê°’ì´ ì•„ë‹Œ íŠ¹ì •ê°’ì„ ì œê±°í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•  ë–„ ë§¤ìš° ìœ ìš©í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.","link":"/2020/05/07/Python-Pandas-%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-%E1%84%90%E1%85%B3%E1%86%A8%E1%84%8C%E1%85%A5%E1%86%BC-%E1%84%8F%E1%85%A5%E1%86%AF%E1%84%85%E1%85%A5%E1%86%B7%E1%84%80%E1%85%A1%E1%86%B9%E1%84%8B%E1%85%B4-row%E1%84%85%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%A6%E1%84%80%E1%85%A5%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"[Mysql]Database ìš©ëŸ‰ í™•ì¸","text":"[ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ìš©ëŸ‰ í™•ì¸]select table_schema â€˜Linear_Regressionâ€™, sum(data_length + index_length) / 1024 / 1024 â€˜size(MB)â€™ from information_schema.tables group by table_schema [íŠ¹ì • DBëª… status í™•ì¸]show table status like â€˜DBëª…â€™ [íŠ¹ì • DB ìš©ëŸ‰ ëŠ˜ë¦¬ê¸°]alter table â€˜DBëª…â€™ max_rows = 400000000 avg_row_length=1500","link":"/2020/06/09/Mysql-Database-%EC%9A%A9%EB%9F%89-%ED%99%95%EC%9D%B8/"},{"title":"Pandas: í•œ ì…€ì˜ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ í–‰ìœ¼ë¡œ ë‚˜ëˆ„ê¸°","text":"12df = pd.DataFrame({'alphabet': ['hello,world,in,python', 'python,is,great', 'data,science']})df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet 0 hello,world,in,python 1 python,is,great 2 data,science ìœ„ì™€ ê°™ì´ í•œ ì…€ì— ë“¤ì–´ìˆëŠ” ë¬¸ìì—´ì„ ì»´ë§ˆë¡œ êµ¬ë¶„í•´ì„œ í•œ ê¸€ìì”© ì—¬ëŸ¬ í–‰ìœ¼ë¡œ ë‚˜ëˆ„ê³  ì‹¶ë‹¤. í•´ê²°ì±…: ë¬¸ìì—´ì„ split í•´ ê° í–‰ì„ ì—¬ëŸ¬ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆˆ í›„ ë³‘í•©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ë¨¼ì €, ê° alphabet ì»¬ëŸ¼ì˜ ë¬¸ìì—´ì„ ë°°ì—´ë¡œ ë‚˜ëˆˆë‹¤.12result = df['alphabet'].str.split(',')result 0 [hello, world, in, python] 1 [python, is, great] 2 [data, science] Name: alphabet, dtype: object ë°°ì—´ì´ Seriesë¥¼ ë¦¬í„´í•˜ê²Œ applyë¥¼ ì ìš©í•˜ë©´ Series -&gt; DataFrameìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.12result = result.apply(lambda x: pd.Series(x))result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 hello world in python 1 python is great NaN 2 data science NaN NaN stack()ì„ í™œìš©í•˜ì—¬ ì»¬ëŸ¼ì„ í–‰ìœ¼ë¡œ ë³€í™˜í•œë‹¤.1result.stack() 0 0 hello 1 world 2 in 3 python 1 0 python 1 is 2 great 2 0 data 1 science dtype: object stack()ì„ ì‹¤í–‰í•˜ë©´, ìœ„ì™€ ê°™ì´ ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ Seriesê°€ ëœë‹¤. ì•ŒíŒŒë²³ ë‚±ìë§Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³ , ê¸°ì¤€ì´ ëœ ì¸ë±ìŠ¤ë„ ì œê±°í•´ë³´ì.1result.stack().reset_index(level=1, drop=True) 0 hello 0 world 0 in 0 python 1 python 1 is 1 great 2 data 2 science dtype: object ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì12result = result.stack().reset_index(level=1, drop=True).to_frame('alphabet_single')result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet_single 0 hello 0 world 0 in 0 python 1 python 1 is 1 great 2 data 2 science ì›ë³¸ í”„ë ˆì„ê³¼ ìœ„ì˜ í”„ë ˆì„ì„ merge í•´ë³´ì12result = df.merge(result, left_index=True, right_index=True, how='left')result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alphabet alphabet_single 0 hello,world,in,python hello 0 hello,world,in,python world 0 hello,world,in,python in 0 hello,world,in,python python 1 python,is,great python 1 python,is,great is 1 python,is,great great 2 data,science data 2 data,science science","link":"/2020/07/13/Pandas-%ED%95%9C-%EC%85%80%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%97%AC%EB%9F%AC-%ED%96%89%EC%9C%BC%EB%A1%9C-%EB%82%98%EB%88%84%EA%B8%B0/"},{"title":"SQLite db view tool.md","text":"DB Browser for SQLite http://sqlitebrowser.org/ ì‚¬ì´íŠ¸ ë“¤ì–´ê°€ì„œ, í•´ë‹¹ OSì—ì„œ ë§ëŠ” íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ì. ì„¤ì¹˜ ì™„ë£Œ í›„, í”„ë¡œê·¸ë¨ì„ ê°€ë™í•˜ì—¬, ì—´ê¸°ë¥¼ í†µí•´ SQLite ì˜ íŒŒì¼ì„ ì—´ì–´ì„œ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ë³¼ ìˆ˜ ìˆë‹¤. ì‚¬ì´íŠ¸ì—ì„œ ì ‘ì†í•˜ì—¬ ìœ„ì— ì¹´í…Œê³ ë¦¬ ë¶€ë¶„ì— downloadë¥¼ í´ë¦­ í›„ í•´ë‹¹ OSì— ë§ëŠ” ê²ƒì„ ì„¤ì¹˜í•´ì£¼ë©´ ë. ì‰½ì§€?","link":"/2020/06/02/SQLite-db-view-tool-md/"},{"title":"[NodeJS]_n_á„‹á…³á†¯_á„á…©á†¼á„’á…¡á„‹á…§_NodeJS_á„‡á…¥á„Œá…¥á†«_á„‡á…§á†«á„€á…§á†¼á„’á…¡á„€á…µ","text":"NodeJSì˜ ê²½ìš° ë²„ì „ ë³€ê²½ì´ êµ‰ì¥íˆ ì¦ê³  ë²„ì „ë§ˆë‹¤ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ê°€ ë§¤ìš° í¬ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” NodeJS ë²„ì „ì„ ê°„ë‹¨íˆ ë³€ê²½í•˜ëŠ” n ì„ ì†Œê°œí•˜ë„ë¡ í• ê²Œ1. npm ì„ í†µí•˜ì—¬ n ì„¤ì¹˜í•˜ê¸°ìš°ì„  í˜„ì¬ nodejs ì˜ ë²„ì „ì„ í™•ì¸í•´ ë´…ë‹ˆë‹¤. 1!node -v v12.17.0 ê·¸ë¦¬ê³  npm ì„ í†µí•˜ì—¬ n ì„ global ë¡œ ì„¤ì¹˜í•˜ì! (nodeJSë¥¼ ì»¨íŠ¸ë¡¤ í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¼ ì•„ì£¼ ì¤‘ìš”í•´!)1!sudo npm install -g n Password: ê·¸ë¦¬ê³  nì„ ì¬ëŒ€ë¡œ ì„¤ì¹˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸ì„ ìœ„í•˜ì—¬ ë²„ì „ì„ í™•ì¸í•˜ë„ë¡ í•˜ì12&lt;img width=\"483\" alt=\"á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-05-31 á„‹á…©á„Œá…¥á†« 10 43 54\" src=\"https://user-images.githubusercontent.com/59719711/83342570-0a01cc80-a32c-11ea-83d3-a67d05de3533.png\"&gt; 2. n ì„ ì´ìš©í•˜ì—¬ ë²„ì „ ë³€ê²½í•˜ê¸°ë²„ì „ ë³€ê²½ë°©ë²•ì€ ë§¤ìš° ê°„ë‹¨í•´. n ë’¤ì— lts, latest í˜¹ì€ ë²„ì „ì„ ì ìœ¼ë©´ ë!12345678# lts ë²„ì „ ì„¤ì¹˜n lts # ìµœì‹  ë²„ì „ ì„¤ì¹˜n latest # íŠ¹ì • ë²„ì „ ì„¤ì¹˜n 11 ë§ˆì§€ë§‰ìœ¼ë¡œ NodeJSì˜ ë²„ì „ì„ ë³€ê²½ í›„ í”„ë¡œì íŠ¸ì˜ node_modulesë¥¼ ì‚­ì œí•˜ê³  yarn í˜¹ì€ npmìœ¼ë¡œ íŒ¨í‚¤ì§€ë¥¼ ì¬ì„¤ì¹˜ í•˜ê±°ë‚˜ ì—…ê·¸ë ˆì´ë“œ í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•´. ì•ˆê·¸ëŸ¬ë©´ ì¢…ì¢… ì˜¤ë¥˜ê°€ ë°œìƒí•´","link":"/2020/05/31/NodeJS-n-%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%84%8B%E1%85%A7-NodeJS-%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB-%E1%84%87%E1%85%A7%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"Tensorflowë¥¼ í™œìš©í•œ íšŒê·€ ëª¨ë¸ë§","text":"ìë™ì°¨ ì—°ë¹„ ì˜ˆì¸¡í•˜ê¸°12345678import pathlibimport matplotlib.pyplot as pltimport seaborn as snsimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersprint(tf.__version__) 2.4.0-dev20200724Auto MPG ë°ì´í„°ì…‹UCI ë¨¸ì‹ ëŸ¬ë‹ ì €ì¥ì†Œì—ì„œ ë‹¤ìš´ë¡œë“œë¥¼ ë°›ì!123dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/\\ machine-learning-databases/auto-mpg/auto-mpg.data\")dataset_path &apos;/Users/wglee/.keras/datasets/auto-mpg.data&apos;1234567# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepwer', 'Weight', 'Acceleration',\\ 'Model_year', 'Origin']dataset = pd.read_csv(dataset_path, names=column_names, na_values='?', comment='\\t', sep=' ',\\ skipinitialspace=True)df = dataset.copy() 1df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPG Cylinders Displacement Horsepwer Weight Acceleration Model_year Origin 396 28.0 4 120.0 79.0 2625.0 18.6 82 1 397 31.0 4 119.0 82.0 2720.0 19.4 82 1 1df['Origin'].unique() array([1, 3, 2]) nullê°’ í™•ì¸ ê²°ê³¼ 6ê°œì˜ ë°ì´í„°ê°€ ëˆ„ë½ëœ ê²ƒì„ í™•ì¸í•˜ì˜€ê³  ì œê±° ì •ì œ ì‘ì—…ì„ í•˜ì˜€ìŠµë‹ˆë‹¤.1df.isnull().sum() MPG 0 Cylinders 0 Displacement 0 Horsepwer 6 Weight 0 Acceleration 0 Model_year 0 Origin 0 dtype: int641df.dropna(inplace=True) 12import missingno as msnomsno.matrix(df, figsize=(8, 2)) &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7faaa5864f10&gt; &quot;Origin&quot; ì—´ì€ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹ˆê³  ë²”ì£¼í˜•ì´ë¯€ë¡œ ì›-í•« ì¸ì½”ë”©(one-hot encoding)ìœ¼ë¡œ ë³€í™˜1origin = df.pop('Origin') 123df['USA'] = (origin == 1) * 1.0df['Europe'] = (origin == 2) * 2.0df['Japan'] = (origin == 3) * 3.0 1df.tail(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MPG Cylinders Displacement Horsepwer Weight Acceleration Model_year USA Europe Japan 396 28.0 4 120.0 79.0 2625.0 18.6 82 1.0 0.0 0.0 397 31.0 4 119.0 82.0 2720.0 19.4 82 1.0 0.0 0.0 ë°ì´í„°ì…‹ ë¶„ë¦¬ (train, test)12train_df = df.sample(frac=0.7, random_state=0)test_df = df.drop(train_df.index) 1len(train_df) 274 ë°ì´í„° EDAë¥¼ í†µí•´ ë°ì´í„°ì˜ ë¶„í¬ ë° í†µê³„ì¹˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤12sns.pairplot(train_df[['MPG','Cylinders','Displacement','Weight']], diag_kind='kde')plt.show() 1234train_stats = train_df.describe()# train_stats.pop(\"MPG\")train_stats = train_stats.T #transposetrain_stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max MPG 274.0 23.323358 7.643458 10.0 17.0 22.0 29.000 46.6 Cylinders 274.0 5.467153 1.690530 3.0 4.0 4.0 8.000 8.0 Displacement 274.0 193.846715 102.402201 68.0 105.0 151.0 260.000 455.0 Horsepwer 274.0 104.135036 37.281034 46.0 76.0 93.0 128.000 225.0 Weight 274.0 2976.879562 829.860536 1649.0 2250.5 2822.5 3573.000 4997.0 Acceleration 274.0 15.590876 2.714719 8.0 14.0 15.5 17.275 24.8 Model_year 274.0 75.934307 3.685839 70.0 73.0 76.0 79.000 82.0 USA 274.0 0.635036 0.482301 0.0 0.0 1.0 1.000 1.0 Europe 274.0 0.335766 0.748893 0.0 0.0 0.0 0.000 2.0 Japan 274.0 0.591241 1.195564 0.0 0.0 0.0 0.000 3.0 ì´ë²ˆì—ëŠ” train, test ë¶„ë¦¬ê°€ ì•„ë‹ˆë¼ featureì™€ labelë¥¼ ë¶„ë¦¬ì‹œì¼œ ì¤ë‹ˆë‹¤.12train_labels = train_df['MPG']test_labels = test_df['MPG'] ë°ì´í„° ì •ê·œí™”featureì˜ í¬ê¸°ì™€ ë²”ìœ„ê°€ ë‹¤ë¥´ë©´ ì •ê·œí™”(normalization)ë¥¼ í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤. ì •ê·œí™”ë¥¼ í•˜ì§€ ì•Šì•„ë„ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ì§€ë§Œ í›ˆë ¨ì‹œí‚¤ê¸° ì–´ë µê³  ì…ë ¥ ë‹¨ìœ„ì— ì˜ì¡´ì ì¸ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§€ê²Œ ë©ë‹ˆë‹¤.12345# # ë°ì´í„° ì •ê·œí™”# from sklearn.preprocessing import StandardScaler# scaler = StandardScaler()# train_df = scaler.fit_transform(train_df)# test_df = scaler.fit_transform(test_df) 1234def norm(x): return (x - train_stats['mean']) / train_stats['std']normed_train_data = norm(train_df)normed_test_data = norm(test_df) ëª¨ë¸ë§ëª¨ë¸ì„ êµ¬ì„±í•´ ë³´ì£ . ì—¬ê¸°ì—ì„œëŠ” ë‘ ê°œì˜ ì™„ì „ ì—°ê²°(densely connected) ì€ë‹‰ì¸µìœ¼ë¡œ Sequential ëª¨ë¸ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ì¶œë ¥ ì¸µì€ í•˜ë‚˜ì˜ ì—°ì†ì ì¸ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‘ ë²ˆì§¸ ëª¨ë¸ì„ ë§Œë“¤ê¸° ì‰½ë„ë¡ build_model í•¨ìˆ˜ë¡œ ëª¨ë¸ êµ¬ì„± ë‹¨ê³„ë¥¼ ê°ì‹¸ê² ìŠµë‹ˆë‹¤.1234567891011121314# ëª¨ë¸ë§def build_model(): model = keras.Sequential([ layers.Dense(128, activation='relu', input_shape=[len(train_df.keys())]), layers.Dense(64, activation='relu'), layers.Dense(1) ]) optimizer = tf.keras.optimizers.RMSprop(0.001) model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse']) return model 1model = build_model() ëª¨ë¸ í™•ì¸.summary() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê°„ë‹¨í•œ ì •ë³´ë¥¼ ì¶œë ¥í•´ì¤ë‹ˆë‹¤.1print(model.summary()) Model: &quot;sequential_15&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_45 (Dense) (None, 128) 1408 _________________________________________________________________ dense_46 (Dense) (None, 64) 8256 _________________________________________________________________ dense_47 (Dense) (None, 1) 65 ================================================================= Total params: 9,729 Trainable params: 9,729 Non-trainable params: 0 _________________________________________________________________ None ëª¨ë¸ì„ í•œë²ˆ ì‹¤í–‰í•´ ë³´ì£ . training ì„¸íŠ¸ì—ì„œ 10ê°œì˜ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë§Œë“¤ì–´ model_predict ë©”ì„œë“œë¥¼ í˜¸ì¶œí•´ ë³´ê² ìŠµë‹ˆë‹¤.123example_batch = normed_train_data[:10]example_result = model.predict(example_batch)example_result WARNING:tensorflow:5 out of the last 15 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7faa9a8f0200&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. array([[-0.03285253], [-0.01362434], [-0.48285854], [ 0.01581845], [ 0.08219826], [ 0.08362657], [ 0.15519306], [ 0.28581452], [ 0.07680693], [ 0.01200353]], dtype=float32)ëª¨ë¸ í›ˆë ¨ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ì (.)ì„ ì¶œë ¥í•´ í›ˆë ¨ ì§„í–‰ ê³¼ì •ì„ í‘œì‹œí•©ë‹ˆë‹¤.1234567891011class PrintDot(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): if epoch % 100 == 0: print('') print('.', end='')EPOCHS = 1000history = model.fit( normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()]) .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... .................................................................................................... acc : í›ˆë ¨ ì •í™•ë„ loss : í›ˆë ¨ ì†ì‹¤ê°’ val_acc : ê²€ì¦ ì •í™•ë„ val_loss : ê²€ì¦ ì†ì‹¤ê°’123hist = pd.DataFrame(history.history)hist['epoch'] = history.epochhist.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } loss mae mse accuracy val_loss val_mae val_mse val_accuracy epoch 995 0.102436 0.234032 0.102436 0.0 0.165683 0.324213 0.165683 0.0 995 996 0.124358 0.292103 0.124358 0.0 0.263786 0.404004 0.263786 0.0 996 997 0.130789 0.295300 0.130789 0.0 0.212862 0.362374 0.212862 0.0 997 998 0.116644 0.275093 0.116644 0.0 0.054454 0.196261 0.054454 0.0 998 999 0.106241 0.280440 0.106241 0.0 0.121306 0.281089 0.121306 0.0 999 12345678910111213141516171819202122232425262728def plot_history(history): hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch plt.figure(figsize=(8,8)) plt.subplot(2,1,1) plt.plot(hist['epoch'], hist['mae'], label='Train Error') plt.xlabel('Epoch') plt.ylabel('Mean Abs Error [MPG]') plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error') plt.ylim([0,5]) plt.legend() plt.subplot(2,1,2) plt.xlabel('Epoch') plt.ylabel('Mean Square Error [$MPG^2$]') plt.plot(hist['epoch'], hist['mse'], label='Train Error') plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error') plt.ylim([0,20]) plt.legend() plt.show()plot_history(history) 123456789model = build_model()# patience ë§¤ê°œë³€ìˆ˜ëŠ” ì„±ëŠ¥ í–¥ìƒì„ ì²´í¬í•  ì—í¬í¬ íšŸìˆ˜ì…ë‹ˆë‹¤early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])plot_history(history) ...................................................................................... ëª¨ë¸ ê²€ì¦í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ í™•ì¸123loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)print(\"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: {:5.2f} MPG\".format(mae)) 4/4 - 0s - loss: 0.4875 - mae: 0.5579 - mse: 0.4875 í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: 0.56 MPGì˜ˆì¸¡í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ìˆëŠ” ìƒ˜í”Œì„ ì´ìš©í•´ MPG ê°’ ì˜ˆì¸¡12345678910test_predictions = model.predict(normed_test_data).flatten()plt.scatter(test_labels, test_predictions)plt.xlabel('True Values [MPG]')plt.ylabel('Predictions [MPG]')plt.axis('equal')plt.axis('square')plt.xlim([0,plt.xlim()[1]])plt.ylim([0,plt.ylim()[1]])_ = plt.plot([-100, 100], [-100, 100]) 1234error = test_predictions - test_labelsplt.hist(error, bins = 25)plt.xlabel(\"Prediction Error [MPG]\")_ = plt.ylabel(\"Count\") 12","link":"/2020/08/21/Tensorflow%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%9A%8C%EA%B7%80-%EB%AA%A8%EB%8D%B8%EB%A7%81/"},{"title":"[Python] SQLAlchemy ì‚¬ìš©í•˜ê¸°","text":"DataFrameì„ MySQLì— ì €ì¥í•˜ê¸° ìœ„í•´ ë¨¼ì € ì—”ì§„ ì»¤ë„¥í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì´ì¬3ì—ì„œëŠ” MySQLdbë¥¼ ì§€ì›í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, pymysqlë¡œ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤. ê¼­ pymysqlì´ ì•„ë‹ˆì–´ë„ ìƒê´€ì—†ì§€ë§Œ, ì‚¬ìš©í•´ë³´ë©´ mysql-connector ë³´ë‹¤ ë¹ ë¥´ë‹¤ëŠ”ê±¸ ì²´ê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì¤ë‹ˆë‹¤.123# python3pip install pymysqlpip install sqlalchemy SQLAlchemy, Pymysql, MySQLdbinstall_as_MySQLdb() í•¨ìˆ˜ë¥¼ í†µí•´ MySQLdbì™€ í˜¸í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ì œ sqlalchemyë¥¼ í†µí•´ DBì— ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ì†Œì—ì„œ root, passwordëŠ” DBì— ë§ê²Œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.123456789import pandas as pdfrom sqlalchemy import create_engine# MySQL Connector using pymysqlpymysql.install_as_MySQLdb()import MySQLdbengine = create_engine(\"mysql://root:\"+\"password\"+\"@public IP/db_name\", encoding='utf-8')conn = engine.connect() MySQLì— ì €ì¥í•˜ê¸°ì´ì œ DataFrameì„ MySQLì— í…Œì´ë¸” í˜•íƒœë¡œ ì €ì¥í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ pandasì˜ to_sql() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.1df.to_sql(name=table_name, con=engine, if_exists='append') ìì£¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‹ˆ í•¨ìˆ˜ë¡œ ë”°ë¡œ ì„¤ì •í•´ì£¼ë©´ ì›í•  ë•Œë§ˆë‹¤ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê² ì£ ? ê·¸ë¦¬ê³  if_existsì˜ ê²½ìš° ë§Œì•½ ë™ì¼ í…Œì´ë¸”ì˜ ì´ë¦„ì´ ì¡´ì¬í•œë‹¤ë©´ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê² ëŠëƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì£¼ëŠ” ê²ƒì¸ë° append ì™¸ì—ë„ replace, delete ë“± ë‹¤ì–‘í•œ ê²ƒì´ ìˆìŠµë‹ˆë‹¤.","link":"/2020/07/19/Python-SQLAlchemy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/"},{"title":"git is awesome","text":"What is git? Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Why to use git?How to use git?","link":"/2020/04/17/git-is-awesome/"},{"title":"mglearnì— ëŒ€í•´","text":"ê°„ë‹¨í•˜ê²Œ ê·¸ë¦¼ì„ ê·¸ë¦¬ê±°ë‚˜ í•„ìš”í•œ ë°ì´í„°ë¥¼ ë°”ë¡œ ë¶ˆëŸ¬ë“¤ì´ê¸° ìœ„í•´ mglearnì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¹ƒí—ˆë¸Œì— ìˆëŠ” ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•  ë•ŒëŠ” ì´ ëª¨ë“ˆì— ê´€í•´ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ë§Œì•½ ë‹¤ë¥¸ ê³³ì—ì„œ mglearn í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë ¤ë©´, pip install mglearn ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì…ë‹ˆë‹¤.7 ë…¸íŠ¸_ ì´ ì±…ì€ NumPy, matplotlib, pandasë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ì½”ë“œëŠ” ë‹¤ìŒì˜ ë„¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. from IPython.display import displayimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport mglearn","link":"/2020/06/17/mglearn%EC%97%90-%EB%8C%80%ED%95%B4/"},{"title":"Ubuntu UTF-8 ì„¤ì •í•˜ê¸°","text":"ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ í˜„ì¬ ì„¤ì •ëœ ì–¸ì–´ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. locale í•œê¸€íŒ©ì„ ì„¤ì¹˜í•´ì¤ë‹ˆë‹¤. apt-get -y install language-pack-ko í•œê¸€ utf8 ì–¸ì–´íŒ©ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤. locale-gen ko_KR.UTF-8 ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì–¸ì–´íŒ©ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. dpkg-reconfigure locales â€œko_KR.UTF-8 UTF-8â€ ì— í•´ë‹¹í•˜ëŠ” ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ê³  ì—”í„°! (ì €ëŠ” 290ë²ˆì´ì˜€ë„¤ìš”. í˜¹ì‹œ ë‹¤ë¥¼ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸í•˜ì„¸ìš”)â€œko_KR.UTF-8â€ ì„ ì„ íƒí•©ë‹ˆë‹¤. (ì €ëŠ” 3ë²ˆê³¼ 4ë²ˆì— ë˜‘ê°™ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê·¸ëƒ¥ 3ë²ˆ ëˆŒë €ìŠµë‹ˆë‹¤.) ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ ì ìš©ë©ë‹ˆë‹¤.update-locale LANG=ko_KR.UTF-8 LC_MESSAGES=POSIX ë¡œê·¸ì•„ì›ƒ í›„ ë‹¤ì‹œ ë¡œê·¸ì¸í•œ í›„(SSH ì¬ì ‘ì†) ë‹¤ì‹œ locale ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ë©´ ì ìš©ë©ë‹ˆë‹¤!ì €ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. LANG=ko_KR.UTF-8LANGUAGE=LC_CTYPE=â€ko_KR.UTF-8â€LC_NUMERIC=â€ko_KR.UTF-8â€LC_TIME=â€ko_KR.UTF-8â€LC_COLLATE=â€ko_KR.UTF-8â€LC_MONETARY=â€ko_KR.UTF-8â€LC_MESSAGES=POSIXLC_PAPER=â€ko_KR.UTF-8â€LC_NAME=â€ko_KR.UTF-8â€LC_ADDRESS=â€ko_KR.UTF-8â€LC_TELEPHONE=â€ko_KR.UTF-8â€LC_MEASUREMENT=â€ko_KR.UTF-8â€LC_IDENTIFICATION=â€ko_KR.UTF-8â€LC_ALL=","link":"/2020/07/10/Ubuntu-UTF-8-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/"},{"title":"wglee87DB-ì—‘ì…€íŒŒì¼-insertí•˜ê¸°","text":"ì—‘ì…€ë¡œ ì •ë¦¬ëœ íŒŒì¼ë“¤ì„ mysql ë°ì´í„°ë² ì´ìŠ¤ë¡œ í•œë²ˆì— ê°€ì ¸ì˜¤ëŠ” ë°©ë²•(ê¸°ë¡ìš©) ì—‘ì…€íŒŒì¼ì˜ ì „ì²˜ë¦¬ ì‘ì—… ê°€ì ¸ì˜¬ ë°ì´í„°ë§Œ ë‚¨ê¸°ê³  ì—´ ì´ë¦„ì€ ì‚­ì œ ì—‘ì…€íŒŒì¼ì„ csvíŒŒì¼ë¡œ ì €ì¥ ë‹¤ë¥¸ì´ë¦„ìœ¼ë¡œ ì €ì¥í•˜ê¸° &gt; encoding=utf8 (utf8 ë¯¸ì„¤ì • ì‹œ í•œê¸€í°íŠ¸ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ) &gt; í™•ì¥ì csvë¡œì €ì¥í•˜ê¸° mysqlë¡œ ì ‘ì†í•˜ê¸° mysqlì— ì ‘ì† (í•„ìëŠ” sequel proë¥¼ í™œìš©) ë°ì´í„°ë² ì´ìŠ¤, í…Œì´ë¸” ìƒì„± ë° encoding ë³€í™˜ ë‹¨, í…Œì´ë¸” êµ¬ì¡°ì™€ csvíŒŒì¼ êµ¬ì¡°ê°€ ê°™ì•„ì•¼í•¨ CREATE TABLE [í…Œì´ë¸”ëª…](columnì´ë¦„ í…ìŠ¤íŠ¸í˜•ì‹) encoding=utf8ë¡œ ë³€í™˜ ALTER DATABASE [DBëª…] CHARACTER SET = utf8 ALTER TABLE [í…Œì´ë¸”ëª…] CHARACTER SET = utf8 ë°ì´í„° insert LOAD DATA LOCAL INFILE [FILE_PATH] INTO TABLE DBëª….TABLEëª… FIELDS TERMINATED BY â€˜,â€™; title: â€˜[wglee87DB] ì—‘ì…€íŒŒì¼ insertí•˜ê¸°â€™date: 2020-04-27 19:24:50tags:","link":"/2020/04/27/wglee87DB-%EC%97%91%EC%85%80%ED%8C%8C%EC%9D%BC-insert%ED%95%98%EA%B8%B0/"},{"title":"leverage-outliar-cooks_distance_anova","text":"1import statsmodels.api as sm ë ˆë²„ë¦¬ì§€ (leverage)ë…ë¦½ë³€ìˆ˜ì˜ ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ê°œë³„ì ì¸ ë°ì´í„° í‘œë³¸ í•˜ë‚˜í•˜ë‚˜ê°€ íšŒê·€ë¶„ì„ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ì€ ë ˆë²„ë¦¬ì§€ ë¶„ì„ì´ë‚˜ ì•„ì›ƒë¼ì´ì–´ ë¶„ì„ì„ í†µí•´ ì•Œ ìˆ˜ ìˆë‹¤. ë ˆë²„ë¦¬ì§€(leverage)ëŠ” ì‹¤ì œ ì¢…ì†ë³€ìˆ˜ê°’ ğ‘¦ ê°€ ì˜ˆì¸¡ì¹˜(predicted target) ğ‘¦Ì‚ ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë‚˜íƒ€ë‚¸ ê°’ì´ë‹¤. self-influence, self-sensitivity ë¼ê³ ë„ í•œë‹¤.12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows Ã— 14 columns 123dfX = sm.add_constant(dfx)df0 = pd.concat([dfX, dfy],axis=1)df0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } const CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 1.0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 1.0 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 1.0 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 1.0 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 1.0 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 1.0 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 1.0 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 1.0 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 1.0 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 1.0 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows Ã— 15 columns statsmodelsë¥¼ ì´ìš©í•œ ë ˆë²„ë¦¬ì§€ ê³„ì‚°ë ˆë²„ë¦¬ì§€ ê°’ì€ RegressionResults í´ë˜ìŠ¤ì˜ get_influence ë©”ì„œë“œë¡œ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.12345678910111213141516171819from sklearn.datasets import make_regression# 100ê°œì˜ ë°ì´í„° ìƒì„±X0, y, coef = make_regression(n_samples=100, n_features=1, noise=20, coef=True, random_state=1)# ë ˆë²„ë¦¬ì§€ê°€ ë†’ì€ ê°€ìƒì˜ ë°ì´í„°ë¥¼ ì¶”ê°€data_100 = (4, 300)data_101 = (3, 150)X0 = np.vstack([X0, np.array([data_100[:1], data_101[:1]])])X = sm.add_constant(X0) #ìƒìˆ˜í•­ ì¶”ê°€y = np.hstack([y, [data_100[1], data_101[1]]])plt.figure(figsize=(14,6))plt.scatter(X0, y, s=30)plt.xlabel(\"x\")plt.ylabel(\"y\")plt.title(\"ê°€ìƒì˜ íšŒê·€ë¶„ì„ìš© ë°ì´í„°\")plt.show() 123model = sm.OLS(pd.DataFrame(y), pd.DataFrame(X))result = model.fit()print(result.summary()) OLS Regression Results ============================================================================== Dep. Variable: 0 R-squared: 0.936 Model: OLS Adj. R-squared: 0.935 Method: Least Squares F-statistic: 1464. Date: Thu, 14 May 2020 Prob (F-statistic): 1.61e-61 Time: 14:22:53 Log-Likelihood: -452.71 No. Observations: 102 AIC: 909.4 Df Residuals: 100 BIC: 914.7 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ 0 3.2565 2.065 1.577 0.118 -0.840 7.353 1 78.3379 2.048 38.260 0.000 74.276 82.400 ============================================================================== Omnibus: 16.191 Durbin-Watson: 1.885 Prob(Omnibus): 0.000 Jarque-Bera (JB): 36.807 Skew: -0.534 Prob(JB): 1.02e-08 Kurtosis: 5.742 Cond. No. 1.14 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ì„ í˜•íšŒê·€ ê²°ê³¼ì—ì„œ get_influence ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ì˜í–¥ë„ ì •ë³´ ê°ì²´ë¥¼ êµ¬í•  ìˆ˜ ìˆê³ , ì´ ê°ì²´ëŠ” hat_matrix_diag ì†ì„±ìœ¼ë¡œ ë ˆë²„ë¦¬ì§€ ë²¡í„°ì˜ ê°’ì„ ê°€ì§€ê³ ë„ ìˆì–´12345678influence = result.get_influence()hat = influence.hat_matrix_diagplt.figure(figsize=(14,6))plt. stem(hat)plt.axhline(0.02, c = 'g', ls = '--') # c = color , ls = linestyleplt.title('ê° ë°ì´í„°ì˜ ë ˆë²„ë¦¬ì§€ ê°’')plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. &quot;&quot;&quot; ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ì½”ë“œì—ì„œ 0.02ì˜ ê°’ì€ ë ˆë²„ë¦¬ì§€ í‰ê· ê°’ì„ êµ¬í•˜ëŠ” ê³µì‹ ë…ë¦½ë³€ìˆ˜ì˜ ê°¯ìˆ˜ / ë°ì´í„°ì˜ ê°¯ìˆ˜ ë¡œ êµ¬í•˜ë©´ ëœë‹¤1![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-05-14 á„‹á…©á„’á…® 2 31 46](https://user-images.githubusercontent.com/59719711/81926278-b2215100-961c-11ea-9613-c5ba582d5de0.png) 123456789plt.figure(figsize=(14,6))ax = plt.subplot()plt.scatter(X0, y,s=30)sm.graphics.abline_plot(model_results=result, ax=ax)idx = hat &gt; 0.05plt.scatter(X0[idx], y[idx], s=300, c=\"r\", alpha=0.5)plt.title(\"íšŒê·€ë¶„ì„ ê²°ê³¼ì™€ ë ˆë²„ë¦¬ì§€ í¬ì¸íŠ¸\")plt.show() ê·¸ë˜í”„ë¥¼ í† ëŒ€ë¡œ í•´ì„ì„ í•˜ìë©´, ë°ì´í„°ê°€ í˜¼ìë§Œ ë„ˆë¬´ ì‘ê±°ë‚˜ ë„ˆë¬´ í¬ê²Œ ë‹¨ë…ìœ¼ë¡œ ì¡´ì¬í• ìˆ˜ë¡ ë ˆë²„ë¦¬ì§€ê°€ ì»¤ì§ì„ ì•Œ ìˆ˜ ìˆì–´. ì´ ë§ì€ ì €ëŸ° ë°ì´í„°ì€ ì „ì²´ íšŒê·€ë¶„ì„ ê²°ê³¼ê°’ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ë§ì´ì•¼ì•„ì›ƒë¼ì´ì–´(outlier)ë°ì´í„°ì™€ ë™ë–¨ì–´ì§„ ê°’ì„ ê°€ì§€ëŠ” ë°ì´í„°, ì¦‰ ì”ì°¨ê°€ í° ë°ì´í„°ë¥¼ ì•„ì›ƒë¼ì´ì–´(outlier)ë¼ê³  í•˜ëŠ”ë°, ì”ì°¨ì˜ í¬ê¸°ëŠ” ë…ë¦½ ë³€ìˆ˜ì˜ ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ ì•„ì›ƒë¼ì´ì–´ë¥¼ ì°¾ìœ¼ë ¤ë©´ ì´ ì˜í–¥ì„ ì œê±°í•œ í‘œì¤€í™”ëœ ì”ì°¨ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤ê³  í•´. ë¬´ìŠ¨ë§ì¸ì§€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ê·¸ë˜ statsmodelsë¥¼ ì´ìš©í•œ í‘œì¤€í™” ì”ì°¨ ê³„ì‚° ì”ì°¨ëŠ” RegressionResult ê°ì²´ì˜ resid ì†ì„±ì— ìˆë‹¤.1234plt.figure(figsize=(14, 6))plt.stem(result.resid)plt.title(\"ê° ë°ì´í„°ì˜ ì”ì°¨\")plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. í‘œì¤€í™” ì”ì°¨ëŠ” resid_pearson ì†ì„±ì— ìˆê³ , ë³´í†µ í‘œì¤€í™” ì”ì°¨ê°€ 2~4ë³´ë‹¤ í¬ë©´ ì•„ì›ƒë¼ì´ì–´ë¡œ ë³´ëŠ”ê²Œ ì¼ë°˜ì ì´ì•¼123456plt.figure(figsize=(14,6))plt. stem(result.resid_pearson)plt.axhline(3, c='g', ls='--')plt.axhline(-3, c='g', ls='--')plt.title('ê° ë°ì´í„°ì˜ í‘œì¤€í™” ì”ì°¨')plt.show() /opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True. Cookâ€™s DistanceíšŒê·€ ë¶„ì„ì—ëŠ” ë ˆë²„ë¦¬ì§€ ë”°ë¡œ, ì”ì°¨ì˜ í¬ê¸°ê°€ í° ë°ì´í„°ê°€ ì•„ì›ƒë¼ì´ì–´ê°€ ë˜ê³  ê·¸ê²ƒì„ ë³´ëŠ” ë”°ë¡œë”°ë¡œì˜ ê¸°ëŠ¥ë„ ìˆì§€ë§Œ ì´ ë‘ê°œë¥¼ ë™ì‹œì— ë³´ëŠ” ë°©ë²•ì´ ë°”ë¡œ Cook&apos;s Distanceì•¼. ì•„ë§ˆë„ Cookì´ë¼ëŠ” ì‚¬ëŒì´ ë§Œë“¤ì—ˆì„ ê°€ëŠ¥ì„±ì´.. ë„˜ì–´ê°€ì ë™ì‹œì— ë³´ëŠ” ê¸°ì¤€ì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê³ , ë‘˜ì¤‘ í•˜ë‚˜ë§Œ ì»¤ì§€ë”ë¼ë„ ì´ Cook&apos;s distance ê°’ì€ ì»¤ì§€ê²Œ ë¼ ëª¨ë“  ë°ì´í„°ì˜ ë ˆë²„ë¦¬ì§€ì™€ ì”ì°¨ë¥¼ ë™ì‹œì— ë³´ë ¤ë©´ plot_leverage_resid2 ëª…ë ¹ì„ ì‚¬ìš©í•˜ëŠ”ë°, ì´ ëª…ë ¹ì€ xì¶•ìœ¼ë¡œ í‘œì¤€í™” ì”ì°¨ì˜ ì œê³±ì„ í‘œì‹œí•˜ê³  yì¶•ìœ¼ë¡œ ë ˆë²„ë¦¬ì§€ê°’ì„ í‘œì‹œí•œë‹¤. ê·¸ë¦¬ê³  ë°ì´í„° ì•„ì´ë””ê°€ í‘œì‹œëœ ë°ì´í„°ë“¤ì´ ë ˆë²„ë¦¬ì§€ê°€ í° ì•„ì›ƒë¼ì´ì–´ ì•¼123456plt.figure(figsize=(14,6))sm.graphics.plot_leverage_resid2(result)plt.show()sm.graphics.influence_plot(result)plt.show() &lt;Figure size 1008x432 with 0 Axes&gt; 1234567891011121314from statsmodels.graphics import utilscooks_d2, pvals = influence.cooks_distanceK = influence.k_varsfox_cr = 4 / (len(y) - K - 1)idx = np.where(cooks_d2 &gt; fox_cr)[0]ax = plt.subplot()plt.scatter(X0, y)plt.scatter(X0[idx], y[idx], s=300, c=\"r\", alpha=0.5)utils.annotate_axes(range(len(idx)), idx, list(zip(X0[idx], y[idx])), [(-20, 15)] * len(idx), size=\"small\", ax=ax)plt.title(\"Fox Recommendaionìœ¼ë¡œ ì„ íƒí•œ ì•„ì›ƒë¼ì´ì–´\")plt.show() ë³´ìŠ¤í„´ ì§‘ê°’ ì˜ˆì¸¡ ë¬¸ì œÂ¶ë³´ìŠ¤í„´ ì§‘ê°’ ë¬¸ì œì— ì•„ì›ƒë¼ì´ì–´ë¥¼ ì ìš©í•´ ë³´ì. MEDVê°€ 50ì¸ ë°ì´í„°ëŠ” ìƒì‹ì ìœ¼ë¡œ ìƒê°í•´ë„ ì´ìƒí•œ ë°ì´í„°ì´ë¯€ë¡œ ì•„ì›ƒë¼ì´ì–´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤. ë‚˜ë¨¸ì§€ ë°ì´í„° ì¤‘ì—ì„œ í­ìŠ¤ ì¶”ì²œê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ ì•„ì›ƒë¼ì´ì–´ë¥¼ ì œì™¸í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.12345678910boston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)dfdfX = sm.add_constant(dfx)df0 = pd.concat([dfX, dfy],axis=1)df0 123456789101112131415161718pred = result_boston.predict(dfX)influence_boston = result_boston.get_influence()cooks_d2, pvals = influence_boston.cooks_distanceK = influence.k_varsfox_cr = 4 / (len(y) - K - 1)idx = np.where(cooks_d2 &gt; fox_cr)[0]# MEDV = 50 ì œê±°idx = np.hstack([idx, np.where(boston.target == 50)[0]])ax = plt.subplot()plt.scatter(dfy, pred)plt.scatter(dfy.MEDV[idx], pred[idx], s=200, c=\"r\", alpha=0.5)utils.annotate_axes(range(len(idx)), idx, list(zip(dfy.MEDV[idx], pred[idx])), [(-20, 15)] * len(idx), size=\"small\", ax=ax)plt.title(\"ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„°ì—ì„œ ì•„ì›ƒë¼ì´ì–´\")plt.show() ë‹¤ìŒì€ ì´ë ‡ê²Œ ì•„ì›ƒë¼ì´ì–´ë¥¼ ì œì™¸í•œ í›„ì— ë‹¤ì‹œ íšŒê·€ë¶„ì„ì„ í•œ ê²°ê³¼ì´ë‹¤.123456idx2 = list(set(range(len(dfX))).difference(idx))dfX = dfX.iloc[idx2, :].reset_index(drop=True)dfy = dfy.iloc[idx2, :].reset_index(drop=True)model_boston2 = sm.OLS(dfy, dfX)result_boston2 = model_boston2.fit()print(result_boston2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.812 Model: OLS Adj. R-squared: 0.806 Method: Least Squares F-statistic: 156.1 Date: Thu, 14 May 2020 Prob (F-statistic): 2.41e-161 Time: 15:14:52 Log-Likelihood: -1285.2 No. Observations: 485 AIC: 2598. Df Residuals: 471 BIC: 2657. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 18.8999 4.107 4.602 0.000 10.830 26.969 CRIM -0.0973 0.024 -4.025 0.000 -0.145 -0.050 ZN 0.0278 0.010 2.651 0.008 0.007 0.048 INDUS -0.0274 0.046 -0.595 0.552 -0.118 0.063 CHAS 0.9228 0.697 1.324 0.186 -0.447 2.292 NOX -9.4922 2.856 -3.323 0.001 -15.105 -3.879 RM 5.0921 0.371 13.735 0.000 4.364 5.821 AGE -0.0305 0.010 -2.986 0.003 -0.051 -0.010 DIS -1.0562 0.150 -7.057 0.000 -1.350 -0.762 RAD 0.1990 0.049 4.022 0.000 0.102 0.296 TAX -0.0125 0.003 -4.511 0.000 -0.018 -0.007 PTRATIO -0.7777 0.098 -7.955 0.000 -0.970 -0.586 B 0.0107 0.002 5.348 0.000 0.007 0.015 LSTAT -0.2846 0.043 -6.639 0.000 -0.369 -0.200 ============================================================================== Omnibus: 45.944 Durbin-Watson: 1.184 Prob(Omnibus): 0.000 Jarque-Bera (JB): 65.791 Skew: 0.679 Prob(JB): 5.17e-15 Kurtosis: 4.188 Cond. No. 1.59e+04 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.59e+04. This might indicate that there are strong multicollinearity or other numerical problems. R-squared ì˜ ì„±ëŠ¥ì ìˆ˜ê°€ ì˜¬ë¼ê°„ ê²ƒì„ ë³¼ ìˆ˜ ìˆì–´. ì´ë ‡ê²Œ ì–´ë–¤ íŠ¹ì • ë°ì´í„°ë¥¼ ê°€ì§€ê³  íšŒê·€ë¶„ì„ ëª¨ë¸ë§ì„ í•  ë•Œì—ëŠ” í•˜ê¸° ì „ì— ë ˆë²„ë¦¬ì§€ê°€ í° ë°ì´í„°ì™€ ì•„ì›ƒë¼ì´ì–´ì˜ ê°’ì„ ì´ëŸ¬í•œ ì ˆì°¨ì— ì˜í•´ ë½‘ì•„ì„œ ì œê±°í•˜ê³  ëª¨ë¸ë§ì„ í•œë‹¤ë©´ ë”ìš± ì„±ëŠ¥ì´ ì¢‹ì€ íšŒê·€ë¶„ì„ ëª¨ë¸ë§ì„ í•  ìˆ˜ ìˆëŠ”ê±°ì•¼ë¶„ì‚° ë¶„ì„ì„ í˜•íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ëŠ” ë‹¨ìˆœíˆ ì”ì°¨ì œê³±í•©(RSS: Residula Sum of Square)ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ì—†ë‹¤. ë³€ìˆ˜ì˜ ë‹¨ìœ„ ì¦‰, ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ì§€ë©´ íšŒê·€ë¶„ì„ê³¼ ìƒê´€ì—†ì´ ì”ì°¨ì œê³±í•©ë„ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì´ì•¼ ( ex.1kmì™€ 1000m) ë¶„ì‚° ë¶„ì„(ANOVA: Analysis of Variance)ì€ ì¢…ì†ë³€ìˆ˜ì˜ ë¶„ì‚°ê³¼ ë…ë¦½ë³€ìˆ˜ì˜ ë¶„ì‚°ê°„ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ í˜•íšŒê·€ë¶„ì„ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ ì í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë¶„ì‚° ë¶„ì„ì€ ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°œì˜ ì„ í˜•íšŒê·€ë¶„ì„ì˜ ì„±ëŠ¥ ë¹„êµì— ì‘ìš©í•  ìˆ˜ ìˆìœ¼ë©° ë…ë¦½ë³€ìˆ˜ê°€ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ì¸ ê²½ìš° ê° ì¹´í…Œê³ ë¦¬ ê°’ì— ë”°ë¥¸ ì˜í–¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ”ë°ë„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë¼ ì—¬ëŸ¬ ìˆ˜ì‹ë“¤ì´ ì¡´ì¬í•˜ì§€ë§Œ ë‚´ê°€ ì´í•´ë¥¼ ëª»í•˜ê² ê³  ê²°ë¡ ì€ ë‹¤ìŒê³¼ ê°™ì•„. ëª¨í˜• ì˜ˆì¸¡ì¹˜ì˜ ì›€ì§ì„ì˜ í¬ê¸°(ë¶„ì‚°,ESS)ì€ ì¢…ì†ë³€ìˆ˜ì˜ ì›€ì§ì„ì˜ í¬ê¸°(ë¶„ì‚°,TSS)ë³´ë‹¤ í´ ìˆ˜ ì—†ì–´ ê·¸ë¦¬ê³  ëª¨í˜•ì˜ ì„±ëŠ¥ì´ ì¢‹ì„ìˆ˜ë¡ ëª¨í˜• ì˜ˆì¸¡ì¹˜ì˜ ì›€ì§ì„ì˜ í¬ê¸°ëŠ” ì¢…ì†ë³€ìˆ˜ì˜ ì›€ì§ì„ì˜ í¬ê¸°ì™€ ë¹„ìŠ·í•´ì§„ë‹¤ëŠ” ì ì´ì•¼F ê²€ì •ì„ ì‚¬ìš©í•œ ë³€ìˆ˜ ì¤‘ìš”ë„ ë¹„êµFê²€ì •ì€ ê° ë…ë¦½ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë°©ë²•ì€ ì „ì²´ ëª¨í˜•ê³¼ ê° ë³€ìˆ˜ í•˜ë‚˜ë§Œì„ ëº€ ëª¨í˜•ë“¤ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ê²ƒì¸ë°, ì´ëŠ” ê°„ì ‘ì ìœ¼ë¡œ ê° ë…ë¦½ ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ ì¸¡ì •í•˜ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„°ì—ì„œ CRIMì´ë€ ë³€ìˆ˜ë¥¼ ëº€ ëª¨ë¸ê³¼ ì „ì²´ ëª¨ë¸ì˜ ë¹„êµí•˜ëŠ” ê²€ì •ì„ í•˜ë©´ ì´ ê²€ì • ê²°ê³¼ëŠ” CRIMë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.123456model_full = sm.OLS.from_formula( \"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)model_reduced = sm.OLS.from_formula( \"MEDV ~ ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)sm.stats.anova_lm(model_reduced.fit(), model_full.fit()) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater return (a &lt; x) &amp; (x &lt; b) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less return (a &lt; x) &amp; (x &lt; b) /opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal cond2 = cond0 &amp; (x &lt;= _a) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 493.0 11322.004277 0.0 NaN NaN NaN 1 492.0 11078.784578 1.0 243.219699 10.801193 0.001087 anova_lm ëª…ë ¹ì—ì„œëŠ” typ ì¸ìˆ˜ë¥¼ 2ë¡œ ì§€ì •í•˜ë©´ í•˜ë‚˜ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¥¼ ëº€ ì¶•ì†Œ ëª¨í˜•ì—ì„œì˜ F ê²€ì •ê°’ì„ í•œêº¼ë²ˆì— ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.ì•„ë…¸ë°” ë¶„ì„ - Fê²€ì •1234model = sm.OLS.from_formula( \"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS\", data=df0)result = model.fit()sm.stats.anova_lm(result, typ=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sum_sq df F PR(&gt;F) CRIM 243.219699 1.0 10.801193 1.086810e-03 ZN 257.492979 1.0 11.435058 7.781097e-04 INDUS 2.516668 1.0 0.111763 7.382881e-01 NOX 487.155674 1.0 21.634196 4.245644e-06 RM 1871.324082 1.0 83.104012 1.979441e-18 AGE 0.061834 1.0 0.002746 9.582293e-01 DIS 1232.412493 1.0 54.730457 6.013491e-13 RAD 479.153926 1.0 21.278844 5.070529e-06 TAX 242.257440 1.0 10.758460 1.111637e-03 PTRATIO 1194.233533 1.0 53.034960 1.308835e-12 B 270.634230 1.0 12.018651 5.728592e-04 LSTAT 2410.838689 1.0 107.063426 7.776912e-23 CHAS 218.970357 1.0 9.724299 1.925030e-03 Residual 11078.784578 492.0 NaN NaN ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ì „ì²´ì™€ ë¹„êµí–ˆì„ ë•Œ ì–¼ë§ˆë§Œí¼ ì¤‘ìš”ë„ë¥¼ ê°€ì§€ëŠ”ë° ì •ëŸ‰ì ìœ¼ë¡œ ë‚˜ì˜¨ ê²°ê³¼ê°’ì´ì•¼. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼í•  ë¶€ë¶„ì€ PR&gt;(&gt;F)ë¶€ë¶„ìœ¼ë¡œ summaryì—ì„œë„ ë‚˜ì˜¤ëŠ” p-valueê°’ì„ ë””í…Œì¼í•˜ê²Œ í’€ì–´ë†“ì€ ê°’ì´ê³  ì˜ˆë¥¼ ë“¤ì–´ LSTAT, RMì˜ ê²½ìš° 10ì˜ -23ìŠ¹, 10ì˜ -18ìŠ¹ìœ¼ë¡œ ìˆ˜ì¹˜ê°€ ì œì¼ ë‚®ì€ê±¸ ì•Œ ìˆ˜ ìˆì–´. ê·¸ëŸ¬ë©´ ì´ 2ê°€ì§€ì˜ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ê³  í•´ì„í•˜ë©´ ë˜ëŠ”ê±°ì•¼ í‘œì˜ Fê°’ì„ ë³´ê³ ë„ ì•Œ ìˆ˜ ìˆì§€ë§Œ Fê°’ì€ í™•ë¥ ì˜ ì˜ë¯¸ëŠ” ì—†ê¸° ë•Œë¬¸ì— ë‹¨ìˆœ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ”ê±° ë¼ë©´ ê²°ì •í•  ìˆ˜ ìˆì§€ë§Œ ë§Œì•½ ê·€ë¬´ê°€ì„¤/ëŒ€ë¦½ê°€ì„¤ì„ accept í•˜ëƒ reject í•˜ëƒì˜ í™•ë¥ ì  ì˜ë¯¸ë¥¼ íŒë‹¨í•œë‹¤ë©´ Fê°’ë§Œìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥í•´12 ë²”ì£¼í˜•ì„ ì‚¬ìš©í•œ ë¹„ì„ í˜•ì„±ë…ë¦½ë³€ìˆ˜ì˜ ë¹„ì„ í˜•ì„±ì„ í¬ì°©í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” ê°•ì œë¡œ ë²”ì£¼í˜• ê°’ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ë²”ì£¼í˜• ê°’ì´ ë˜ë©´ì„œ ë…ë¦½ë³€ìˆ˜ì˜ ì˜¤ì°¨ê°€ ìƒê¸°ì§€ë§Œ ì´ë¡œ ì¸í•œ ì˜¤ì°¨ë³´ë‹¤ ë¹„ì„ í˜•ì„±ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ìµì´ í´ ìˆ˜ë„ ìˆë‹¤. ë³´ìŠ¤í„´ ì§‘ê°’ ë°ì´í„°ì—ì„œ ì¢…ì†ë³€ìˆ˜ì™€ RM ë³€ìˆ˜ì˜ ê´€ê³„ëŠ” ì„ í˜•ì— ê°€ê¹ì§€ë§Œ ë°©ì˜ ê°¯ìˆ˜ê°€ ì•„ì£¼ ì‘ì•„ì§€ê±°ë‚˜ ì•„ì£¼ ì»¤ì§€ë©´ ì„ í˜•ëª¨í˜•ì—ì„œ ë²—ì–´ë‚œë‹¤.123plt.figure(figsize=(14,6))sns.scatterplot(x=\"RM\", y=\"MEDV\", data=df0, s=60)plt.show() 123model_rm = sm.OLS.from_formula('MEDV ~ RM', data=df0)result_rm = model_rm.fit()print(result_rm.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.484 Model: OLS Adj. R-squared: 0.483 Method: Least Squares F-statistic: 471.8 Date: Thu, 14 May 2020 Prob (F-statistic): 2.49e-74 Time: 19:28:18 Log-Likelihood: -1673.1 No. Observations: 506 AIC: 3350. Df Residuals: 504 BIC: 3359. Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ Intercept -34.6706 2.650 -13.084 0.000 -39.877 -29.465 RM 9.1021 0.419 21.722 0.000 8.279 9.925 ============================================================================== Omnibus: 102.585 Durbin-Watson: 0.684 Prob(Omnibus): 0.000 Jarque-Bera (JB): 612.449 Skew: 0.726 Prob(JB): 1.02e-133 Kurtosis: 8.190 Cond. No. 58.4 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ì´ë ‡ê²Œ RM ë°ì´í„° ì „ì²´ë¥¼ ë†“ê³  ë³´ë©´ ì¢…ì†ë³€ìˆ˜ yì™€ ì•„ì£¼ í° ìƒê´€ê´€ê³„ê°€ ìˆëŠ”ê²ƒìœ¼ë¡œ ë³´ì´ì§€ë§Œ ìœ„ì— ê·¸ë˜í”„ì—ì„œ ë´¤ë“¯ì´, ë°©ì˜ ê°¯ìˆ˜ê°€ ì•„ì£¼ ì ê±°ë‚˜, ë§ìœ¼ë©´ ì„ í˜•ì„±ì„ ë³´ì´ì§€ ì•ŠëŠ” êµ¬ê°„ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ë””í…Œì¼í•˜ê²Œ ìƒê´€ê´€ê²Œë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ RM ë°ì´í„°ë¥¼ ê°•ì œë¡œ ë²”ì£¼í™” ì‹œì¼œ RM ë°ì´í„°ê°€ ê°€ì§€ëŠ” ë¹„ì„ í˜•ì„±ì„ ì¡ì„ ìˆ˜ ìˆë‹¤.1234567rooms = np.arange(3,10)labels = [str(r) for r in rooms[:-1]]df0['CAT_RM'] = np.round(df['RM'])plt.figure(figsize=(14,6))sns.barplot('CAT_RM', 'MEDV', data=df0)plt.show() ì´ë ‡ê²Œ í•˜ë©´ RM ë³€ìˆ˜ìœ¼ë¡œ ì¸í•œ ì¢…ì†ë³€ìˆ˜ì˜ ë³€í™”ë¥¼ ë¹„ì„ í˜• ìƒìˆ˜í•­ìœ¼ë¡œ ëª¨í˜•í™” í•  ìˆ˜ ìˆë‹¤. ì„ í˜•ëª¨í˜•ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.123model_rm2 = sm.OLS.from_formula(\"MEDV ~ C(np.round(RM))\", data=df0)result_rm2 = model_rm2.fit()print(result_rm2.summary()) OLS Regression Results ============================================================================== Dep. Variable: MEDV R-squared: 0.537 Model: OLS Adj. R-squared: 0.532 Method: Least Squares F-statistic: 115.8 Date: Thu, 14 May 2020 Prob (F-statistic): 3.57e-81 Time: 19:33:48 Log-Likelihood: -1645.6 No. Observations: 506 AIC: 3303. Df Residuals: 500 BIC: 3329. Df Model: 5 Covariance Type: nonrobust ========================================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------------------ Intercept 17.0200 2.814 6.049 0.000 11.492 22.548 C(np.round(RM))[T.5.0] -2.0741 2.998 -0.692 0.489 -7.964 3.816 C(np.round(RM))[T.6.0] 2.3460 2.836 0.827 0.409 -3.226 7.918 C(np.round(RM))[T.7.0] 11.0272 2.869 3.843 0.000 5.389 16.665 C(np.round(RM))[T.8.0] 28.5425 3.093 9.228 0.000 22.466 34.619 C(np.round(RM))[T.9.0] 23.6133 4.595 5.139 0.000 14.586 32.641 ============================================================================== Omnibus: 81.744 Durbin-Watson: 0.799 Prob(Omnibus): 0.000 Jarque-Bera (JB): 467.887 Skew: 0.542 Prob(JB): 2.51e-102 Kurtosis: 7.584 Cond. No. 31.1 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.ì‹œê°„ ë…ë¦½ë³€ìˆ˜ì˜ ë³€í˜•ë…ë¦½ë³€ìˆ˜ê°€ ì‹œê°„ì¸ ê²½ìš°ì—ëŠ” íŠ¹ì • ì‹œì ì—ì„œ ê²½ê³¼ëœ ì‹œê°„ê°’ìœ¼ë¡œ ë³€í˜•í•´ì•¼ í•œë‹¤. ì¼ê°„ ì „ê¸° ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ ì„¤ëª…í•œë‹¤.12345data = sm.datasets.get_rdataset(\"elecdaily\", package=\"fpp2\")df_elec = data.data.drop(columns=[\"WorkDay\", \"Temperature\"])df_elec[\"Date\"] = pd.date_range(\"2014-1-1\", \"2014-12-31\")df_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date 360 173.727990 2014-12-27 361 188.512817 2014-12-28 362 191.273009 2014-12-29 363 186.240144 2014-12-30 364 186.370181 2014-12-31 íŒŒì´ì¬ datetime ìë£Œí˜•ì€ toordinal ëª…ë ¹ìœ¼ë¡œ íŠ¹ì • ì‹œì ìœ¼ë¡œë¶€í„° ê²½ê³¼í•œ ì‹œê°„ì˜ ì¼ë‹¨ìœ„ ê°’ì„ êµ¬í•˜ê±°ë‚˜ timestamp ë©”ì„œë“œë¡œ ì´ˆë‹¨ìœ„ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.12345import datetime as dtdf_elec[\"Ordinal\"] = df_elec.Date.map(dt.datetime.toordinal)df_elec[\"Timestamp\"] = df_elec.Date.map(dt.datetime.timestamp)df_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date Ordinal Timestamp 360 173.727990 2014-12-27 735594 1.419606e+09 361 188.512817 2014-12-28 735595 1.419692e+09 362 191.273009 2014-12-29 735596 1.419779e+09 363 186.240144 2014-12-30 735597 1.419865e+09 364 186.370181 2014-12-31 735598 1.419952e+09 ì—¬ê¸°ì—ì„œëŠ” ì¼ë‹¨ìœ„ ì‹œê°„ ê°’ì„ ì‚¬ìš©í•˜ì—¬ íšŒê·€ë¶„ì„ì„ í•œë‹¤. ì‹œê°„ ê°’ì˜ ê²½ìš° í¬ê¸°ê°€ í¬ë¯€ë¡œ ë°˜ë“œì‹œ ìŠ¤ì¼€ì¼ë§ì„ í•´ ì£¼ì–´ì•¼ í•œë‹¤.123model5 = sm.OLS.from_formula(\"Demand ~ scale(Ordinal)\", data=df_elec)result5 = model5.fit()print(result5.summary()) OLS Regression Results ============================================================================== Dep. Variable: Demand R-squared: 0.031 Model: OLS Adj. R-squared: 0.028 Method: Least Squares F-statistic: 11.58 Date: Thu, 14 May 2020 Prob (F-statistic): 0.000739 Time: 19:35:40 Log-Likelihood: -1709.7 No. Observations: 365 AIC: 3423. Df Residuals: 363 BIC: 3431. Df Model: 1 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 221.2775 1.374 160.997 0.000 218.575 223.980 scale(Ordinal) -4.6779 1.374 -3.404 0.001 -7.381 -1.975 ============================================================================== Omnibus: 43.105 Durbin-Watson: 0.677 Prob(Omnibus): 0.000 Jarque-Bera (JB): 96.485 Skew: 0.614 Prob(JB): 1.12e-21 Kurtosis: 5.199 Cond. No. 1.00 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. í•˜ì§€ë§Œ ì‹œê°„ ë…ë¦½ë³€ìˆ˜ëŠ” ì´ ì™¸ì—ë” ë‹¤ì–‘í•œ íŠ¹ì§•ë“¤ì„ ìˆ¨ê¸°ê³  ìˆë‹¤. ì˜ˆë“¤ ë“¤ì–´ ì—°ë„, ì›”, ì¼, ìš”ì¼ ë°ì´í„°ë¥¼ ë³„ë„ì˜ ë…ë¦½ë³€ìˆ˜ë¡œ ë¶„ë¦¬í•˜ê±°ë‚˜ í•œ ë‹¬ ë‚´ì—ì„œ ëª‡ë²ˆì§¸ ë‚ ì§œì¸ì§€ ì›”ì˜ ì‹œì‘ ë˜ëŠ” ëì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì€ ëª¨ë‘ íŠ¹ì§•ê°’ì´ ë  ìˆ˜ ìˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” dt íŠ¹ìˆ˜ ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.12345678910df_elec[\"Year\"] = df_elec['Date'].dt.yeardf_elec[\"Month\"] = df_elec.Date.dt.monthdf_elec[\"DayOfYear\"] = df_elec.Date.dt.dayofyeardf_elec[\"DayOfMonth\"] = df_elec.Date.dt.daysinmonthdf_elec[\"DayOfWeek\"] = df_elec.Date.dt.dayofweekdf_elec[\"WeekOfYear\"] = df_elec.Date.dt.weekofyeardf_elec[\"Weekday\"] = df_elec.Date.dt.weekdaydf_elec[\"IsMonthStart\"] = df_elec.Date.dt.is_month_startdf_elec[\"IsMonthEnd\"] = df_elec.Date.dt.is_month_enddf_elec.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Demand Date Ordinal Timestamp Year Month DayOfYear DayOfMonth DayOfWeek WeekOfYear Weekday IsMonthStart IsMonthEnd 360 173.727990 2014-12-27 735594 1.419606e+09 2014 12 361 31 5 52 5 False False 361 188.512817 2014-12-28 735595 1.419692e+09 2014 12 362 31 6 52 6 False False 362 191.273009 2014-12-29 735596 1.419779e+09 2014 12 363 31 0 1 0 False False 363 186.240144 2014-12-30 735597 1.419865e+09 2014 12 364 31 1 1 1 False False 364 186.370181 2014-12-31 735598 1.419952e+09 2014 12 365 31 2 1 2 False True ì´ë ‡ê²Œ ì¶”ê°€ì ì¸ íŠ¹ì§•ê°’ì„ ì´ìš©í•˜ì—¬ êµ¬í•œ ëª¨í˜•ì€ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.1234567891011feature_names = df_elec.columns.tolist()feature_names.remove(\"Demand\")feature_names.remove(\"Date\")formula = \"\"\"Demand ~ scale(Ordinal) + C(Month) + DayOfYear + C(DayOfMonth) + C(DayOfWeek) + C(Weekday) + C(IsMonthStart) + C(IsMonthEnd)\"\"\"model6 = sm.OLS.from_formula(formula, data=df_elec)result6 = model6.fit()print(result6.summary()) OLS Regression Results ============================================================================== Dep. Variable: Demand R-squared: 0.537 Model: OLS Adj. R-squared: 0.511 Method: Least Squares F-statistic: 19.98 Date: Thu, 14 May 2020 Prob (F-statistic): 4.74e-46 Time: 19:37:49 Log-Likelihood: -1574.8 No. Observations: 365 AIC: 3192. Df Residuals: 344 BIC: 3273. Df Model: 20 Covariance Type: nonrobust =========================================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------------------- Intercept 58.6105 2.423 24.188 0.000 53.844 63.377 C(Month)[T.2] 14.5730 4.587 3.177 0.002 5.551 23.595 C(Month)[T.3] -1.2369 8.663 -0.143 0.887 -18.276 15.802 C(Month)[T.4] -29.1875 10.239 -2.851 0.005 -49.326 -9.049 C(Month)[T.5] 23.4037 15.493 1.511 0.132 -7.069 53.876 C(Month)[T.6] 11.3667 3.758 3.024 0.003 3.974 18.759 C(Month)[T.7] 64.8095 22.750 2.849 0.005 20.063 109.556 C(Month)[T.8] 66.5692 26.490 2.513 0.012 14.467 118.671 C(Month)[T.9] 22.7687 9.491 2.399 0.017 4.100 41.437 C(Month)[T.10] 59.0491 33.895 1.742 0.082 -7.619 125.717 C(Month)[T.11] 33.4276 16.778 1.992 0.047 0.427 66.429 C(Month)[T.12] 72.2523 41.334 1.748 0.081 -9.047 153.552 C(DayOfMonth)[T.30] 38.3755 13.530 2.836 0.005 11.763 64.988 C(DayOfMonth)[T.31] 5.6620 7.806 0.725 0.469 -9.691 21.015 C(DayOfWeek)[T.1] 3.4766 1.829 1.900 0.058 -0.121 7.075 C(DayOfWeek)[T.2] 1.5756 1.821 0.865 0.387 -2.006 5.157 C(DayOfWeek)[T.3] 2.8568 1.831 1.560 0.120 -0.745 6.459 C(DayOfWeek)[T.4] 0.8832 1.831 0.482 0.630 -2.719 4.485 C(DayOfWeek)[T.5] -12.8982 1.831 -7.045 0.000 -16.499 -9.297 C(DayOfWeek)[T.6] -16.4623 1.829 -8.999 0.000 -20.060 -12.864 C(Weekday)[T.1] 3.4766 1.829 1.900 0.058 -0.121 7.075 C(Weekday)[T.2] 1.5756 1.821 0.865 0.387 -2.006 5.157 C(Weekday)[T.3] 2.8568 1.831 1.560 0.120 -0.745 6.459 C(Weekday)[T.4] 0.8832 1.831 0.482 0.630 -2.719 4.485 C(Weekday)[T.5] -12.8982 1.831 -7.045 0.000 -16.499 -9.297 C(Weekday)[T.6] -16.4623 1.829 -8.999 0.000 -20.060 -12.864 C(IsMonthStart)[T.True] 1.2012 5.781 0.208 0.836 -10.169 12.571 C(IsMonthEnd)[T.True] 4.7608 5.781 0.824 0.411 -6.609 16.131 scale(Ordinal) -101.7884 4.209 -24.182 0.000 -110.068 -93.509 DayOfYear 0.6769 0.085 7.926 0.000 0.509 0.845 ============================================================================== Omnibus: 150.460 Durbin-Watson: 0.577 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1586.415 Skew: 1.422 Prob(JB): 0.00 Kurtosis: 12.809 Cond. No. 1.05e+18 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.49e-29. This might indicate that there are strong multicollinearity problems or that the design matrix is singular.12 12","link":"/2020/05/14/leverage-outliar-cooks-distance-anova/"},{"title":"êµ¬ê¸€ ì§€ì˜¤ì½”ë”© API í‚¤ ë°œê¸‰ ë°›ëŠ” ë°©ë²• (How to be issued the Geocoding API key from Google)","text":"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ êµ¬ê¸€ ë§µ ìœ„, ìœ„ì¹˜ì— ë§ˆì»¤ë¥¼ ì°ì–´ ì§€ë„ ìƒ ìœ„ì¹˜ë¥¼ í•œëˆˆì— ì‰½ê²Œ ì•Œì•„ë³´ê¸° ìœ„í•œ GPS ì¢Œí‘œì— ëŒ€í•œ ë¶€ë¶„ì„ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì“°ì´ëŠ” ì£¼ì†Œ(ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ....)ì™€ GPS ì¢Œí‘œë¥¼ ì„œë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ êµ¬ê¸€ì—ì„œ Geocoding APIë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. Geocoding API ì‚¬ìš© ì„¤ì •ê³¼ API í‚¤ ë°œê¸‰ ê³¼ì •ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ê³¼ì •ì€ ì¡°ê¸ˆ ë³µì¡í•  ìˆ˜ë„ ìˆê¸°ì§€ë§Œ ì‰½ê²Œ ë”°ë¼ í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ìì„¸íˆ ì„¤ëª…í•´ë³´ê² ìŠµë‹ˆë‹¤.1. êµ¬ê¸€ í´ë¼ìš°ë“œ ì½˜ì†” ì‚¬ì´íŠ¸ì— ë°©ë¬¸ì•„ë˜ ë§í¬ë¥¼ í´ë¦­í•´ êµ¬ê¸€ ì§€ë„ í”Œë«í¼ ì‚¬ì´íŠ¸ë¡œ ì ‘ì†í•´ì£¼ì„¸ìš”. https://cloud.google.com/maps-platform/ êµ¬ê¸€ ì§€ë„ í”Œë«í¼ ì‚¬ì´íŠ¸ì—ì„œ â€œì‹œì‘í•˜ê¸°â€ í˜¹ì€ â€œì½˜ì†”â€ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê³„ì† ì§„í–‰í•´ì£¼ì„¸ìš”.2. ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê¸°í”„ë¡œì íŠ¸ ì„ íƒ -&gt; ìƒˆ í”„ë¡œì íŠ¸ ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”. 3. API ì‚¬ìš© ì„¤ì •í•˜ê¸°í”„ë¡œì íŠ¸ë¥¼ ë§Œë“  í›„ ì´ì œ ì‚¬ìš©í•  APIë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. êµ¬ê¸€ í´ë¼ìš°ë“œ í”Œë«í¼ì˜ API ë° ì„œë¹„ìŠ¤ -&gt; ë¼ì´ë¸ŒëŸ¬ë¦¬ ë©”ë‰´ë¡œ ì´ë™í•´ì£¼ì„¸ìš”. ê²€ìƒ‰ì°½ì— â€œGeocoding APIâ€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. í´ë¦­!!!!! Geocoding APIì˜ â€œì‚¬ìš© ì„¤ì •â€ ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”. 4. ì‚¬ìš©ì ì¸ì¦ ì •ë³´ ë§Œë“¤ê¸°ì´ì œ ìì‹ ì˜ API í‚¤ë¥¼ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ê¸€ í´ë¼ìš°ë“œ í”Œë«í¼ì˜ API ë° ì„œë¹„ìŠ¤ -&gt; ì‚¬ìš©ì ì¸ì¦ ì •ë³´ ë©”ë‰´ë¡œ ì´ë™í•´ì£¼ì„¸ìš”. ì‚¬ìš©ì ì¸ì¦ ì •ë³´ ë§Œë“¤ê¸° -&gt; API í‚¤ ì„ íƒ6ë²ˆ 5. API í‚¤ ë°œê¸‰ ì™„ë£Œì´ì œ API í‚¤ë¥¼ ë³µì‚¬í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‚¤ ì œí•œì˜ ê²½ìš° ì†Œì¤‘í•œ ìì‹ ì˜ API KEYë¥¼ ì•„ë¬´ë‚˜ í•¨ë¶€ë¡œ ì“¸ ìˆ˜ ì—†ë„ë¡ í•˜ëŠ” ì„¤ì •ì…ë‹ˆë‹¤. ì„¤ì •ì„ ì•ˆí•´ë„ KEYëŠ” ì„¤ì •ì´ ê°€ëŠ¥í•˜ì§€ë§Œ ì œí•œì„ ê±°ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.","link":"/2020/08/12/%E1%84%80%E1%85%AE%E1%84%80%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A9%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B5%E1%86%BC-API-%E1%84%8F%E1%85%B5-%E1%84%87%E1%85%A1%E1%86%AF%E1%84%80%E1%85%B3%E1%86%B8-%E1%84%87%E1%85%A1%E1%86%AE%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-How-to-be-issued-the-Geocoding-API-key-from-Google/"},{"title":"ê³µê³µë°ì´í„°csv_í•œê¸€ê¹¨ì§í˜„ìƒ_ë¬¸ì œí•´ê²°.md","text":"pandas á„€á…©á†¼ë°ì´í„° í•œê¸€ê¹¨ì§í˜„ìƒ ë¬¸ì œí•´ê²° ê³µê³µë°ì´í„° í¬í„¸ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì€ csvíŒŒì¼ì„ pandasì—ì„œ ë¡œë”©í•  ë•Œ í•œê¸€ê¹¨ì§ í˜„ìƒì„ í•´ê²°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. í•œê¸€ ê¹¨ì§ í˜„ìƒì„ í•´ê²°í•˜ê¸° ì „ì— ì˜ì–´ëŠ” ì•„ë¬´ ë¬¸ì œê°€ ì—†ì§€ë§Œ, í•œê¸€ íŒŒì¼ì„ ì½ì–´ì˜¬ ë•ŒëŠ” ì¢…ì¢… ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤. Encoding. ê¸°ë³¸ì ì¸ ì´í•´ ë¬¸ìí˜• ë°ì´í„°ëŠ” ì»´í“¨í„°ê°€ ì¸ì‹ì„ í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì´ê²ƒì„ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ Bit í˜•íƒœë¡œ ë³€í˜•í•´ì•¼í•©ë‹ˆë‹¤. 1Byte = 8Bit Ascii ê³„ì—´ì˜ ë¬¸ìì—´ì€ 0~127ê¹Œì§€ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— 1Byte ì•ˆì— ì¶©ë¶„íˆ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, í•œê¸€ì€ Ascii ì•ˆì— í‘œí˜„ì´ ë¶ˆê°€í•˜ê¸° ë•Œë¬¸ì— í‘œí˜„í•˜ê¸° ìœ„í•´ì„œëŠ” Byteê°€ ì¶©ë¶„íˆ ë” í•„ìš”í•©ë‹ˆë‹¤. ì¸ì½”ë”©ì€ í•œê¸€ê³¼ ê°™ì€ Ascii ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ë¬¸ìë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ë³€í˜• ì‘ì—…ì´ë¼ê³  ì´í•´í•˜ì‹œë©´ ì‰½ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ë¬¸ì œëŠ” ì´ëŸ¬í•œ ì¸ì½”ë”© ë°©ì‹ì´ ì—¬ëŸ¬ê°€ì§€ ì…ë‹ˆë‹¤. ê³µê³µë°ì´í„°(csv) íŒŒì¼ì˜ encoding ê³µê³µë°ì´í„°íŒŒì¼ì€ utf8 ë°©ì‹ì´ë©´ ì¢‹ê² ìœ¼ë‚˜, ì•„ì‰½ê²Œë„(?) cp949 í˜¹ì€ euc-kr í˜•ì‹ìœ¼ë¡œ ì¸ì½”ë”©ì´ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë•Œë¬¸ì— pandasì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ë©´,, â€˜utf8 codec canâ€™t decode byte 0xb1 in position 0: invalid start byteâ€™ ë¼ëŠ” ì—ëŸ¬ë©”ì„¸ì§€ê°€ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤. í•´ê²°ì±…1 pandasì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ì‹œ, â€˜engine=pythonâ€™ ì´ë¼ëŠ” ì½”ë“œë¥¼ ê°™ì´ ì‘ì„±í•˜ê¸° df = pd.read_csv(â€˜test.csvâ€™, engine=â€™pythonâ€™) ë¶ˆëŸ¬ì˜¤ê²Œ ë˜ë©´ ì‘ë™ì€ í•˜ë‚˜ í•œê¸€ì´ ???í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ ë˜ê¸°ë„ í•©ë‹ˆë‹¤. í•´ê²°ì±…2 pandasì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ì‹œ, â€˜encoding=utf8â€™ ì´ë¼ëŠ” ì½”ë“œë¥¼ ê°™ì´ ì‘ì„±í•˜ê¸° df = pd.read_csv(â€˜test.csvâ€™, encoding=â€™utf8â€™) ê·¸ëŸ¬ë‚˜ ê³µê³µë°ì´í„°ì˜ ê²½ìš° í•´ê²°ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. í•´ê²°ì±…3 Excelì—ì„œ íŒŒì¼ì˜ ì¸ì½”ë”© ì˜µì…˜ ë³€ê²½í•˜ê¸° íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ í›„, ë‹¤ë¦„ì´ë¦„ ì €ì¥í•˜ê¸°ë¥¼ í†µí•´ ê³ ê¸‰ì˜µì…˜ì˜ íŒŒì¼ì¸ì½”ë”© í˜•ì‹ì„ utf8ë¡œ ë³€ê²½ í›„ ì €ì¥í•˜ê¸° í•´ê²°ì±…4í•´ê²°ì±…1ë²ˆì˜ ì½”ë“œë¥¼ ì‘ì„±í•œ í›„, encoding=cp949 ë¼ëŠ” ì½”ë“œë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¤ê¸°(ì´ë ‡ê²Œ í•´ì„œ ê³µê³µë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°ë¥¼ ì„±ê³µí•˜ì˜€ìœ¼ë‚˜, utf8ì˜ í˜¸í™˜ì„±ì´ ê°€ì¥ ì¢‹ê¸°ì— utf8ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ë‹¤ê³  íŒë‹¨ë˜ì–´ì§‘ë‹ˆë‹¤.)","link":"/2020/04/29/%EA%B3%B5%EA%B3%B5%EB%8D%B0%EC%9D%B4%ED%84%B0csv-%ED%95%9C%EA%B8%80%EA%B9%A8%EC%A7%90%ED%98%84%EC%83%81-%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0-md/"},{"title":"ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°","text":"Python pandas DataFrame ì´ë‚˜ Series ë‚´ â€˜ë¬¸ìì—´ ì¹¼ëŸ¼â€™ì„ â€˜ìˆ«ìí˜•â€™ìœ¼ë¡œ ë³€í™˜(how to convert string columns to numeric data types in pandas DataFrame, Series) í•˜ëŠ” 2ê°€ì§€ ë°©ë²•ì— ëŒ€í•œ í•´ê²°ì±…! (1) pd.to_numeric() í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ë¬¸ìì—´ ì¹¼ëŸ¼ì˜ ìˆ«ìí˜• ë³€í™˜(2) astype() ë©”ì†Œë“œë¥¼ ì´ìš©í•œ ë¬¸ìì—´ ì¹¼ëŸ¼ì˜ ìˆ«ìí˜• ë³€í™˜ 1-1. í•œê°œì˜ ë¬¸ìì—´ ì¹¼ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë°”ê¾¸ê¸°ë³€ìˆ˜ëª…[â€˜ìƒˆë¡œìš´ì»¬ëŸ¼â€™] = pd.to_numeric(ë³€ìˆ˜ëª…[â€˜ìˆ«ìí˜•ìœ¼ë¡œ ë°”ê¿€ ë¬¸ìí˜• ì»¬ëŸ¼â€™]) 1-2. apply() í•¨ìˆ˜ì™€ to_numeric() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ DataFrame ë‚´ ë‹¤ìˆ˜ì˜ ë¬¸ìì—´ ì¹¼ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë°”ê¾¸ê¸°ë³€ìˆ˜ëª…[[â€˜ìƒˆë¡œìš´ì»¬ëŸ¼1â€™, â€˜ìƒˆë¡œìš´ì»¬ëŸ¼2â€™]] = ë³€ìˆ˜ëª…[[â€˜ê¸°ì¡´ì»¬ëŸ¼â€™, â€˜ê¸°ì¡´ì»¬ëŸ¼â€™]].apply(pd.to_numeric) 1-3. ëª¨ë‘ í•œë²ˆì— ë°”ê¾¸ê¸°[ìƒˆë¡œìš´ë³€ìˆ˜ëª…] = [ê¸°ì¡´ë³€ìˆ˜ëª…].apply(pd.to_numeric) 2-1. DataFrame ë‚´ ëª¨ë“  ë¬¸ìì—´ ì¹¼ëŸ¼ì„ floatë¡œ í•œêº¼ë²ˆì— ë³€í™˜í•˜ê¸°[ì„¸ë¡œìš´ë³€ìˆ˜ëª…] = [ë³€ìˆ˜ëª…].astype(float) 2-2. DataFrame ë‚´ ë¬¸ìì—´ ì¹¼ëŸ¼ë³„ë¡œ int, float ë°ì´í„° í˜•ì‹ ê°œë³„ ì§€ì •í•´ì„œ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í•˜ê¸°[ìƒˆë¡œìš´ë³€ìˆ˜ëª…] = [ë³€ìˆ˜ëª…].astype({â€˜ì»¬ëŸ¼1â€™: int, â€˜ì»¬ëŸ¼2â€™: np.float}) DataFrameì— ë¬¸ìê°€ í¬í•¨ëœ ì¹¼ëŸ¼ì´ ê°™ì´ ìˆì„ ê²½ìš° ValueError","link":"/2020/05/04/%E1%84%86%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%A1%E1%84%92%E1%85%A7%E1%86%BC-%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5-%E1%84%89%E1%85%AE%E1%86%BA%E1%84%8C%E1%85%A1%E1%84%92%E1%85%A7%E1%86%BC%20%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5%E1%84%85%E1%85%A9%20%E1%84%87%E1%85%A7%E1%86%AB%E1%84%92%E1%85%AA%E1%86%AB%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/"},{"title":"ê³µë¶€ë‚´ìš©","text":"markdown, vim, git ê¸°ì´ˆê°œë… githubì— repo ìƒì„± í›„ íŒŒì¼ add, commit, push í•˜ëŠ” ë°©ë²• github ë¸”ë¡œê·¸ ìƒì„± (hexoë¥¼ í†µí•´)","link":"/2020/04/25/%EA%B3%B5%EB%B6%80%EB%82%B4%EC%9A%A9/"},{"title":"ë°ì´í„° ì „ì²˜ë¦¬(Data Scaling with sklearn)","text":"ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì´ë€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì˜ í•˜ë‚˜ì…ë‹ˆë‹¤. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì„ í•´ì£¼ëŠ” ì´ìœ ëŠ” ë°ì´í„°ì˜ ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ í˜¹ì€ ì‘ì€ ê²½ìš°ì— ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµê³¼ì •ì—ì„œ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê±°ë‚˜ ë¬´í•œìœ¼ë¡œ ë°œì‚°í•´ë²„ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, scalingì€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ êµ‰ì¥íˆ ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤. ê°€ë³ê²Œ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤1. scaleì´ë€?1!pip install mglearn Collecting mglearn Downloading mglearn-0.1.9.tar.gz (540 kB) \u001b[K |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 540 kB 506 kB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.18.1) Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (3.2.1) Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.22.2.post1) Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.0.3) Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (7.0.0) Requirement already satisfied: cycler in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.10.0) Requirement already satisfied: imageio in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (2.8.0) Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.14.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.8.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (1.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.4.7) Requirement already satisfied: scipy&gt;=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;mglearn) (1.4.1) Requirement already satisfied: pytz&gt;=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;mglearn) (2019.3) Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from cycler-&gt;mglearn) (1.14.0) Building wheels for collected packages: mglearn Building wheel for mglearn (setup.py) ... \u001b[?25ldone \u001b[?25h Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=6bf4e55bc798dd18a2ce5a5d67e6f134dfc35d54bb51cd8020f85b838a7173b1 Stored in directory: /Users/wglee/Library/Caches/pip/wheels/f1/17/e1/1720d6dcd70187b6b6c3750cb3508798f2b1d57c9d3214b08b Successfully built mglearn Installing collected packages: mglearn Successfully installed mglearn-0.1.912import mglearnmglearn.plots.plot_scaling() (1) StandardScaler ê° featureì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. ëª¨ë“  íŠ¹ì„±ë“¤ì´ ê°™ì€ ìŠ¤ì¼€ì¼ì„ ê°–ê²Œ ë©ë‹ˆë‹¤. (2) RobustScaler ëª¨ë“  íŠ¹ì„±ë“¤ì´ ê°™ì€ í¬ê¸°ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ì ì—ì„œ StandardScalerì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, í‰ê· ê³¼ ë¶„ì‚° ëŒ€ì‹  medianê³¼ quartileì„ ì‚¬ìš©í•©ë‹ˆë‹¤. RobustScalerëŠ” ì´ìƒì¹˜ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤. (3) MinMaxScaler ëª¨ë“  featureê°€ 0ê³¼ 1ì‚¬ì´ì— ìœ„ì¹˜í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ë°ì´í„°ê°€ 2ì°¨ì› ì…‹ì¼ ê²½ìš°, ëª¨ë“  ë°ì´í„°ëŠ” xì¶•ì˜ 0ê³¼ 1 ì‚¬ì´ì—, yì¶•ì˜ 0ê³¼ 1ì‚¬ì´ì— ìœ„ì¹˜í•˜ê²Œ ë©ë‹ˆë‹¤. (4) Normalizer StandardScaler, RobustScaler, MinMaxScalerê°€ ê° columnsì˜ í†µê³„ì¹˜ë¥¼ ì´ìš©í•œë‹¤ë©´ NormalizerëŠ” rowë§ˆë‹¤ ê°ê° ì •ê·œí™”ë©ë‹ˆë‹¤. NormalizerëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬ê°€ 1ì´ ë˜ë„ë¡ ë°ì´í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. (ìœ í´ë¦¬ë“œ ê±°ë¦¬ëŠ” ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ë•Œ ì“°ëŠ” ë°©ë²•)2. Codescikit-learnì— ìˆëŠ” ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°ì…‹ìœ¼ë¡œ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.12345from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitiris = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0) ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤. scalerë¥¼ ì‚¬ìš©í•˜ê¸° ì´ì „ì— ì£¼ì˜í•´ì•¼ë  ì ì„ ë¨¼ì € ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. scalerëŠ” fitê³¼ transform ë©”ì„œë“œë¥¼ ì§€ë‹ˆê³  ìˆìŠµë‹ˆë‹¤. fit ë©”ì„œë“œë¡œ ë°ì´í„° ë³€í™˜ì„ í•™ìŠµí•˜ê³ , transform ë©”ì„œë“œë¡œ ì‹¤ì œ ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•©ë‹ˆë‹¤. ì´ë•Œ, fit ë©”ì„œë“œëŠ” í•™ìŠµìš© ë°ì´í„°ì—ë§Œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ í›„, transform ë©”ì„œë“œë¥¼ í•™ìŠµìš© ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•©ë‹ˆë‹¤. scalerëŠ” fit_transform()ì´ë€ ë‹¨ì¶• ë©”ì„œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. í•™ìŠµìš© ë°ì´í„°ì—ëŠ” fit_transform()ë©”ì„œë“œë¥¼ ì ìš©í•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” transform()ë©”ì„œë“œë¥¼ ì ìš©í•©ë‹ˆë‹¤.(1) StandardScaler code1234567from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train_scale = scaler.fit_transform(X_train)print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : {}'.format(X_train.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : {}'.format(X_train.max(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : {}'.format(X_train_scale.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : {}'.format(X_train_scale.max(axis=0))) ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : [4.3 2.2 1.1 0.1] ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : [7.9 4.4 6.9 2.5] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : [-1.73905934 -2.11220356 -1.37231262 -1.32054283] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : [2.32920943 3.06374081 1.74766642 1.68511841](2) RobustScaler code1234567from sklearn.preprocessing import RobustScalerscaler = RobustScaler()X_train_scale = scaler.fit_transform(X_train)print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : {}'.format(X_train.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : {}'.format(X_train.max(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : {}'.format(X_train_scale.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : {}'.format(X_train_scale.max(axis=0))) ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : [4.3 2.2 1.1 0.1] ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : [7.9 4.4 6.9 2.5] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : [-1.01818182 -1.47826087 -0.82993197 -0.70588235] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : [1.6 2.34782609 0.74829932 0.70588235](3) MinMaxScaler code1234567from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : {}'.format(X_train.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : {}'.format(X_train.max(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : {}'.format(X_train_scale.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : {}'.format(X_train_scale.max(axis=0))) ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : [4.3 2.2 1.1 0.1] ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : [7.9 4.4 6.9 2.5] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : [0. 0. 0. 0.] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : [1. 1. 1. 1.](4) Normalizer code1234567from sklearn.preprocessing import Normalizer scaler = Normalizer()X_train_scale = scaler.fit_transform(X_train)print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : {}'.format(X_train.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : {}'.format(X_train.max(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : {}'.format(X_train_scale.min(axis=0)))print('ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : {}'.format(X_train_scale.max(axis=0))) ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Min value : [4.3 2.2 1.1 0.1] ìŠ¤ì¼€ì¼ ì¡°ì • ì „ features Max value : [7.9 4.4 6.9 2.5] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Min value : [0.67017484 0.2383917 0.16783627 0.0147266 ] ìŠ¤ì¼€ì¼ ì¡°ì • í›„ features Max value : [0.86093857 0.60379053 0.63265489 0.2553047 ]3. ì ìš©í•´ë³´ê¸°ì˜ì‚¬ê²°ì •ë‚˜ë¬´(decisionTree)ë¡œ breat_cancer ë°ì´í„°ì…‹ì„ í•™ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤. ë¨¼ì €, ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ì§€ ì•Šì€ ì±„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.12345678from sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_breast_cancercancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)tree = DecisionTreeClassifier(criterion='entropy', max_depth=1)tree.fit(X_train, y_train)print('test accuracy : %3f' %tree.score(X_test, y_test)) test accuracy : 0.881119 ë‹¤ìŒì€ ë°ì´í„°ë¥¼ MinMaxScalerë¡œ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•˜ê³  ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.1234567from sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler()X_train_scale = scaler.fit_transform(X_train)X_test_scale = scaler.fit_transform(X_test)tree.fit(X_train_scale, y_train)print('test accuracy : %3f' %tree.score(X_test_scale, y_test)) test accuracy : 0.860140 ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜¨ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.12 12","link":"/2020/06/13/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-Data-Scaling-with-sklearn/"},{"title":"ì„ í˜•íšŒê·€ë¶„ì„ ì •ë¦¬","text":"ì„ í˜•íšŒê·€ë¶„ì„ ì •ë¦¬ì˜¤ëŠ˜ì˜ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ìŠ¤ì¿¨ì—ì„œëŠ” ë³¸ê²©ì ìœ¼ë¡œ ì œëŒ€ë¡œ ëœ ì„ í˜•íšŒê·€ë¶„ì„ì— ëŒ€í•´ ë°°ìš¸ ìˆ˜ ìˆì—ˆì–´. ë˜ ë°°ìš´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íŒŒì´ì¬ì„ ì´ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•´ ì§ì ‘ êµ¬í˜„í•´ë³¼ ìˆ˜ë„ ìˆì—ˆëŠ”ë°, ìš”ì•½ëœ ì •ë³´ë“¤ì´ ê°€ì§€ëŠ” í•¨ì¶•ì ì¸ ì˜ë¯¸ê°€ ìˆì–´. ê¸¸ê³ , ì–´ë ¤ìš´ ë‚´ìš©ë“¤ì´ì—ˆì§€ë§Œ ê°„ì‹ íˆ ë§¥ë½ì€ ì¡ê³  ìˆëŠ” ê²ƒ ê°™ì•„ ìŠê¸° ì „ì— ë¹ ë¥¸ ì •ë¦¬ë¥¼ í†µí•´ ë³µìŠµì„ í•˜ë ¤ê³  í•´. listen. ìš°ì„  ì„ í˜•íšŒê·€ë¶„ì„ì´ë€, ì¢…ì† ë³€ìˆ˜ yì™€ í•œ ê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ xì™€ì˜ ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì´ì•¼. ì‚¬ì‹¤ ì´ëŸ° ë‹¨ì–´ë“¤ì„ ì´ìš©í•´ ì„¤ëª…í•˜ë©´ ì‰¬ìš´ ê²ƒë„ ì–´ë µê²Œ ëŠê»´ì§€ê¸° ë§ˆë ¨ì¸ë°, ë‚´ê°€ ìˆ˜ì—…ì‹œê°„ì— ê¹¨ë‹¬ì€ ì„ í˜•íšŒê·€ë¶„ì„ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ì•„. seaborn ë°ì´í„°ì…‹ì— ìˆëŠ” tip ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ë´¤ì–´. ì´ ì§€ì¶œ ë¹„ìš©, íŒì˜ ê¸ˆì•¡, íŒì„ ì¤€ ì‚¬ëŒì˜ ì„±ë³„ê³¼ í¡ì—° ì—¬ë¶€, ìš”ì¼ ì‹œê°„, í•¨ê»˜ ì˜¨ ì¸ì›ë“¤ì˜ ì •ë³´ ë‚˜ì—´ë˜ì–´ ìˆë„¤. ë§‰ë¬´ê°€ë‚´ë¡œ ì„œë¡œ ì—°ê´€ì„± ì—†ì´ ë‚˜ì—´ë˜ì–´ ìˆëŠ” ê²ƒ ê°™ì€ ì´ ë°ì´í„°ë“¤ ì†ì—ì„œ ê·¸ ì–´ë–¤ ìƒê´€ì„±ì´ ìˆì„ê¹Œ?ê°€ë ¹, ì‹ì‚¬ ì¸ì›ì´ ë§ìœ¼ë©´ ë§ì„ ìˆ˜ë¡ íŒì˜ ë¹„ìš©ì´ ì»¤ì§€ê±°ë‚˜, ì €ë… íƒ€ì„ì— ì˜¤ëŠ” ì†ë‹˜ì´ íŒì„ ë” ë§ì´ ì¤€ë‹¤ ë“± ë°ì´í„°ë¥¼ í†µí•´ ì´ëŸ¬í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì–´ ë‚´ì•¼ í•˜ëŠ” ê²ƒì¸ë°, ì´ëŸ¬í•œ ì„ í˜•íšŒê·€ë¶„ì„ì€ ì´ëŸ¬í•œ ë°ì´í„°ì˜ ì„±ë³„, í¡ì—°ì—¬ë¶€, ì‹ì‚¬ ì‹œê°„ëŒ€ ë“±ê³¼ ê°™ì€ ë°ì´í„°(ë…ë¦½ë³€ìˆ˜, x)ë“¤ì´ íŒ(ì¢…ì†ë³€ìˆ˜, y)ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ì§€, ì˜í–¥ì„ ì£¼ë©´ ì–´ë–¤ í•­ëª©ì´ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ì£¼ëŠ”ì§€ ì•Œ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ì— í•˜ë‚˜ë¼ê³  í•  ìˆ˜ ìˆì§€. ê·¸ë ‡ë‹¤ë©´ íŒ ë°ì´í„°ë¥¼ ë²¡í„°ê°’ yë¡œ ë†“ê³  ë‚˜ë¨¸ì§€ ë²¡í„°ë“¤, ì¦‰ ë‚˜ë¨¸ì§€ í–‰ë ¬ ë°ì´í„°ë¥¼ xë¡œ ë†“ê³  ì¼ë°˜ì ìœ¼ë¡œ ì•Œê³  ìˆëŠ” â€˜y = wxâ€™ë¥¼ ë§Œë“¤ ìˆ˜ ìˆê² ì§€? ì—¬ê¸°ì„œ ê¸°ìš¸ê¸°ì— í•´ë‹¹í•˜ëŠ” â€˜wâ€™ê°€ ë°”ë¡œ ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì–´ í•˜ëŠ” ê°€ì¤‘ì¹˜ë¼ëŠ” ê²ƒì´ì•¼.xì˜ í•­ëª©ë“¤(ì„±ë³„, í¡ì—°ì—¬ë¶€, ì‹ì‚¬ì‹œê°„ëŒ€ ë“±) ì¤‘ ì–´ë–¤ í•­ëª©ì´ íŒì´ ë§ê³  ì ìŒì— ìƒê´€ì´ ìˆëŠ”ì§€, ì¦‰ íŒì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³ì„œ íŒì˜ ê¸ˆì•¡ì´ ì˜¬ë¼ê°€ê²Œ í•˜ëŠ”ì§€, í˜¹ì€ ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³ì„œ íŒì˜ ê¸ˆì•¡ì„ ë‚´ë ¤ê°€ê²Œ í•˜ëŠ”ì§€, í˜¹ì€ ì „í˜€ ìƒê´€ì´ ì—†ëŠ”ì§€ ë§ì•¼. ì´ ë•Œ wxë¥¼ xì— ëŒ€í•œ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚œë‹¤ë©´ yì™€ x ì˜ ê´€ê³„ëŠ” ì´ë ‡ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆì–´. (ì–´ë µë‹¤ ì°¸) ì•„ë¬´íŠ¼! ì—¬ê¸°ì„œ y ìœ„ì˜ ^ì™€ ë¬¼ê²°í‘œê°€ ë¶™ìœ¼ë©´ì„œ ì¢€ ë” ê¹Šê²Œ ë“¤ì–´ê°€ê²Œ ë˜ëŠ”ë°, ^ê°€ ë¶™ì€ yë¥¼ y hatì´ë¼ê³  í•´. yëŠ” ìš°ë¦¬ê°€ ë§ˆì£¼í•˜ê³  ìˆëŠ” í˜„ì‹¤ ì„¸ê³„ì—ì„œ ì¼ì–´ë‚œ ì‹¤ì œ ë°ì´í„°ì´ê³ , y hatì€ ì˜ˆì¸¡í•œ ê°€ì¤‘ì¹˜, wì˜ ì˜í–¥ì„ ë°›ì€ â€˜ì˜ˆì¸¡ê°’â€™ì´ë¼ê³  í•  ìˆ˜ ìˆì–´. ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì–´ë–¤ ê°’ì„ ê°€ì§ˆ ì§€ ëª¨ë¥´ëŠ” ê°€ì¤‘ì¹˜ wì˜ ê°’ì„ ì¡°ì •í•˜ì—¬ ìš°ë¦¬ê°€ ì˜ˆì¸¡í•œ ì˜ˆì¸¡ì¹˜ y hatê³¼ ì‹¤ì œ ë°œìƒí•œ ë°ì´í„° ê°’ yì˜ ì°¨ì´(ì”ì°¨)ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆì§€. ì–¸ê¸‰í–ˆë˜ y, y hat, w ëª¨ë‘ëŠ” ìŠ¤ì¹¼ë¼ ê°’ì´ ì•„ë‹ˆë¼ ë²¡í„° í˜¹ì€ í–‰ë ¬ì˜ í˜•íƒœë¥¼ ê°–ì¶”ê³  ìˆì–´. ìœ„ì˜ ì‚¬ì§„ì—ì„œ ê°€ì¤‘ì¹˜ ê°’ wëŠ” ì´ ì„ í˜•íšŒê·€ ëª¨í˜•ì˜ ëª¨ìˆ˜ ì¦‰ parameter ì•¼. ê·¸ë¦¬ê³  x1, x2 â€¦ ëŠ” tip ë°ì´í„°ì—ì„œëŠ” ì¢…ì† ë³€ìˆ˜ tipë°ì´í„°ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€, ì¦‰ ì„±ë³„, í¡ì—°ì—¬ë¶€, ì‹ì‚¬ ì‹œê°„ëŒ€ ë“±ì´ ë˜ëŠ”ê±°ì•¼. ìš°ì„  ì„ í˜•íšŒê·€ë¶„ì„ì— ê²°ì •ë¡ ì  ë°©ë²•ê³¼ í™•ë¥ ë¡ ì  ë°©ë²•ì´ë¼ëŠ” 2ê°€ì§€ ë°©ë²•ì´ ìˆì–´. ê²°ì •ë¡ ì  ë°©ë²•ì€ ë‹¨ìˆœí•˜ê²Œ ë…ë¦½ë³€ìˆ˜ xì— ëŒ€ì‘í•˜ëŠ” ì¢…ì†ë³€ìˆ˜ y ê°’ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒì¸ ë°˜ë©´ í™•ë¥ ë¡ ì  ë°©ë²•ì€ ì´ë¦„ ê·¸ëŒ€ë¡œ x, y ë’¤ì— ì–´ë–¤ â€˜í™•ë¥  ëª¨í˜•â€™ì´ ìˆ¨ì–´ì ¸ ìˆë‹¤ëŠ” ê°€ì •ì´ â€˜ì¶”ê°€â€™ë˜ëŠ”ë°, ì´ ê°€ì •ì´ ì¶”ê°€ë¨ìœ¼ë¡œì¨ ê²°ì •ë¡ ì  ë°©ë²•ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê³  ì‹œì‘í•˜ê²Œ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€.(ë¹„ë¡ ê°€ì •ì´ë¼ í• ì§€ë¼ë„) ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê³  ì‹œì‘í•¨ìœ¼ë¡œì¨ ê²°ì •ë¡ ì  ë°©ë²•ë³´ë‹¤ ì¡°ê¸ˆ ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆì–´. 12345678910111213141516import matplotlibfrom matplotlib import font_manager, rcimport platformtry : if platform.system() == 'windows': # windowsì˜ ê²½ìš° font_name = font_manager.FomntProperties(fname=\"c:/Windows/Font\") rc('font', family = font_name) else: # macì˜ ê²½ìš° rc('font', family = 'AppleGothic')except : passmatplotlib.rcParams['axes.unicode_minus'] = False 12345678from sklearn.datasets import load_bostonboston = load_boston()dfx = pd.DataFrame(boston.data, columns=boston.feature_names)dfy = pd.DataFrame(boston.target, columns=[\"MEDV\"])df = pd.concat([dfx,dfy],axis=1)df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0.0 0.573 6.593 69.1 2.4786 1.0 273.0 21.0 391.99 9.67 22.4 502 0.04527 0.0 11.93 0.0 0.573 6.120 76.7 2.2875 1.0 273.0 21.0 396.90 9.08 20.6 503 0.06076 0.0 11.93 0.0 0.573 6.976 91.0 2.1675 1.0 273.0 21.0 396.90 5.64 23.9 504 0.10959 0.0 11.93 0.0 0.573 6.794 89.3 2.3889 1.0 273.0 21.0 393.45 6.48 22.0 505 0.04741 0.0 11.93 0.0 0.573 6.030 80.8 2.5050 1.0 273.0 21.0 396.90 7.88 11.9 506 rows Ã— 14 columns 1sns.pairplot(df[['MEDV','RM']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2a600f90&gt; 1sns.pairplot(df[['MEDV','CRIM']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2a60d0d0&gt; 1sns.pairplot(df[['MEDV','AGE']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2adfd150&gt; 1sns.pairplot(df[['MEDV','CHAS']]) &lt;seaborn.axisgrid.PairGrid at 0x1a2b239d50&gt; 123456789from sklearn.linear_model import LinearRegressionmodel = LinearRegression().fit(dfx, dfy)predicted = model.predict(dfx)plt.scatter(dfy, predicted, s=10)plt.xlabel(\"ì‹¤ì œ ê°€ê²©\")plt.ylabel(\"ì˜ˆì¸¡ ê°€ê²©\")plt.title(\"ë³´ìŠ¤í„´ ì£¼íƒê°€ê²© ì˜ˆì¸¡ê²°ê³¼\")plt.show() 1import statsmodels.api as sm 123model = sm.OLS(dfy, dfx)result = model.fit()result.params CRIM -0.092897 ZN 0.048715 INDUS -0.004060 CHAS 2.853999 NOX -2.868436 RM 5.928148 AGE -0.007269 DIS -0.968514 RAD 0.171151 TAX -0.009396 PTRATIO -0.392191 B 0.014906 LSTAT -0.416304 dtype: float641print(result.summary()) OLS Regression Results ======================================================================================= Dep. Variable: MEDV R-squared (uncentered): 0.959 Model: OLS Adj. R-squared (uncentered): 0.958 Method: Least Squares F-statistic: 891.3 Date: Tue, 12 May 2020 Prob (F-statistic): 0.00 Time: 19:14:28 Log-Likelihood: -1523.8 No. Observations: 506 AIC: 3074. Df Residuals: 493 BIC: 3128. Df Model: 13 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ CRIM -0.0929 0.034 -2.699 0.007 -0.161 -0.025 ZN 0.0487 0.014 3.382 0.001 0.020 0.077 INDUS -0.0041 0.064 -0.063 0.950 -0.131 0.123 CHAS 2.8540 0.904 3.157 0.002 1.078 4.630 NOX -2.8684 3.359 -0.854 0.394 -9.468 3.731 RM 5.9281 0.309 19.178 0.000 5.321 6.535 AGE -0.0073 0.014 -0.526 0.599 -0.034 0.020 DIS -0.9685 0.196 -4.951 0.000 -1.353 -0.584 RAD 0.1712 0.067 2.564 0.011 0.040 0.302 TAX -0.0094 0.004 -2.395 0.017 -0.017 -0.002 PTRATIO -0.3922 0.110 -3.570 0.000 -0.608 -0.176 B 0.0149 0.003 5.528 0.000 0.010 0.020 LSTAT -0.4163 0.051 -8.197 0.000 -0.516 -0.317 ============================================================================== Omnibus: 204.082 Durbin-Watson: 0.999 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1374.225 Skew: 1.609 Prob(JB): 3.90e-299 Kurtosis: 10.404 Cond. No. 8.50e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 8.5e+03. This might indicate that there are strong multicollinearity or other numerical problems.","link":"/2020/05/12/%E1%84%89%E1%85%A5%E1%86%AB%E1%84%92%E1%85%A7%E1%86%BC%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8_%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5/"},{"title":"wget ì„¤ì¹˜í•˜ëŠ”ë²•","text":"Mac OS Xì—ì„œ wgetì„ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•wgetëŠ” ì •ë§ í¸ë¦¬í•œ ëª…ë ¹ì¤„ ìœ í‹¸ë¦¬í‹°ì§€ë§Œ, ì•ˆíƒ€ê¹ê²Œë„ OS Xì—ëŠ” í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤. ë¬¼ë¡  Curlì€ ì ì ˆí•œ ëŒ€ì²´ë¬¼ì´ ë  ìˆ˜ ìˆì§€ë§Œ, ì¢…ì¢… wgetë¡œ ìŠ¤í¬ë¦½íŠ¸ê°€ ì“°ì—¬ì§€ê³ , curlì„ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì€ ì–´ë µê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ì„œ wgetì´ ì„ í˜¸ë˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. homebrewë¡œ wgetì„ ì„¤ì¹˜í•˜ë©´ ëœë‹¤.1!brew install wget Updating Homebrew... \u001b[34m==&gt;\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m Updated 2 taps (homebrew/core and homebrew/cask). \u001b[34m==&gt;\u001b[0m \u001b[1mUpdated Formulae\u001b[0m plenv postgresql@11 swiftformat tomee-webprofile postgresql postgresql@9.5 tomee-plume postgresql@10 postgresql@9.6 tomee-plus \u001b[34m==&gt;\u001b[0m \u001b[1mUpdated Casks\u001b[0m cryo switchresx \u001b[33mWarning:\u001b[0m wget 1.20.3_2 is already installed and up-to-date To reinstall 1.20.3_2, run `brew reinstall wget`1234!brew uninstall --force node!brew uninstall icu4c &amp;&amp; brew install icu4c!brew unlink icu4c &amp;&amp; brew link icu4c --force!brew install node Uninstalling node... (4,660 files, 60.3MB) \u001b[31mError:\u001b[0m Refusing to uninstall /usr/local/Cellar/icu4c/66.1 because it is required by graphviz, harfbuzz and pango, which are currently installed. You can override this and force removal with: brew uninstall --ignore-dependencies icu4c Unlinking /usr/local/Cellar/icu4c/66.1... 0 symlinks removed \u001b[33mWarning:\u001b[0m Refusing to link macOS provided/shadowed software: icu4c If you need to have icu4c first in your PATH run: echo &apos;export PATH=&quot;/usr/local/opt/icu4c/bin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile echo &apos;export PATH=&quot;/usr/local/opt/icu4c/sbin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile For compilers to find icu4c you may need to set: export LDFLAGS=&quot;-L/usr/local/opt/icu4c/lib&quot; export CPPFLAGS=&quot;-I/usr/local/opt/icu4c/include&quot; Updating Homebrew... \u001b[34m==&gt;\u001b[0m \u001b[1mDownloading https://homebrew.bintray.com/bottles/node-14.3.0.catalina.bottle\u001b[0m \u001b[34m==&gt;\u001b[0m \u001b[1mDownloading from https://akamai.bintray.com/e3/e34c4c25365bc0f5cc245d791dd93\u001b[0m ######################################################################## 100.0% \u001b[34m==&gt;\u001b[0m \u001b[1mPouring node-14.3.0.catalina.bottle.tar.gz\u001b[0m \u001b[34m==&gt;\u001b[0m \u001b[1mCaveats\u001b[0m Bash completion has been installed to: /usr/local/etc/bash_completion.d \u001b[34m==&gt;\u001b[0m \u001b[1mSummary\u001b[0m ğŸº /usr/local/Cellar/node/14.3.0: 4,659 files, 60.8MB ** í•„ìê°€ ê°‘ìê¸° hexoê°€ ì‘ë™ë˜ì§€ ì•Šì•„ ì ì–ì´ ë‹¹í™©í•˜ë˜ ì¤‘ í•´ê²°ë²•ì„ ì°¾ì•„ì„œ ì´ê²ƒë„ ê³µìœ í•´. !brew uninstall --force node !brew uninstall icu4c &amp;&amp; brew install icu4c !brew unlink icu4c &amp;&amp; brew link icu4c --force !brew install node ë…¸ë“œ ì–¸ì¸ìŠ¤í†¨ í›„ ì¬ì„¤ì¹˜í•˜ë©´ ë¬¸ì œ ì—†ì´ ì‘ë™í•œë‹¤.","link":"/2020/05/28/wget-%EC%84%A4%EC%B9%98%ED%95%98%EB%8A%94%EB%B2%95/"},{"title":"ê³¼ëŒ€ì í•©(Overfitting)ê³¼ ê³¼ì†Œì í•©(Underfitting)","text":"ê³¼ëŒ€ì í•©(Overfitting)ê³¼ ê³¼ì†Œì í•©(Underfitting)ì¼ì • ì—í¬í¬ ë™ì•ˆ í›ˆë ¨ì„ ì‹œí‚¤ë©´ ê²€ì¦ì„¸íŠ¸ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì´ ìµœê³ ì ì— ë„ë‹¬í•œ ë‹¤ìŒ ê°ì†Œí•˜ê¸° ì‹œì‘í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ ì§„ì§œ ì›í•˜ëŠ” ê²ƒì€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸(ë˜ëŠ” ì´ì „ì— ë³¸ ì  ì—†ëŠ” ë°ì´í„°)ì— ì˜ ì¼ë°˜í™”ë˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ê³¼ì†Œì í•©ì´ë€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë  ì—¬ì§€ê°€ ì•„ì§ ìˆì„ ë•Œ ì¼ì–´ë‚©ë‹ˆë‹¤. ë°œìƒí•˜ëŠ” ì›ì¸ì€ ì—¬ëŸ¬ê°€ì§€ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜, ê·œì œê°€ ë„ˆë¬´ ë§ê±°ë‚˜, ê·¸ëƒ¥ ë‹¨ìˆœíˆ ì¶©ë¶„íˆ ì˜¤ë˜ í›ˆë ¨í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì…ë‹ˆë‹¤. ì¦‰ ë„¤íŠ¸ì›Œí¬ê°€ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ì ì ˆí•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í–ˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ëª¨ë¸ì„ ë„ˆë¬´ ì˜¤ë˜ í›ˆë ¨í•˜ë©´ ê³¼ëŒ€ì í•©ë˜ê¸° ì‹œì‘í•˜ê³  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì¼ë°˜í™”ë˜ì§€ ëª»í•˜ëŠ” íŒ¨í„´ì„ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ í•™ìŠµí•©ë‹ˆë‹¤. ê³¼ëŒ€ì í•©ê³¼ ê³¼ì†Œì í•© ì‚¬ì´ì—ì„œ ê· í˜•ì„ ì¡ì•„ì•¼ í•©ë‹ˆë‹¤. ê· í˜•ì„ ì˜ ì¡ê³  ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ 2ê°€ì§€ ê·œì œë°©ë²•ì„ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤1234567import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersimport numpy as npimport matplotlib.pyplot as pltprint(tf.__version__) 2.4.0-dev20200724 ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œë¥¼ ë°›ê³  ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì!1234567891011121314NUM_WORDS = 1000(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)def multi_hot_sequences(sequences, dimension): # 0ìœ¼ë¡œ ì±„ì›Œì§„ (len(sequences), dimension) í¬ê¸°ì˜ í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤ results = np.zeros((len(sequences), dimension)) for i, word_indices in enumerate(sequences): results[i, word_indices] = 1.0 # results[i]ì˜ íŠ¹ì • ì¸ë±ìŠ¤ë§Œ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤ return resultstrain_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS) 1234plt.plot(train_data[0])plt.grid(False)plt.xticks(rotation=45)plt.show() ê¸°ì¤€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ê¸°ì¤€ë³´ë‹¤ ìœ ë‹›ì˜ ìˆ˜ê°€ í¬ê±°ë‚˜ ì‘ì€ ëª¨ë¸ê³¼ ë¹„êµë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤.1234567891011base_model = keras.Sequential([ keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(16, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])base_model.summary() Model: &quot;sequential_2&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 16) 16016 _________________________________________________________________ dense_12 (Dense) (None, 16) 272 _________________________________________________________________ dense_13 (Dense) (None, 1) 17 ================================================================= Total params: 16,305 Trainable params: 16,305 Non-trainable params: 0 _________________________________________________________________12base_history = base_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.2555 - accuracy: 0.8971 - binary_crossentropy: 0.2555 - val_loss: 0.3410 - val_accuracy: 0.8558 - val_binary_crossentropy: 0.3410 Epoch 2/20 49/49 - 0s - loss: 0.2436 - accuracy: 0.9030 - binary_crossentropy: 0.2436 - val_loss: 0.3454 - val_accuracy: 0.8540 - val_binary_crossentropy: 0.3454 Epoch 3/20 49/49 - 0s - loss: 0.2356 - accuracy: 0.9068 - binary_crossentropy: 0.2356 - val_loss: 0.3525 - val_accuracy: 0.8508 - val_binary_crossentropy: 0.3525 Epoch 4/20 49/49 - 0s - loss: 0.2259 - accuracy: 0.9102 - binary_crossentropy: 0.2259 - val_loss: 0.3638 - val_accuracy: 0.8482 - val_binary_crossentropy: 0.3638 Epoch 5/20 49/49 - 0s - loss: 0.2178 - accuracy: 0.9142 - binary_crossentropy: 0.2178 - val_loss: 0.3701 - val_accuracy: 0.8487 - val_binary_crossentropy: 0.3701 Epoch 6/20 49/49 - 0s - loss: 0.2093 - accuracy: 0.9188 - binary_crossentropy: 0.2093 - val_loss: 0.3809 - val_accuracy: 0.8469 - val_binary_crossentropy: 0.3809 Epoch 7/20 49/49 - 0s - loss: 0.2026 - accuracy: 0.9208 - binary_crossentropy: 0.2026 - val_loss: 0.3854 - val_accuracy: 0.8465 - val_binary_crossentropy: 0.3854 Epoch 8/20 49/49 - 0s - loss: 0.1963 - accuracy: 0.9240 - binary_crossentropy: 0.1963 - val_loss: 0.3996 - val_accuracy: 0.8430 - val_binary_crossentropy: 0.3996 Epoch 9/20 49/49 - 0s - loss: 0.1905 - accuracy: 0.9254 - binary_crossentropy: 0.1905 - val_loss: 0.4014 - val_accuracy: 0.8421 - val_binary_crossentropy: 0.4014 Epoch 10/20 49/49 - 0s - loss: 0.1846 - accuracy: 0.9307 - binary_crossentropy: 0.1846 - val_loss: 0.4143 - val_accuracy: 0.8418 - val_binary_crossentropy: 0.4143 Epoch 11/20 49/49 - 0s - loss: 0.1787 - accuracy: 0.9322 - binary_crossentropy: 0.1787 - val_loss: 0.4300 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.4300 Epoch 12/20 49/49 - 0s - loss: 0.1739 - accuracy: 0.9329 - binary_crossentropy: 0.1739 - val_loss: 0.4402 - val_accuracy: 0.8372 - val_binary_crossentropy: 0.4402 Epoch 13/20 49/49 - 0s - loss: 0.1663 - accuracy: 0.9373 - binary_crossentropy: 0.1663 - val_loss: 0.4508 - val_accuracy: 0.8358 - val_binary_crossentropy: 0.4508 Epoch 14/20 49/49 - 0s - loss: 0.1613 - accuracy: 0.9396 - binary_crossentropy: 0.1613 - val_loss: 0.4584 - val_accuracy: 0.8364 - val_binary_crossentropy: 0.4584 Epoch 15/20 49/49 - 0s - loss: 0.1581 - accuracy: 0.9400 - binary_crossentropy: 0.1581 - val_loss: 0.4805 - val_accuracy: 0.8356 - val_binary_crossentropy: 0.4805 Epoch 16/20 49/49 - 0s - loss: 0.1534 - accuracy: 0.9419 - binary_crossentropy: 0.1534 - val_loss: 0.4836 - val_accuracy: 0.8343 - val_binary_crossentropy: 0.4836 Epoch 17/20 49/49 - 0s - loss: 0.1477 - accuracy: 0.9454 - binary_crossentropy: 0.1477 - val_loss: 0.5082 - val_accuracy: 0.8330 - val_binary_crossentropy: 0.5082 Epoch 18/20 49/49 - 0s - loss: 0.1440 - accuracy: 0.9458 - binary_crossentropy: 0.1440 - val_loss: 0.5069 - val_accuracy: 0.8342 - val_binary_crossentropy: 0.5069 Epoch 19/20 49/49 - 0s - loss: 0.1382 - accuracy: 0.9489 - binary_crossentropy: 0.1382 - val_loss: 0.5187 - val_accuracy: 0.8323 - val_binary_crossentropy: 0.5187 Epoch 20/20 49/49 - 0s - loss: 0.1339 - accuracy: 0.9520 - binary_crossentropy: 0.1339 - val_loss: 0.5385 - val_accuracy: 0.8310 - val_binary_crossentropy: 0.5385 ì‘ì€ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì1234567891011small_model = keras.Sequential([ keras.layers.Dense(6, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(6, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])small_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])small_model.summary() Model: &quot;sequential_4&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 6) 6006 _________________________________________________________________ dense_19 (Dense) (None, 6) 42 _________________________________________________________________ dense_20 (Dense) (None, 1) 7 ================================================================= Total params: 6,055 Trainable params: 6,055 Non-trainable params: 0 _________________________________________________________________12small_history = small_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.2994 - accuracy: 0.8785 - binary_crossentropy: 0.2994 - val_loss: 0.3305 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3305 Epoch 2/20 49/49 - 0s - loss: 0.2972 - accuracy: 0.8790 - binary_crossentropy: 0.2972 - val_loss: 0.3306 - val_accuracy: 0.8599 - val_binary_crossentropy: 0.3306 Epoch 3/20 49/49 - 0s - loss: 0.2970 - accuracy: 0.8782 - binary_crossentropy: 0.2970 - val_loss: 0.3343 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3343 Epoch 4/20 49/49 - 0s - loss: 0.2965 - accuracy: 0.8777 - binary_crossentropy: 0.2965 - val_loss: 0.3312 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3312 Epoch 5/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8794 - binary_crossentropy: 0.2960 - val_loss: 0.3314 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3314 Epoch 6/20 49/49 - 0s - loss: 0.2957 - accuracy: 0.8783 - binary_crossentropy: 0.2957 - val_loss: 0.3320 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3320 Epoch 7/20 49/49 - 0s - loss: 0.2968 - accuracy: 0.8768 - binary_crossentropy: 0.2968 - val_loss: 0.3321 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3321 Epoch 8/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8790 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8594 - val_binary_crossentropy: 0.3323 Epoch 9/20 49/49 - 0s - loss: 0.2960 - accuracy: 0.8787 - binary_crossentropy: 0.2960 - val_loss: 0.3323 - val_accuracy: 0.8582 - val_binary_crossentropy: 0.3323 Epoch 10/20 49/49 - 0s - loss: 0.2959 - accuracy: 0.8784 - binary_crossentropy: 0.2959 - val_loss: 0.3327 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3327 Epoch 11/20 49/49 - 0s - loss: 0.2953 - accuracy: 0.8789 - binary_crossentropy: 0.2953 - val_loss: 0.3334 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3334 Epoch 12/20 49/49 - 0s - loss: 0.2970 - accuracy: 0.8775 - binary_crossentropy: 0.2970 - val_loss: 0.3334 - val_accuracy: 0.8578 - val_binary_crossentropy: 0.3334 Epoch 13/20 49/49 - 0s - loss: 0.2951 - accuracy: 0.8798 - binary_crossentropy: 0.2951 - val_loss: 0.3341 - val_accuracy: 0.8581 - val_binary_crossentropy: 0.3341 Epoch 14/20 49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3323 - val_accuracy: 0.8590 - val_binary_crossentropy: 0.3323 Epoch 15/20 49/49 - 0s - loss: 0.2950 - accuracy: 0.8786 - binary_crossentropy: 0.2950 - val_loss: 0.3324 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3324 Epoch 16/20 49/49 - 0s - loss: 0.2949 - accuracy: 0.8790 - binary_crossentropy: 0.2949 - val_loss: 0.3330 - val_accuracy: 0.8593 - val_binary_crossentropy: 0.3330 Epoch 17/20 49/49 - 0s - loss: 0.2946 - accuracy: 0.8784 - binary_crossentropy: 0.2946 - val_loss: 0.3324 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3324 Epoch 18/20 49/49 - 0s - loss: 0.2952 - accuracy: 0.8784 - binary_crossentropy: 0.2952 - val_loss: 0.3329 - val_accuracy: 0.8585 - val_binary_crossentropy: 0.3329 Epoch 19/20 49/49 - 0s - loss: 0.2943 - accuracy: 0.8794 - binary_crossentropy: 0.2943 - val_loss: 0.3330 - val_accuracy: 0.8588 - val_binary_crossentropy: 0.3330 Epoch 20/20 49/49 - 0s - loss: 0.2949 - accuracy: 0.8789 - binary_crossentropy: 0.2949 - val_loss: 0.3329 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3329 í° ëª¨ë¸ ë§Œë“¤ê¸°1234567891011big_model = keras.Sequential([ keras.layers.Dense(128, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(1, activation='sigmoid')])big_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])big_model.summary() Model: &quot;sequential_5&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_21 (Dense) (None, 128) 128128 _________________________________________________________________ dense_22 (Dense) (None, 128) 16512 _________________________________________________________________ dense_23 (Dense) (None, 1) 129 ================================================================= Total params: 144,769 Trainable params: 144,769 Non-trainable params: 0 _________________________________________________________________12big_history = big_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 0s - loss: 0.0047 - accuracy: 0.9999 - binary_crossentropy: 0.0047 - val_loss: 0.6867 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.6867 Epoch 2/20 49/49 - 0s - loss: 0.0029 - accuracy: 1.0000 - binary_crossentropy: 0.0029 - val_loss: 0.7205 - val_accuracy: 0.8382 - val_binary_crossentropy: 0.7205 Epoch 3/20 49/49 - 0s - loss: 0.0019 - accuracy: 1.0000 - binary_crossentropy: 0.0019 - val_loss: 0.7533 - val_accuracy: 0.8388 - val_binary_crossentropy: 0.7533 Epoch 4/20 49/49 - 0s - loss: 0.0014 - accuracy: 1.0000 - binary_crossentropy: 0.0014 - val_loss: 0.7802 - val_accuracy: 0.8383 - val_binary_crossentropy: 0.7802 Epoch 5/20 49/49 - 0s - loss: 0.0010 - accuracy: 1.0000 - binary_crossentropy: 0.0010 - val_loss: 0.8079 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8079 Epoch 6/20 49/49 - 0s - loss: 8.0437e-04 - accuracy: 1.0000 - binary_crossentropy: 8.0437e-04 - val_loss: 0.8324 - val_accuracy: 0.8392 - val_binary_crossentropy: 0.8324 Epoch 7/20 49/49 - 0s - loss: 6.4169e-04 - accuracy: 1.0000 - binary_crossentropy: 6.4169e-04 - val_loss: 0.8510 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8510 Epoch 8/20 49/49 - 0s - loss: 5.2259e-04 - accuracy: 1.0000 - binary_crossentropy: 5.2259e-04 - val_loss: 0.8707 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.8707 Epoch 9/20 49/49 - 0s - loss: 4.3499e-04 - accuracy: 1.0000 - binary_crossentropy: 4.3499e-04 - val_loss: 0.8885 - val_accuracy: 0.8395 - val_binary_crossentropy: 0.8885 Epoch 10/20 49/49 - 0s - loss: 3.6612e-04 - accuracy: 1.0000 - binary_crossentropy: 3.6612e-04 - val_loss: 0.9055 - val_accuracy: 0.8397 - val_binary_crossentropy: 0.9055 Epoch 11/20 49/49 - 0s - loss: 3.1179e-04 - accuracy: 1.0000 - binary_crossentropy: 3.1179e-04 - val_loss: 0.9202 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9202 Epoch 12/20 49/49 - 0s - loss: 2.6851e-04 - accuracy: 1.0000 - binary_crossentropy: 2.6851e-04 - val_loss: 0.9358 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9358 Epoch 13/20 49/49 - 0s - loss: 2.3418e-04 - accuracy: 1.0000 - binary_crossentropy: 2.3418e-04 - val_loss: 0.9482 - val_accuracy: 0.8399 - val_binary_crossentropy: 0.9482 Epoch 14/20 49/49 - 0s - loss: 2.0480e-04 - accuracy: 1.0000 - binary_crossentropy: 2.0480e-04 - val_loss: 0.9615 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9615 Epoch 15/20 49/49 - 0s - loss: 1.8099e-04 - accuracy: 1.0000 - binary_crossentropy: 1.8099e-04 - val_loss: 0.9732 - val_accuracy: 0.8396 - val_binary_crossentropy: 0.9732 Epoch 16/20 49/49 - 0s - loss: 1.6065e-04 - accuracy: 1.0000 - binary_crossentropy: 1.6065e-04 - val_loss: 0.9851 - val_accuracy: 0.8400 - val_binary_crossentropy: 0.9851 Epoch 17/20 49/49 - 0s - loss: 1.4336e-04 - accuracy: 1.0000 - binary_crossentropy: 1.4336e-04 - val_loss: 0.9966 - val_accuracy: 0.8401 - val_binary_crossentropy: 0.9966 Epoch 18/20 49/49 - 0s - loss: 1.2880e-04 - accuracy: 1.0000 - binary_crossentropy: 1.2880e-04 - val_loss: 1.0070 - val_accuracy: 0.8399 - val_binary_crossentropy: 1.0070 Epoch 19/20 49/49 - 0s - loss: 1.1636e-04 - accuracy: 1.0000 - binary_crossentropy: 1.1636e-04 - val_loss: 1.0171 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0171 Epoch 20/20 49/49 - 0s - loss: 1.0553e-04 - accuracy: 1.0000 - binary_crossentropy: 1.0553e-04 - val_loss: 1.0270 - val_accuracy: 0.8398 - val_binary_crossentropy: 1.0270training datasetì˜ loss(ì†ì‹¤)ê°’ê³¼ test datasetì˜ loss(ì†ì‹¤)ê°’ ì‹œê°í™” 1234567891011121314151617def plot_history(histories, key='binary_crossentropy'): plt.figure(figsize=(16,6)) for name, history in histories: val = plt.plot(history.epoch, history.history['val_' + key], '--', label=name.title()+' Val') plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+'Train') plt.xlabel('Epochs') plt.ylabel(key.replace('-', ' ').title()) plt.legend() plt.xlim([0, max(history.epoch)])plot_history([('base', base_history), ('smaller', small_history), ('bigger', big_history)]) big modelì˜ ê²½ìš° ì—í¬í¬ê°€ ì‹œì‘í•˜ìë§ˆì ê³¼ëŒ€ì í•©(Overfitting)ì´ ì¼ì–´ë‚˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê³  ìƒê°ë³´ë‹¤ ì‹¬í•˜ê²Œ ì´ë¤„ì§‘ë‹ˆë‹¤. ëª¨ë¸ ë„¤íŠ¸ì›Œí¬ì˜ ìš©ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ê³¼ëŒ€ì í•©ì´ ë  í™•ë¥ ì´ ì»¤ì§‘ë‹ˆë‹¤.(í›ˆë ¨ lossê°’ê³¼ ê²€ì¦ lossê°’ ì‚¬ì´ì— í° ì°¨ì´ê°€ ë°œìƒ) ê³¼ëŒ€ì í•©(Overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì „ëµ - ê°€ì¤‘ì¹˜ ê·œì œí•˜ê¸° 1. í›ˆë ¨ ë°ì´í„°ì™€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë°ì´í„°ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ê°€ì¤‘ì¹˜ì˜ ì¡°í•©ì„ ê°„ë‹¨í•˜ê²Œ! 2. ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ë¶„í¬ë¥¼ ë´¤ì„ ë•Œ ì—”íŠ¸ë¡œí”¼ê°€ ì‘ì€ ëª¨ë¸(ì ì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ëª¨ë¸), ì¦‰ ê³¼ëŒ€ì í•©ì„ ì™„í™”ì‹œí‚¤ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì€ ê°€ì¤‘ì¹˜ê°€ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ë„¤íŠ¸ì›Œí¬ì˜ ë³µì¡ë„ì— ì œì•½ì„ ê°€í•˜ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. &apos;ê°€ì¤‘ì¹˜ ê·œì œ(Weight regularization) * L1 ê·œì œëŠ” ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ì— ë¹„ë¡€í•˜ëŠ” ë¹„ìš©ì´ ì¶”ê°€ * L2 ê·œì œëŠ” ê°€ì¤‘ì¹˜ì˜ ì œê³±ì— ë¹„ë¡€í•˜ëŠ” ë¹„ìš©ì´ ì¶”ê°€, ì‹ ê²½ë§ì—ì„œëŠ” L2ê·œì œë¥¼ ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¼ê³ ë„ í•©ë‹ˆë‹¤.1234567891011121314l2_model = keras.Sequential([ keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'), keras.layers.Dense(1, activation='sigmoid')])l2_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])l2_history = l2_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 1s - loss: 0.6362 - accuracy: 0.6929 - binary_crossentropy: 0.5927 - val_loss: 0.4927 - val_accuracy: 0.8113 - val_binary_crossentropy: 0.4513 Epoch 2/20 49/49 - 0s - loss: 0.4164 - accuracy: 0.8462 - binary_crossentropy: 0.3749 - val_loss: 0.3873 - val_accuracy: 0.8545 - val_binary_crossentropy: 0.3460 Epoch 3/20 49/49 - 0s - loss: 0.3636 - accuracy: 0.8669 - binary_crossentropy: 0.3230 - val_loss: 0.3708 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3312 Epoch 4/20 49/49 - 0s - loss: 0.3498 - accuracy: 0.8721 - binary_crossentropy: 0.3113 - val_loss: 0.3687 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3312 Epoch 5/20 49/49 - 0s - loss: 0.3440 - accuracy: 0.8726 - binary_crossentropy: 0.3073 - val_loss: 0.3640 - val_accuracy: 0.8602 - val_binary_crossentropy: 0.3283 Epoch 6/20 49/49 - 0s - loss: 0.3393 - accuracy: 0.8760 - binary_crossentropy: 0.3044 - val_loss: 0.3622 - val_accuracy: 0.8598 - val_binary_crossentropy: 0.3281 Epoch 7/20 49/49 - 0s - loss: 0.3369 - accuracy: 0.8749 - binary_crossentropy: 0.3034 - val_loss: 0.3604 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3276 Epoch 8/20 49/49 - 0s - loss: 0.3349 - accuracy: 0.8754 - binary_crossentropy: 0.3027 - val_loss: 0.3595 - val_accuracy: 0.8595 - val_binary_crossentropy: 0.3281 Epoch 9/20 49/49 - 0s - loss: 0.3325 - accuracy: 0.8746 - binary_crossentropy: 0.3015 - val_loss: 0.3608 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3304 Epoch 10/20 49/49 - 0s - loss: 0.3332 - accuracy: 0.8744 - binary_crossentropy: 0.3031 - val_loss: 0.3599 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3304 Epoch 11/20 49/49 - 0s - loss: 0.3305 - accuracy: 0.8750 - binary_crossentropy: 0.3012 - val_loss: 0.3563 - val_accuracy: 0.8592 - val_binary_crossentropy: 0.3274 Epoch 12/20 49/49 - 0s - loss: 0.3290 - accuracy: 0.8748 - binary_crossentropy: 0.3004 - val_loss: 0.3554 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3272 Epoch 13/20 49/49 - 0s - loss: 0.3272 - accuracy: 0.8752 - binary_crossentropy: 0.2991 - val_loss: 0.3526 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247 Epoch 14/20 49/49 - 0s - loss: 0.3251 - accuracy: 0.8760 - binary_crossentropy: 0.2972 - val_loss: 0.3522 - val_accuracy: 0.8596 - val_binary_crossentropy: 0.3243 Epoch 15/20 49/49 - 0s - loss: 0.3232 - accuracy: 0.8759 - binary_crossentropy: 0.2953 - val_loss: 0.3547 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3268 Epoch 16/20 49/49 - 0s - loss: 0.3214 - accuracy: 0.8770 - binary_crossentropy: 0.2936 - val_loss: 0.3522 - val_accuracy: 0.8601 - val_binary_crossentropy: 0.3246 Epoch 17/20 49/49 - 0s - loss: 0.3201 - accuracy: 0.8781 - binary_crossentropy: 0.2926 - val_loss: 0.3512 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3238 Epoch 18/20 49/49 - 0s - loss: 0.3194 - accuracy: 0.8766 - binary_crossentropy: 0.2921 - val_loss: 0.3544 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3271 Epoch 19/20 49/49 - 0s - loss: 0.3180 - accuracy: 0.8772 - binary_crossentropy: 0.2908 - val_loss: 0.3509 - val_accuracy: 0.8603 - val_binary_crossentropy: 0.3238 Epoch 20/20 49/49 - 0s - loss: 0.3167 - accuracy: 0.8768 - binary_crossentropy: 0.2896 - val_loss: 0.3491 - val_accuracy: 0.8608 - val_binary_crossentropy: 0.3221123plot_history([('base', base_history), ('L2', l2_history) ]) ê²°ê³¼ì—ì„œ ë³´ë“¯ì´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ëŠ” ë˜‘ê°™ì§€ë§Œ L2ê·œì œë¥¼ ì ìš©í•œ ëª¨ë¸ì´ base modelë³´ë‹¤ ê³¼ëŒ€ì í•©ì— í›¨ì”¬ ì˜ ê²¬ë””ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. - dropout ì¶”ê°€í•˜ê¸° * ì‹ ê²½ë§ì—ì„œ ì“°ì´ëŠ” ê°€ì¥ íš¨ê³¼ì ì´ê³  ë„ë¦¬ ì‚¬ìš©í•˜ëŠ” ê·œì œ ê¸°ë²•ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. * dropoutì€ ì¸µì„ ì´ìš©í•´ ë„¤íŠ¸ì›Œí¬ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ê°œì˜ ì¸µì— dropout ê·œì œë¥¼ ì¶”ê°€í•˜ì—¬ ê³¼ëŒ€ì í•©ì´ ì–¼ë§ˆë‚˜ ê°ì†Œí•˜ëŠ”ì§€ ì•Œì•„ ë³´ê² ìŠµë‹ˆë‹¤.1234567891011121314dpt_model = keras.Sequential([ keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)), keras.layers.Dropout(0.5), keras.layers.Dense(16, activation='relu'), keras.layers.Dropout(0.5), keras.layers.Dense(1, activation='sigmoid')])dpt_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])dpt_history = dpt_model.fit(train_data, train_labels, epochs=20, batch_size=512, validation_data=(test_data, test_labels), verbose=2) Epoch 1/20 49/49 - 1s - loss: 0.6841 - accuracy: 0.5583 - binary_crossentropy: 0.6841 - val_loss: 0.6280 - val_accuracy: 0.7269 - val_binary_crossentropy: 0.6280 Epoch 2/20 49/49 - 0s - loss: 0.5848 - accuracy: 0.6974 - binary_crossentropy: 0.5848 - val_loss: 0.4655 - val_accuracy: 0.8180 - val_binary_crossentropy: 0.4655 Epoch 3/20 49/49 - 0s - loss: 0.4784 - accuracy: 0.7861 - binary_crossentropy: 0.4784 - val_loss: 0.3797 - val_accuracy: 0.8453 - val_binary_crossentropy: 0.3797 Epoch 4/20 49/49 - 0s - loss: 0.4250 - accuracy: 0.8195 - binary_crossentropy: 0.4250 - val_loss: 0.3453 - val_accuracy: 0.8510 - val_binary_crossentropy: 0.3453 Epoch 5/20 49/49 - 0s - loss: 0.3931 - accuracy: 0.8381 - binary_crossentropy: 0.3931 - val_loss: 0.3338 - val_accuracy: 0.8548 - val_binary_crossentropy: 0.3338 Epoch 6/20 49/49 - 0s - loss: 0.3758 - accuracy: 0.8480 - binary_crossentropy: 0.3758 - val_loss: 0.3299 - val_accuracy: 0.8587 - val_binary_crossentropy: 0.3299 Epoch 7/20 49/49 - 0s - loss: 0.3600 - accuracy: 0.8544 - binary_crossentropy: 0.3600 - val_loss: 0.3224 - val_accuracy: 0.8612 - val_binary_crossentropy: 0.3224 Epoch 8/20 49/49 - 0s - loss: 0.3493 - accuracy: 0.8607 - binary_crossentropy: 0.3493 - val_loss: 0.3227 - val_accuracy: 0.8600 - val_binary_crossentropy: 0.3227 Epoch 9/20 49/49 - 0s - loss: 0.3442 - accuracy: 0.8605 - binary_crossentropy: 0.3442 - val_loss: 0.3226 - val_accuracy: 0.8618 - val_binary_crossentropy: 0.3226 Epoch 10/20 49/49 - 0s - loss: 0.3317 - accuracy: 0.8674 - binary_crossentropy: 0.3317 - val_loss: 0.3230 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3230 Epoch 11/20 49/49 - 0s - loss: 0.3267 - accuracy: 0.8691 - binary_crossentropy: 0.3267 - val_loss: 0.3247 - val_accuracy: 0.8604 - val_binary_crossentropy: 0.3247 Epoch 12/20 49/49 - 0s - loss: 0.3242 - accuracy: 0.8695 - binary_crossentropy: 0.3242 - val_loss: 0.3261 - val_accuracy: 0.8597 - val_binary_crossentropy: 0.3261 Epoch 13/20 49/49 - 0s - loss: 0.3153 - accuracy: 0.8721 - binary_crossentropy: 0.3153 - val_loss: 0.3289 - val_accuracy: 0.8586 - val_binary_crossentropy: 0.3289 Epoch 14/20 49/49 - 0s - loss: 0.3092 - accuracy: 0.8742 - binary_crossentropy: 0.3092 - val_loss: 0.3294 - val_accuracy: 0.8573 - val_binary_crossentropy: 0.3294 Epoch 15/20 49/49 - 0s - loss: 0.3103 - accuracy: 0.8772 - binary_crossentropy: 0.3103 - val_loss: 0.3312 - val_accuracy: 0.8576 - val_binary_crossentropy: 0.3312 Epoch 16/20 49/49 - 0s - loss: 0.3010 - accuracy: 0.8815 - binary_crossentropy: 0.3010 - val_loss: 0.3363 - val_accuracy: 0.8583 - val_binary_crossentropy: 0.3363 Epoch 17/20 49/49 - 0s - loss: 0.3010 - accuracy: 0.8788 - binary_crossentropy: 0.3010 - val_loss: 0.3338 - val_accuracy: 0.8570 - val_binary_crossentropy: 0.3338 Epoch 18/20 49/49 - 0s - loss: 0.2975 - accuracy: 0.8824 - binary_crossentropy: 0.2975 - val_loss: 0.3343 - val_accuracy: 0.8564 - val_binary_crossentropy: 0.3343 Epoch 19/20 49/49 - 0s - loss: 0.2923 - accuracy: 0.8823 - binary_crossentropy: 0.2923 - val_loss: 0.3417 - val_accuracy: 0.8556 - val_binary_crossentropy: 0.3417 Epoch 20/20 49/49 - 0s - loss: 0.2910 - accuracy: 0.8830 - binary_crossentropy: 0.2910 - val_loss: 0.3452 - val_accuracy: 0.8560 - val_binary_crossentropy: 0.3452ê²€ì¦ ê³ ê³  123plot_history([('base', base_history), ('dropout', dpt_history) ]) 1234plot_history([('base', base_history), ('dropout', dpt_history), ('L2', l2_history) ]) ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ê²°ë¡ 1. ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. 2. ë„¤íŠ¸ì›Œí¬ì˜ ìš©ëŸ‰ì„ ì¤„ì¸ë‹¤. (ex. Dense(16 ..) 3. ê°€ì¤‘ì¹˜ ê·œì œë¥¼ ì¶”ê°€í•œë‹¤. (L2) 4. ë“œë¡­ì•„ì›ƒì„ ì¶”ê°€í•œë‹¤.12","link":"/2020/08/25/%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9-Overfitting-%EA%B3%BC-%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9-Underfitting/"},{"title":"ë°ì´í„°í”„ë ˆì„ ë³‘í•©í•˜ê¸°(merge, concat)","text":"ë³µìˆ˜ì˜ ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ ë§Œë“œëŠ” ê³¼ì •ì„ ë§Œì ¸ë³´ì 12df1 = pd.DataFrame({'êµ­ì–´':[87, 69], 'ìˆ˜í•™':[77,96]}, index=['í™ê¸¸ë™', 'ì„êº½ì •'])df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ í™ê¸¸ë™ 87 77 ì„êº½ì • 69 96 12df2 = pd.DataFrame({'êµ­ì–´':[82,81], 'ì˜ì–´':[86,90]}, index=['ì „ë´‰ì¤€','ì¥ê¸¸ì‚°'])df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ì˜ì–´ ì „ë´‰ì¤€ 82 86 ì¥ê¸¸ì‚° 81 90 12df3 = pd.DataFrame({'êµ­ì–´':[82,81], 'ì˜ì–´':[86,90]}, index=['ì „ë´‰ì¤€','ì¥ê¸¸ì‚°'])df3 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ì˜ì–´ ì „ë´‰ì¤€ 82 86 ì¥ê¸¸ì‚° 81 90 concatí•¨ìˆ˜ì„¸ë¡œë¡œ ë³‘í•©í•˜ê¸°(ê·¸ëƒ¥ ì•„ë˜ë¡œ ë¶™ì´ëŠ”ê±°) ë°ì´í„°í”„ë ˆì„ë¼ë¦¬ ì»¬ëŸ¼ì´ ë‹¬ë¼ë„ ëœë‹¤. (ì—†ëŠ” ì»¬ëŸ¼ì—” NaNê°’ìœ¼ë¡œ ì•Œì•„ì„œ ì±„ì›Œì§) ê·¼ë°, ë°ì´í„°í”„ë ˆì„ê°„ì— ì»¬ëŸ¼ì´ ì„œë¡œ ë‹¤ë¥´ë©´ sort=Falseë¥¼ ë„£ì–´ì¤˜ì•¼ ê²½ê³ ê°€ ë°œìƒí•˜ì§€ ì•Šì•„ [sort íŒŒë¼ë¯¸í„°ëŠ” ì»¬ëŸ¼ ì´ë¦„ì„ ì •ë ¬í•´ì£¼ëŠ” ì˜µì…˜]12df_concat1 = pd.concat([df1, df2, df3], sort=False)df_concat1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ ì˜ì–´ í™ê¸¸ë™ 87 77.0 NaN ì„êº½ì • 69 96.0 NaN ì „ë´‰ì¤€ 82 NaN 86.0 ì¥ê¸¸ì‚° 81 NaN 90.0 ì „ë´‰ì¤€ 82 NaN 86.0 ì¥ê¸¸ì‚° 81 NaN 90.0 ê°€ë¡œë¡œ ë³‘í•©í•˜ê¸° (ìœ„ì— ì–¸ê¸‰í–ˆë“¯ì´ axis=1 ì˜µì…˜ë§Œ ì£¼ë©´ ë˜)12df_concat2 = pd.concat([df1, df2, df3], sort=False, axis=1)df_concat2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ êµ­ì–´ ì˜ì–´ êµ­ì–´ ì˜ì–´ í™ê¸¸ë™ 87.0 77.0 NaN NaN NaN NaN ì„êº½ì • 69.0 96.0 NaN NaN NaN NaN ì „ë´‰ì¤€ NaN NaN 82.0 86.0 82.0 86.0 ì¥ê¸¸ì‚° NaN NaN 81.0 90.0 81.0 90.0 ì‰½ì§€?? ë„˜ì–´ê°€ë„ë¡ í• ê²Œ 12 mergeí•¨ìˆ˜ê³µí†µ ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ê¸° ê°€ëŠ¥123pd.merge(df1, df2)df1ì´ë‘ df2ëŠ” ì„œë¡œ ê²¹ì¹˜ëŠ” ê²Œ ì—†ì–´ì„œ ì»¬ëŸ¼ë§Œ ë‚˜ì˜¤ë„¤ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ ì˜ì–´ 1234pd.merge(df1, df2, how='outer')ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì€ ê·¸ëƒ¥ NaNìœ¼ë¡œ ì±„ìš°ë©´ì„œ ì„œë¡œ ë³‘í•©í•˜ëŠ”ê±°ì•¼ë°©ë²•ì€ how = 'outer' íŒŒë¼ë¯¸í„°ë¥¼ ì“°ë©´ ë¼ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ ì˜ì–´ 0 87 77.0 NaN 1 69 96.0 NaN 2 82 NaN 86.0 3 81 NaN 90.0 how íŒŒë¼ë¯¸í„°ëŠ” outer, innerë§Œ ìˆëŠ”ê²Œ ì•„ë‹ˆì•¼. left, rightë„ ìˆì–´1234pd.merge(df1, df2, how='left')ì™¼ìª½êº¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë†“ê³ , ì˜¤ë¥¸ìª½ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í•©ë³‘ì‹œí‚¤ë˜, ì˜¤ë¥¸ìª½ì—ì„œ ê°€ì ¸ì˜¬ ë°ì´í„°ê°€ ì—†ëŠ” ìë¦¬ì—ëŠ” NaNê°’ìœ¼ë¡œ ì±„ì›Œ ë„£ì–´ì¤˜ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ ì˜ì–´ 0 87 77 NaN 1 69 96 NaN 123pd.merge(df1, df2, how='right')ë§ˆì°¬ê°€ì§€ë¡œ ì´ê±°ëŠ” ì˜¤ë¥¸ìª½ì„ ê¸°ì¤€ìœ¼ë¡œ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } êµ­ì–´ ìˆ˜í•™ ì˜ì–´ 0 82 NaN 86 1 81 NaN 90 merge í•¨ìˆ˜ì™€ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì˜ join ì´ë¼ëŠ” í•¨ìˆ˜ë„ ìˆì–´ê·¼ë° joiní•¨ìˆ˜ëŠ” mergeë‘ ì™„ì „ ê°™ì€ ê¸°ëŠ¥ì´ë¼ ë”°ë¡œ ì •ë¦¬ëŠ” ì•ˆí• ê±°ì•¼ì´ëŸ¬í•œ ê¸°ëŠ¥ì´ í•„ìš”í•  ë• mergeí•¨ìˆ˜ë¥¼ ì“°ë©´ ë˜ì§€ 12 12 ì¤‘ë³µë˜ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì˜ ì—´ë‹¨ìœ„ ë³‘í•©(merge) ì–´ë–»ê²Œë“  ì¡°í•©ì„ ë§Œë“¤ì–´ë‚´ë‹ˆê¹Œ ë„ˆë¬´ ê±±ì •ë§ˆ 12df_1 = pd.DataFrame({'ì•„ì´ë””':['a','b','c','d'], 'ê²°ì œê¸ˆì•¡':[1000, 1200, 1700, 3200]})df_1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì•„ì´ë”” ê²°ì œê¸ˆì•¡ 0 a 1000 1 b 1200 2 c 1700 3 d 3200 12df_2 = pd.DataFrame({'ì•„ì´ë””':['a','b','c','d'], 'ì ë¦½ê¸ˆ':[120, 1700, 200, 320]})df_2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì•„ì´ë”” ì ë¦½ê¸ˆ 0 a 120 1 b 1700 2 c 200 3 d 320 1pd.merge(df_1, df_2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì•„ì´ë”” ê²°ì œê¸ˆì•¡ ì ë¦½ê¸ˆ 0 a 1000 120 1 b 1200 1700 2 c 1700 200 3 d 3200 320 ì¼ë‹¨ ê¸°ë³¸ mergeë¥¼ ì‹œì¼œì„œ ëª¨ë“  ê²½ìš°ì˜ìˆ˜ë¥¼ ë‹¤ ë´ê·¸ë¦¬ê³  ë‚˜ì„œ ìˆ˜ì •ì„ í•˜ë“ ë§ë“  ì˜¤í‚¤? ê²¹ì¹˜ëŠ” ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì¸ ê²½ìš°ì—ëŠ”?ê°„ë‹¨í•´. ì´ëŸ´ë•ŒëŠ” â€˜onâ€™ ì´ë¼ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ onê°’ì„ ê¸°ì¤€ìœ¼ë¡œ mergeê°€ ë˜ 1234df_a = pd.DataFrame({'ê³ ê°ëª…':['ê¸¸ë™','ê¸¸ì‚°'], 'ë°ì´í„°':['2000','1700'], 'ë‚ ì§œ':['2020-05-08', '2020-06-08']})df_a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„° ë‚ ì§œ 0 ê¸¸ë™ 2000 2020-05-08 1 ê¸¸ì‚° 1700 2020-06-08 1234df_b = pd.DataFrame({'ê³ ê°ëª…':['ê¸¸ë™','ê¸¸ì‚°'], 'ë°ì´í„°':['21ì„¸','17ì„¸'], })df_b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„° 0 ê¸¸ë™ 21ì„¸ 1 ê¸¸ì‚° 17ì„¸ 1pd.merge(df_a, df_b) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„° ë‚ ì§œ mergeê°€ ë˜ì§€ ì•Šì•„. ì™œëƒë¯„ ê³µí†µë˜ëŠ” ì»¬ëŸ¼ì´ 2ê°œê°€ ë˜ë‹ˆê¹ ë‘˜ ë‹¤ ì°¸ì¡°ë¥¼ í•˜ê²Œë˜ì„œ.. ë‘ ì»¬ëŸ¼ ì‚¬ì´ì— êµì§‘í•©ì´ ì—†ìœ¼ë‹ˆ mergeë¥¼ í•´ë„ ì†Œìš©ì´ ì—†ëŠ”ê±°ì•¼ 1pd.merge(df_a, df_b, on='ê³ ê°ëª…') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„°_x ë‚ ì§œ ë°ì´í„°_y 0 ê¸¸ë™ 2000 2020-05-08 21ì„¸ 1 ê¸¸ì‚° 1700 2020-06-08 17ì„¸ ì on íŒŒë¼ë¯¸í„°ë¡œ ê¸°ì¤€ì„ ê³ ê°ëª…ìœ¼ë¡œ ì¤¬ë”ë‹ˆ ì¶œë ¥ì´ ëì–´.ì´ê±°ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´, ê¸°ì¤€ì—´ì€ ì•„ë‹ˆë©´ì„œ ì´ë¦„ì´ ê°™ì€ ì—´ì¸ ê²½ìš°ì—ëŠ” _x ì™€ _yê°€ ë¶™ì–´ 12 _x ë‘ _yê°€ ì¢€ ê·¸ë ‡ì§€? ê·¸ëŸ¬ë©´ ì»¬ëŸ¼ëª…ì„ ë³€ê²½ì„ í•˜ì 123xx = pd.merge(df_a, df_b, on='ê³ ê°ëª…')xx = xx.rename(columns={'ë°ì´í„°_x': 'ê¸ˆì•¡', 'ë°ì´í„°_y':'ë‚˜ì´'})xx .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ê¸ˆì•¡ ë‚ ì§œ ë‚˜ì´ 0 ê¸¸ë™ 2000 2020-05-08 21ì„¸ 1 ê¸¸ì‚° 1700 2020-06-08 17ì„¸ Good Job! 12 ê³µí†µ ì»¬ëŸ¼ì€ ì¡´ì¬í•˜ì§€ ì•Šì§€ë§Œ, ìš°ë¦¬ê°€ ì´ë¦„ì´ ë‹¤ë¥¸ ë‘ ì»¬ëŸ¼ì„ ì§€ì •í•´ì„œ mergeí•˜ë¼ê³  ëª…ë ¹ í• ìˆ˜ë„ ìˆì–´1df_a .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„° ë‚ ì§œ 0 ê¸¸ë™ 2000 2020-05-08 1 ê¸¸ì‚° 1700 2020-06-08 12df_b = df_b.rename(columns={'ê³ ê°ëª…':'ì´ë¦„'})df_b .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì´ë¦„ ë°ì´í„° 0 ê¸¸ë™ 21ì„¸ 1 ê¸¸ì‚° 17ì„¸ ì í•´ë³´ì 12 12merged = pd.merge(df_a, df_b, left_on='ê³ ê°ëª…', right_on='ì´ë¦„')merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„°_x ë‚ ì§œ ì´ë¦„ ë°ì´í„°_y 0 ê¸¸ë™ 2000 2020-05-08 ê¸¸ë™ 21ì„¸ 1 ê¸¸ì‚° 1700 2020-06-08 ê¸¸ì‚° 17ì„¸ ì™¼ìª½ì˜ ê³ ê°ëª…ê³¼ ì˜¤ë¥¸ìª½ì˜ ì´ë¦„ì´ ê°™ì€ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ë¼ëŠ” ì˜ë¯¸ë„¤. ã…‡ã…‹ ì´ë ‡ê²Œ ë˜ëŠ”êµ¬ë‚˜. ê·¸ëŸ¼ ì´ì œ ê²¹ì¹˜ëŠ” ì»¬ëŸ¼ì„ ì§€ìš°ì12merged.drop('ì´ë¦„',axis=1, inplace=True)merged .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ê³ ê°ëª… ë°ì´í„°_x ë‚ ì§œ ë°ì´í„°_y 0 ê¸¸ë™ 2000 2020-05-08 21ì„¸ 1 ê¸¸ì‚° 1700 2020-06-08 17ì„¸ 12 ??? : mergeëŠ” í•­ìƒ ì»¬ëŸ¼ ê¸°ì¤€ì´ì•¼?? ì˜¤~ ë„ˆ ì°¸ ë˜‘ë˜‘ì´êµ¬ë‚˜? 12 Indexë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¹ì—°íˆ mergeê°€ ë˜ left_index, right_index íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•˜ë©´ë˜ëŠ”ë°,, ì¼ë‹¨ í•´ë³´ìê³  12 12ì—­ì‚¬ = pd.DataFrame({'ì—­ì‚¬':[90,82]}, index=['í•„êµ¬','ë´‰êµ¬'])ì—­ì‚¬ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ í•„êµ¬ 90 ë´‰êµ¬ 82 12ìˆ˜í•™ = pd.DataFrame({'ìˆ˜í•™':[81,92]}, index=['í•„êµ¬','ë§¹êµ¬'])ìˆ˜í•™ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ìˆ˜í•™ í•„êµ¬ 81 ë§¹êµ¬ 92 12ì—­ì‚¬ìˆ˜í•™ = pd.merge(ì—­ì‚¬, ìˆ˜í•™, left_index=True, right_index=True)ì—­ì‚¬ìˆ˜í•™ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ ìˆ˜í•™ í•„êµ¬ 90 81 ë‘ ë°ì´í„°ì—ì„œ ê²¹ì¹˜ëŠ” indexì¸ í•„êµ¬ë§Œ ì¡ì•„ì„œ ì¶”ê°€í•´ì£¼ë„¤ ã…‡ã…‹? 12ì—­ì‚¬ìˆ˜í•™ = pd.merge(ì—­ì‚¬, ìˆ˜í•™, left_index=True, right_index=True, how='outer')ì—­ì‚¬ìˆ˜í•™ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ ìˆ˜í•™ ë§¹êµ¬ NaN 92.0 ë´‰êµ¬ 82.0 NaN í•„êµ¬ 90.0 81.0 12 ê·¸ëŸ¬ë©´ ì´ëŸ° ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í•˜ëŠ” ê±°ì•¼? 12ì—­ì‚¬ = pd.DataFrame({'ì—­ì‚¬':[90,82]}, index=['í•„êµ¬','ë´‰êµ¬'])ì—­ì‚¬ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ í•„êµ¬ 90 ë´‰êµ¬ 82 12ìˆ˜í•™ = pd.DataFrame({'ìˆ˜í•™':[81,92],'ì´ë¦„':['í•„êµ¬','ë´‰êµ¬']})ìˆ˜í•™ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ìˆ˜í•™ ì´ë¦„ 0 81 í•„êµ¬ 1 92 ë´‰êµ¬ ì´ ë‘ê°œë¥¼ mergeí•˜ê³  ì‹¶ì€ë°,ì„œë¡œ ì°¸ì¡°ì‹œí‚¬ ê²ƒì´ í•˜ë‚˜ëŠ” indexì— ìˆê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” columnìœ¼ë¡œ ê°–ê³  ìˆì–ì•„.. ì´ëŸ´ë•Œ ë°”ë¡œ left/right_indexì™€ left/right_onì„ ì‘ìš©í•´ì„œ ì‰í‚·ì‰í‚·! 12 12ì—­ì‚¬ìˆ˜í•™_ = pd.merge(ì—­ì‚¬,ìˆ˜í•™, left_index=True, right_on='ì´ë¦„')ì—­ì‚¬ìˆ˜í•™_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ ìˆ˜í•™ ì´ë¦„ 0 90 81 í•„êµ¬ 1 82 92 ë´‰êµ¬ ì •ë¦¬ë§Œ í•´ì£¼ê³  ëë‚´ì í˜ë“¤ë‹¤!123ì—­ì‚¬ìˆ˜í•™_.index = ì—­ì‚¬ìˆ˜í•™_['ì´ë¦„'].valuesì—­ì‚¬ìˆ˜í•™_.drop('ì´ë¦„', axis=1, inplace=True)ì—­ì‚¬ìˆ˜í•™_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ì—­ì‚¬ ìˆ˜í•™ í•„êµ¬ 90 81 ë´‰êµ¬ 82 92","link":"/2020/05/08/%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%EB%B3%91%ED%95%A9%ED%95%98%EA%B8%B0-merge-concat/"},{"title":"ë¹„ì§€ë„í•™ìŠµ : PCA ì£¼ì„±ë¶„ ë¶„ì„","text":"ë¹„ì§€ë„ í•™ìŠµì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” ì´ìœ ëŠ” ì—¬ëŸ¬ê°€ì§€ì´ë‹¤. ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œëŠ” ë°ì´í„°ë¥¼ ì‹œê°í™” ì••ì¶•, ì¶”ê°€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì •ë³´ê°€ ë” ì˜ ë“œëŸ¬ë‚˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ì£¼ì„±ë¶„ ë¶„ì„(PCA) PCAì˜ ë³¸ì§ˆì€ íƒìƒ‰ì  ë¶„ì„ì´ë‹¤. ì¦‰, ë³€ì¸ì„ íƒìƒ‰í•´ì„œ ë³€í™˜ì„ í†µí•´ ì£¼ì„±ë¶„ì„ ê²°ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì£¼ì„±ë¶„ì´ë€ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” íŠ¹ì„± ì¤‘ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” íŠ¹ì„±ì„ ë§í•œë‹¤. ë°ì´í„°ì˜ íŠ¹ì„±ì´ ë§ì„ë•Œ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ì¼ë¶€ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì„¤ëª… ë˜ëŠ” ëª¨ë¸ë§í•˜ê³ ì í• ë•Œ PCAë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ ì£¼ì„±ë¶„ì„ ì•Œì•„ë³´ëŠ” ê²ƒ ì™¸ì— ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. 12import mglearnmglearn.plots.plot_pca_illustration() ì™¼ìª½ ìœ„ ê·¸ë˜í”„ì—ì„œ ë¶„ì‚°ì´ ê°€ì¥ í° ë°©í–¥ì„ ì°¾ëŠ”ë‹¤. ë°”ë¡œ ì„±ë¶„ 1ì´ë‹¤. ì´ ë°©í–¥ì€ ë°ì´í„°ì—ì„œ ê°€ì¥ ë§ì€ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤. ì¦‰, íŠ¹ì„±ë“¤ì˜ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ í° ë°©í–¥ì´ë‹¤. ê·¸ í›„ ì²« ë²ˆì§¸ ë°©í–¥ê³¼ ì§ê°„ì¸ ë°©í–¥ ì¤‘ì—ì„œ ê°€ì¥ ë§ì€ ì •ë³´ë¥¼ ë‹´ì€ ë°©í–¥ì„ ì°¾ëŠ”ë‹¤. í•´ë‹¹ ê·¸ë˜í”„ì—ì„œ ì£¼ì„±ë¶„ì„ í•˜ë‚˜ë¡œ ì„ ì •í•œë‹¤ë©´ 2ë²ˆì§¸ ì„±ë¶„ì€ ì œê±°ëœë‹¤. ê·¸ë¦¬ê³  ë‹¤ì‹œ ì›ë˜ ëª¨ì–‘ìœ¼ë¡œ íšŒì „ì‹œí‚¨ë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ ë³€í™˜ì€ ë°ì´í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê±°ë‚˜ ì£¼ì„±ë¶„ì—ì„œ ìœ ì§€ë˜ëŠ” ì •ë³´ë¥¼ ì‹œê°í™”ëŠ”ë° ì¢…ì¢… ì‚¬ìš©ëœë‹¤. ì˜ˆì‹œ)íŒë‹¤ìŠ¤ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ë¶“ê½ƒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. ê·¸ í›„ ê° íŠ¹ì„±ì˜ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§í•œ í›„ ì£¼ì„±ë¶„ ë¶„ì„ì„ ì‹¤ì‹œí•˜ì˜€ë‹¤ 12iris_df = sns.load_dataset('iris')iris_df.tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica 12345678from sklearn.preprocessing import StandardScaler # í‘œì¤€í™” íŒ¨í‚¤ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ x = df.drop(['target'], axis=1).values # ë…ë¦½ë³€ì¸ë“¤ì˜ valueê°’ë§Œ ì¶”ì¶œy = df['target'].values # ì¢…ì†ë³€ì¸ ì¶”ì¶œx = StandardScaler().fit_transform(x) # xê°ì²´ì— xë¥¼ í‘œì¤€í™”í•œ ë°ì´í„°ë¥¼ ì €ì¥features = ['sepal length', 'sepal width', 'petal length', 'petal width']pd.DataFrame(x, columns=features).head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length sepal width petal length petal width 0 -0.900681 1.032057 -1.341272 -1.312977 1 -1.143017 -0.124958 -1.341272 -1.312977 2 -1.385353 0.337848 -1.398138 -1.312977 3 -1.506521 0.106445 -1.284407 -1.312977 4 -1.021849 1.263460 -1.341272 -1.312977 123456from sklearn.decomposition import PCApca = PCA(n_components=2) # ì£¼ì„±ë¶„ì„ ëª‡ê°œë¡œ í• ì§€ ê²°ì •printcipalComponents = pca.fit_transform(x)principalDf = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2'])# ì£¼ì„±ë¶„ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° í”„ë ˆì„ êµ¬ì„±principalDf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } principal component1 principal component2 0 -2.264542 0.505704 1 -2.086426 -0.655405 2 -2.367950 -0.318477 3 -2.304197 -0.575368 4 -2.388777 0.674767 ... ... ... 145 1.870522 0.382822 146 1.558492 -0.905314 147 1.520845 0.266795 148 1.376391 1.016362 149 0.959299 -0.022284 150 rows Ã— 2 columns 1pca.explained_variance_ratio_ array([0.72770452, 0.23030523])ì£¼ì„±ë¶„ì„ 2ê°œë¡œ í–ˆì„ë•ŒëŠ” ì²«ë²ˆì§¸ ì„±ë¶„ì´ ë°ì´í„°ë¥¼ 72%ë¥¼ ì„¤ëª…í•œë‹¤. 1pca.components_ array([[ 0.52237162, -0.26335492, 0.58125401, 0.56561105], [ 0.37231836, 0.92555649, 0.02109478, 0.06541577]])ìœ„ì˜ ì£¼ì„±ë¶„ 2ê°œì—ì„œ ê° ë³€ìˆ˜ ì¤‘ìš”ë„ 1234567891011121314151617finalDf = pd.concat([principalDf, df[['target']]], axis = 1)fig = plt.figure(figsize = (8, 8))ax = fig.add_subplot(1, 1, 1)ax.set_xlabel('Principal Component 1', fontsize = 15)ax.set_ylabel('Principal Component 2', fontsize = 15)ax.set_title('2 component PCA', fontsize=20)targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']colors = ['r', 'g', 'b']for target, color in zip(targets,colors): indicesToKeep = finalDf['target'] == target ax.scatter(finalDf.loc[indicesToKeep, 'principal component1'] , finalDf.loc[indicesToKeep, 'principal component2'] , c = color , s = 50)ax.legend(targets)ax.grid() 12 12 12 12 12","link":"/2020/06/23/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5-PCA-%EC%A3%BC%EC%84%B1%EB%B6%84-%EB%B6%84%EC%84%9D/"},{"title":"íŒŒì´ì¬ì—ì„œ ì„¤ì¹˜ë˜ì–´ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ì²´í¬í•˜ê¸°","text":"íŒŒì´ì¬ì„ ì‚¬ìš©í•˜ë‹¤ ë³´ë©´ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ë”°ë¼ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ëª…ë ¹ì–´ê°€ ë‹¤ë¥¸ ê²½ìš°ê°€ ìˆì–´. ê·¸ëŸ´ ê²½ìš° ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë²„ì „ì„ ì•Œì•„ì•¼ ê±°ê¸°ì— ë§ê²Œ ì§„í–‰ì„ í•  ìˆ˜ ìˆê² ì§€? ì´ë•Œ, ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì˜ ë²„ì „ì´ ë¬´ì—‡ì¸ì§€ë¶€í„° ë¨¼ì € í™•ì¸í•´ë³´ì! 1pip freeze anaconda-client==1.7.2 anaconda-navigator==1.9.12 appnope==0.1.0 asn1crypto==1.3.0 attrs==19.3.0 autopep8==1.5.2 autopy==4.0.0 backcall==0.1.0 backports.functools-lru-cache==1.6.1 backports.tempfile==1.0 backports.weakref==1.0.post1 beautifulsoup4==4.6.0 bleach==3.1.0 boto==2.49.0 boto3==1.13.12 botocore==1.16.12 branca==0.4.0 bs4==0.0.1 cachetools==4.0.0 certifi==2020.4.5.1 cffi==1.14.0 chardet==3.0.4 chart-studio==1.0.0 click==7.1.1 cloudpickle==1.3.0 clyent==1.2.2 colorama==0.4.3 colorlover==0.3.0 conda==4.8.3 conda-build==3.18.9 conda-package-handling==1.6.0 conda-verify==3.4.2 configobj==5.0.6 cryptography==2.8 cufflinks==0.17.3 cvxpy==1.0.31 cycler==0.10.0 cytoolz==0.10.1 dask==2.14.0 decorator==4.4.2 defusedxml==0.6.0 dill==0.3.1.1 docutils==0.15.2 dpkt==1.9.2 ecos==2.0.7.post1 entrypoints==0.3 et-xmlfile==1.0.1 filelock==3.0.12 finance-datareader==0.9.6 folium==0.10.1 funcy==1.14 future==0.18.2 gensim==3.8.3 glob2==0.7 google-api-core==1.16.0 google-auth==1.12.0 google-auth-oauthlib==0.4.1 google-cloud-bigquery==1.24.0 google-cloud-core==1.3.0 google-resumable-media==0.5.0 googleapis-common-protos==1.51.0 idna==2.9 imageio==2.8.0 importlib-metadata==1.5.0 inflect==4.1.0 ipykernel==5.1.4 ipython==7.13.0 ipython-genutils==0.2.0 ipywidgets==7.5.1 jaraco.itertools==5.0.0 jdcal==1.4.1 jedi==0.16.0 Jinja2==2.11.1 jmespath==0.10.0 joblib==0.14.1 JPype1==0.7.5 jsonschema==3.2.0 jupyter-client==6.1.2 jupyter-core==4.6.3 jupyterthemes==0.20.0 kiwisolver==1.2.0 konlpy==0.5.2 lesscpy==0.14.0 libarchive-c==2.8 lief==0.9.0 lxml==4.5.0 MarkupSafe==1.1.1 matplotlib==3.2.1 missingno==0.4.2 mistune==0.8.4 mkl-fft==1.0.15 mkl-random==1.1.0 mkl-service==2.3.0 more-itertools==8.2.0 mpmath==1.1.0 multiprocess==0.70.9 navigator-updater==0.2.1 nbconvert==5.6.1 nbformat==5.0.4 netifaces==0.10.9 networkx==2.4 nltk==3.5 notebook==6.0.3 numexpr==2.7.1 numpy==1.18.1 oauthlib==3.1.0 olefile==0.46 opencv-python==4.2.0.34 openpyxl==3.0.3 osqp==0.6.1 packaging==20.3 pandas==1.0.3 pandas-gbq==0.13.1 pandocfilters==1.4.2 parso==0.6.2 patsy==0.5.1 pexpect==4.8.0 pgmpy==0.1.10 picklable-itertools==0.1.1 pickleshare==0.7.5 Pillow==7.0.0 pkginfo==1.5.0.1 plotly==4.5.0 pluggy==0.13.1 ply==3.11 prometheus-client==0.7.1 prompt-toolkit==3.0.4 protobuf==3.11.3 psutil==5.7.0 ptyprocess==0.6.0 py==1.8.1 pyasn1==0.4.8 pyasn1-modules==0.2.8 pycodestyle==2.5.0 pycosat==0.6.3 pycparser==2.20 pydata-google-auth==0.3.0 Pygments==2.6.1 pyLDAvis==2.1.2 PyMySQL==0.9.3 pyOpenSSL==19.1.0 pyparsing==2.4.7 pyrsistent==0.16.0 PySocks==1.7.1 pytest==5.4.2 python-dateutil==2.8.1 python-xlib==0.27 pytz==2019.3 PyWavelets==1.1.1 PyYAML==5.3.1 pyzmq==18.1.1 QtPy==1.9.0 regex==2020.5.14 requests==2.23.0 requests-file==1.4.3 requests-oauthlib==1.3.0 retrying==1.3.3 rsa==4.0 ruamel-yaml==0.15.87 s3transfer==0.3.3 schedule==0.6.0 scikit-image==0.16.2 scikit-learn==0.22.2.post1 scipy==1.4.1 scs==2.1.2 seaborn==0.10.0 selenium==3.141.0 Send2Trash==1.5.0 six==1.14.0 sklearn==0.0 smart-open==2.0.0 soupsieve==2.0 soynlp==0.0.493 SQLAlchemy==1.3.16 statsmodels==0.11.0 sympy==1.5.1 terminado==0.8.3 testpath==0.4.4 toolz==0.10.0 torch==1.5.0 tornado==6.0.4 tqdm==4.44.1 traitlets==4.3.3 tweepy==3.8.0 urllib3==1.25.8 wcwidth==0.1.9 webencodings==0.5.1 wget==3.2 widgetsnbextension==3.5.1 wordcloud==1.7.0 xlrd==1.2.0 xmltodict==0.12.0 zipp==2.2.0 Note: you may need to restart the kernel to use updated packages.12 12 12 12 12 12 ì–´ë•Œ? ì‰½ì§€? ë¬¼ë¡  íŠ¹ì • ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¡ì•„ì„œ í•˜ëŠ” ë°©ë²•ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ ì„¤ì¹˜ëœ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë²„ì „ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ì•¼. íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¢…ë¥˜ê°€ ë§¤ìš° ë§ì€ë° ì´ ëª…ë ¹ì–´ í•˜ë‚˜ë¡œ ëª¨ë“  ì„¤ì¹˜ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë²„ì „ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ ìƒë‹¹íˆ í¸ë¦¬í•œ ê²ƒ ê°™ë„¤.","link":"/2020/05/19/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-%EC%84%A4%EC%B9%98%EB%90%98%EC%96%B4%EC%9E%88%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%B2%84%EC%A0%84-%EC%B2%B4%ED%81%AC%ED%95%98%EA%B8%B0/"}],"tags":[{"name":"git","slug":"git","link":"/tags/git/"},{"name":"fastcampus","slug":"fastcampus","link":"/tags/fastcampus/"},{"name":"python","slug":"python","link":"/tags/python/"}],"categories":[]}