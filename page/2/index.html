<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Geony&#039;s Tech Blog</title><meta description="Data Analyst&amp;#39;s blog"><meta property="og:type" content="blog"><meta property="og:title" content="Geony&#039;s Tech Blog"><meta property="og:url" content="http://wglee87.github.io/"><meta property="og:site_name" content="Geony&#039;s Tech Blog"><meta property="og:description" content="Data Analyst&amp;#39;s blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://wglee87.github.io/img/og_image.png"><meta property="article:author" content="wglee87"><meta property="article:tag" content="data_analysis"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wglee87.github.io"},"headline":"Geony's Tech Blog","image":["http://wglee87.github.io/img/og_image.png"],"author":{"@type":"Person","name":"WGLee87"},"description":"Data Analyst&#39;s blog"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/" alt="Geony&#039;s Tech Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="/null">Download on GitHub</a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-13T08:39:13.000Z" title="2020-06-13T08:39:13.000Z">2020-06-13</time><span class="level-item">11 minutes read (About 1611 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/13/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-Data-Scaling-with-sklearn/">데이터 전처리(Data Scaling with sklearn)</a></h1><div class="content"><pre><code>데이터 스케일링이란 데이터 전처리 과정의 하나입니다.

데이터 스케일링을 해주는 이유는 데이터의 값이 너무 크거나 혹은 작은 경우에 모델 알고리즘 학습과정에서 0으로 수렴하거나 무한으로 발산해버릴 수 있기 때문입니다.

따라서, scaling은 데이터 전처리 과정에서 굉장히 중요한 과정입니다.

가볍게 살펴보도록 하겠습니다</code></pre><h5 id="1-scale이란"><a href="#1-scale이란" class="headerlink" title="1. scale이란?"></a>1. scale이란?</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install mglearn</span><br></pre></td></tr></table></figure>

<pre><code>Collecting mglearn
  Downloading mglearn-0.1.9.tar.gz (540 kB)
[K     |████████████████████████████████| 540 kB 506 kB/s eta 0:00:01
[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.18.1)
Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (3.2.1)
Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.22.2.post1)
Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (1.0.3)
Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (7.0.0)
Requirement already satisfied: cycler in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.10.0)
Requirement already satisfied: imageio in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (2.8.0)
Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from mglearn) (0.14.1)
Requirement already satisfied: python-dateutil&gt;=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.8.1)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (1.2.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib-&gt;mglearn) (2.4.7)
Requirement already satisfied: scipy&gt;=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn-&gt;mglearn) (1.4.1)
Requirement already satisfied: pytz&gt;=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas-&gt;mglearn) (2019.3)
Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from cycler-&gt;mglearn) (1.14.0)
Building wheels for collected packages: mglearn
  Building wheel for mglearn (setup.py) ... [?25ldone
[?25h  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=6bf4e55bc798dd18a2ce5a5d67e6f134dfc35d54bb51cd8020f85b838a7173b1
  Stored in directory: /Users/wglee/Library/Caches/pip/wheels/f1/17/e1/1720d6dcd70187b6b6c3750cb3508798f2b1d57c9d3214b08b
Successfully built mglearn
Installing collected packages: mglearn
Successfully installed mglearn-0.1.9</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line">mglearn.plots.plot_scaling()</span><br></pre></td></tr></table></figure>


<img width="854" alt="output_3_0" src="https://user-images.githubusercontent.com/59719711/84565399-d602be80-ada3-11ea-95bc-d17cdde6b4a3.png">


<pre><code>(1) StandardScaler
각 feature의 평균을 0, 분산을 1로 변경합니다. 모든 특성들이 같은 스케일을 갖게 됩니다.


(2) RobustScaler
모든 특성들이 같은 크기를 갖는다는 점에서 StandardScaler와 비슷하지만, 평균과 분산 대신 median과 quartile을 사용합니다. RobustScaler는 이상치에 영향을 받지 않습니다.


(3) MinMaxScaler
모든 feature가 0과 1사이에 위치하게 만듭니다. 데이터가 2차원 셋일 경우, 모든 데이터는 x축의 0과 1 사이에, y축의 0과 1사이에 위치하게 됩니다.


(4) Normalizer
StandardScaler, RobustScaler, MinMaxScaler가 각 columns의 통계치를 이용한다면 Normalizer는 row마다 각각 정규화됩니다. Normalizer는 유클리드 거리가 1이 되도록 데이터를 조정합니다. (유클리드 거리는 두 점 사이의 거리를 계산할 때 쓰는 방법)</code></pre><h5 id="2-Code"><a href="#2-Code" class="headerlink" title="2. Code"></a>2. Code</h5><pre><code>scikit-learn에 있는 아이리스 데이터셋으로 데이터 스케일링을 해보겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>데이터를 학습용과 테스트용으로 분할했습니다.

scaler를 사용하기 이전에 주의해야될 점을 먼저 살펴보겠습니다.

scaler는 fit과 transform 메서드를 지니고 있습니다. fit 메서드로 데이터 변환을 학습하고, transform 메서드로 실제 데이터의 스케일을 조정합니다. 

이때, fit 메서드는 학습용 데이터에만 적용해야 합니다. 그 후, transform 메서드를 학습용 데이터와 테스트 데이터에 적용합니다. scaler는 fit_transform()이란 단축 메서드를 제공합니다. 학습용 데이터에는 fit_transform()메서드를 적용하고, 테스트 데이터에는 transform()메서드를 적용합니다.</code></pre><h6 id="1-StandardScaler-code"><a href="#1-StandardScaler-code" class="headerlink" title="(1) StandardScaler code"></a>(1) StandardScaler code</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scale = scaler.fit_transform(X_train)</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Min value : &#123;&#125;'</span>.format(X_train.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Max value : &#123;&#125;'</span>.format(X_train.max(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Min value : &#123;&#125;'</span>.format(X_train_scale.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Max value : &#123;&#125;'</span>.format(X_train_scale.max(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>스케일 조정 전  features Min value : [4.3 2.2 1.1 0.1]
스케일 조정 전  features Max value : [7.9 4.4 6.9 2.5]
스케일 조정 후  features Min value : [-1.73905934 -2.11220356 -1.37231262 -1.32054283]
스케일 조정 후  features Max value : [2.32920943 3.06374081 1.74766642 1.68511841]</code></pre><h6 id="2-RobustScaler-code"><a href="#2-RobustScaler-code" class="headerlink" title="(2) RobustScaler code"></a>(2) RobustScaler code</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line">scaler = RobustScaler()</span><br><span class="line">X_train_scale = scaler.fit_transform(X_train)</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Min value : &#123;&#125;'</span>.format(X_train.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Max value : &#123;&#125;'</span>.format(X_train.max(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Min value : &#123;&#125;'</span>.format(X_train_scale.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Max value : &#123;&#125;'</span>.format(X_train_scale.max(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>스케일 조정 전  features Min value : [4.3 2.2 1.1 0.1]
스케일 조정 전  features Max value : [7.9 4.4 6.9 2.5]
스케일 조정 후  features Min value : [-1.01818182 -1.47826087 -0.82993197 -0.70588235]
스케일 조정 후  features Max value : [1.6        2.34782609 0.74829932 0.70588235]</code></pre><h6 id="3-MinMaxScaler-code"><a href="#3-MinMaxScaler-code" class="headerlink" title="(3) MinMaxScaler code"></a>(3) MinMaxScaler code</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X_train_scale = scaler.fit_transform(X_train)</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Min value : &#123;&#125;'</span>.format(X_train.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Max value : &#123;&#125;'</span>.format(X_train.max(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Min value : &#123;&#125;'</span>.format(X_train_scale.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Max value : &#123;&#125;'</span>.format(X_train_scale.max(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>스케일 조정 전  features Min value : [4.3 2.2 1.1 0.1]
스케일 조정 전  features Max value : [7.9 4.4 6.9 2.5]
스케일 조정 후  features Min value : [0. 0. 0. 0.]
스케일 조정 후  features Max value : [1. 1. 1. 1.]</code></pre><h6 id="4-Normalizer-code"><a href="#4-Normalizer-code" class="headerlink" title="(4) Normalizer code"></a>(4) Normalizer code</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer </span><br><span class="line">scaler = Normalizer()</span><br><span class="line">X_train_scale = scaler.fit_transform(X_train)</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Min value : &#123;&#125;'</span>.format(X_train.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 전  features Max value : &#123;&#125;'</span>.format(X_train.max(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Min value : &#123;&#125;'</span>.format(X_train_scale.min(axis=<span class="number">0</span>)))</span><br><span class="line">print(<span class="string">'스케일 조정 후  features Max value : &#123;&#125;'</span>.format(X_train_scale.max(axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>스케일 조정 전  features Min value : [4.3 2.2 1.1 0.1]
스케일 조정 전  features Max value : [7.9 4.4 6.9 2.5]
스케일 조정 후  features Min value : [0.67017484 0.2383917  0.16783627 0.0147266 ]
스케일 조정 후  features Max value : [0.86093857 0.60379053 0.63265489 0.2553047 ]</code></pre><h5 id="3-적용해보기"><a href="#3-적용해보기" class="headerlink" title="3. 적용해보기"></a>3. 적용해보기</h5><pre><code>의사결정나무(decisionTree)로 breat_cancer 데이터셋을 학습해보겠습니다.
먼저, 데이터 스케일링을 적용하지 않은 채 진행하겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line">tree = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=<span class="number">1</span>)</span><br><span class="line">tree.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">'test accuracy : %3f'</span> %tree.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>test accuracy : 0.881119


다음은 데이터를 MinMaxScaler로 스케일을 조정하고 의사결정나무 모델로 학습시켜보겠습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X_train_scale = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scale = scaler.fit_transform(X_test)</span><br><span class="line">tree.fit(X_train_scale, y_train)</span><br><span class="line">print(<span class="string">'test accuracy : %3f'</span> %tree.score(X_test_scale, y_test))</span><br></pre></td></tr></table></figure>

<pre><code>test accuracy : 0.860140


비슷하게 나온 것을 알 수 있습니다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-09T10:02:14.000Z" title="2020-06-09T10:02:14.000Z">2020-06-09</time><span class="level-item">a few seconds read (About 62 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/09/Mysql-Database-%EC%9A%A9%EB%9F%89-%ED%99%95%EC%9D%B8/">[Mysql]Database 용량 확인</a></h1><div class="content"><p>[전체 데이터베이스 용량 확인]<br>select table_schema ‘Linear_Regression’, sum(data_length + index_length) / 1024 / 1024 ‘size(MB)’ from information_schema.tables group by table_schema</p>
<p>[특정 DB명 status 확인]<br>show table status like ‘DB명’</p>
<p>[특정 DB 용량 늘리기]<br>alter table ‘DB명’ max_rows = 400000000 avg_row_length=1500</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-04T11:22:46.000Z" title="2020-06-04T11:22:46.000Z">2020-06-04</time><span class="level-item">3 minutes read (About 403 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/04/DataFrame-Functions/">DataFrame Functions</a></h1><div class="content"><h6 id="데이터-분포-변환"><a href="#데이터-분포-변환" class="headerlink" title="데이터 분포 변환"></a>데이터 분포 변환</h6><pre><code>대부분의 모델은 변수가 특정 분포를 따른다는 가정을 기반으로 한다. 예를 들어 선형 모델의 경우, 종속변수가 정규분포와 유사할 경우 성능이 높아지는 것으로 알려져 있다. 자주 쓰이는 방법은 Log, Exp, Sqrt 등 함수를 이용해 데이터 분포를 변환하는 것이다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 특정 변수에만 함수 적용</span></span><br><span class="line">df[<span class="string">'X_log'</span>] = preprocessing.scale(np.log(df[<span class="string">'X'</span>]+<span class="number">1</span>)) <span class="comment"># 로그</span></span><br><span class="line">df[<span class="string">'X_sqrt'</span>] = preprocessing.scale(np.sqrt(df[<span class="string">'X'</span>]+<span class="number">1</span>)) <span class="comment"># 제곱근</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 프레임 전체에 함수 적용 (단, 숫자형 변수만 있어야 함)</span></span><br><span class="line">df_log = df.apply(<span class="keyword">lambda</span> x: np.log(x+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h6 id="중복된-행-제거"><a href="#중복된-행-제거" class="headerlink" title="중복된 행 제거"></a>중복된 행 제거</h6><pre><code>위, 아래 행이 모두 같은 성분을 가지는 행이 여러개 있을때, 하나만 사용하기(데이터프레임)
중복된 행이 제거되고 unique한 값만 가져올 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.drop_duplicates()</span><br><span class="line"></span><br><span class="line">df.drop_duplicated() 는 boolean값으로 반환!</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'name'</span>] = df[<span class="string">'name'</span>].apply(<span class="keyword">lambda</span> e: e.split()[<span class="number">0</span>])</span><br><span class="line">df[<span class="string">'email'</span>].str.get(i=<span class="number">0</span>)  데이터프레임이나 시리즈형식에서 문자를 나누고 [<span class="number">0</span>]번째 문자만 가져오기</span><br></pre></td></tr></table></figure>

<h6 id="데이터-프레임-모든-열에-특정-스칼라값-or-특정-컬럼-value-연산"><a href="#데이터-프레임-모든-열에-특정-스칼라값-or-특정-컬럼-value-연산" class="headerlink" title="데이터 프레임 모든 열에 특정 스칼라값 or 특정 컬럼.value 연산"></a>데이터 프레임 모든 열에 특정 스칼라값 or 특정 컬럼.value 연산</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[컬럼명] *= (스칼라값)  / 해당 데이터프레임 컬럼이 와도 됨</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[컬럼명] = df[컬럼명].div(스칼라값 <span class="keyword">or</span> 컬럼, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>   a=10, b=20, c=3</p>
<p>   Operator    Description    Example<br>    +    더하기             a + b        30<br>    -    빼기                a - b       -10<br>    *    곱하기              a * b       200<br>    /    나누기              b / a        2.0<br>    % 나머지            b % a        0<br>    **    제곱               a ** c      1000<br>    //    몫                 a // c          3</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-06-02T10:49:44.000Z" title="2020-06-02T10:49:44.000Z">2020-06-02</time><span class="level-item">a minute read (About 125 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/SQLite-db-view-tool-md/">SQLite db view tool.md</a></h1><div class="content"><p>DB Browser for SQLite</p>
<p><a href="http://sqlitebrowser.org/">http://sqlitebrowser.org/</a></p>
<p>사이트 들어가서, 해당 OS에서 맞는 파일을 다운로드 하자. 설치 완료 후, 프로그램을 가동하여, 열기를 통해 SQLite 의 파일을 열어서 테이블 형식으로 아래와 같이 볼 수 있다.</p>
<p><img src="https://user-images.githubusercontent.com/59719711/83511951-67537480-a50a-11ea-9bca-762b7deadfed.png" alt="screenshot"></p>
<p>사이트에서 접속하여 위에 카테고리 부분에 download를 클릭 후 해당 OS에 맞는 것을 설치해주면 끝.</p>
<p>쉽지?</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-31T01:48:53.000Z" title="2020-05-31T01:48:53.000Z">2020-05-31</time><span class="level-item">2 minutes read (About 285 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/31/NodeJS-n-%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%90%E1%85%A9%E1%86%BC%E1%84%92%E1%85%A1%E1%84%8B%E1%85%A7-NodeJS-%E1%84%87%E1%85%A5%E1%84%8C%E1%85%A5%E1%86%AB-%E1%84%87%E1%85%A7%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5/">[NodeJS]_n_을_통하여_NodeJS_버전_변경하기</a></h1><div class="content"><pre><code>NodeJS의 경우 버전 변경이 굉장히 잦고 버전마다 의존성 패키지가 매우 크기 때문에 여기서는 NodeJS 버전을 간단히 변경하는 n 을 소개하도록 할게</code></pre><h6 id="1-npm-을-통하여-n-설치하기"><a href="#1-npm-을-통하여-n-설치하기" class="headerlink" title="1. npm 을 통하여 n 설치하기"></a>1. npm 을 통하여 n 설치하기</h6><pre><code>우선 현재 nodejs 의 버전을 확인해 봅니다. </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!node -v</span><br></pre></td></tr></table></figure>

<pre><code>v12.17.0


그리고 npm 을 통하여 n 을 global 로 설치하자! (nodeJS를 컨트롤 할 수 있는 라이브러리라 아주 중요해!)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!sudo npm install -g n</span><br></pre></td></tr></table></figure>

<pre><code>Password:

그리고 n을 재대로 설치 되었는지 확인을 위하여 버전을 확인하도록 하자</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;img width=<span class="string">"483"</span> alt=<span class="string">"스크린샷 2020-05-31 오전 10 43 54"</span> src=<span class="string">"https://user-images.githubusercontent.com/59719711/83342570-0a01cc80-a32c-11ea-83d3-a67d05de3533.png"</span>&gt;</span><br></pre></td></tr></table></figure>

<h6 id="2-n-을-이용하여-버전-변경하기"><a href="#2-n-을-이용하여-버전-변경하기" class="headerlink" title="2. n 을 이용하여 버전 변경하기"></a>2. n 을 이용하여 버전 변경하기</h6><pre><code>버전 변경방법은 매우 간단해. n 뒤에 lts, latest 혹은 버전을 적으면 끝!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lts 버전 설치</span></span><br><span class="line">n lts</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 최신 버전 설치</span></span><br><span class="line">n latest</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 특정 버전 설치</span></span><br><span class="line">n <span class="number">11</span></span><br></pre></td></tr></table></figure>

<pre><code>마지막으로 NodeJS의 버전을 변경 후 프로젝트의 node_modules를 삭제하고 yarn 혹은 npm으로 패키지를 재설치 하거나 업그레이드 하는 것을 추천해. 안그러면 종종 오류가 발생해</code></pre></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-28T10:44:58.000Z" title="2020-05-28T10:44:58.000Z">2020-05-28</time><span class="level-item">3 minutes read (About 486 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/28/wget-%EC%84%A4%EC%B9%98%ED%95%98%EB%8A%94%EB%B2%95/">wget 설치하는법</a></h1><div class="content"><h6 id="Mac-OS-X에서-wget을-설치하는-방법"><a href="#Mac-OS-X에서-wget을-설치하는-방법" class="headerlink" title="Mac OS X에서 wget을 설치하는 방법"></a>Mac OS X에서 wget을 설치하는 방법</h6><pre><code>wget는 정말 편리한 명령줄 유틸리티지만, 안타깝게도 OS X에는 포함되지 않았다. 물론  Curl은 적절한 대체물이 될 수 있지만, 종종 wget로 스크립트가 쓰여지고, curl을 사용하는 것으로 변환하는 것은 어렵고 시간이 오래 걸릴 수 있어서 wget이 선호되는 경향이 있다.

homebrew로 wget을 설치하면 된다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!brew install wget</span><br></pre></td></tr></table></figure>

<pre><code>Updating Homebrew...
[34m==&gt;[0m [1mAuto-updated Homebrew![0m
Updated 2 taps (homebrew/core and homebrew/cask).
[34m==&gt;[0m [1mUpdated Formulae[0m
plenv               postgresql@11       swiftformat         tomee-webprofile
postgresql          postgresql@9.5      tomee-plume
postgresql@10       postgresql@9.6      tomee-plus
[34m==&gt;[0m [1mUpdated Casks[0m
cryo                                     switchresx

[33mWarning:[0m wget 1.20.3_2 is already installed and up-to-date
To reinstall 1.20.3_2, run `brew reinstall wget`</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!brew uninstall --force node</span><br><span class="line">!brew uninstall icu4c &amp;&amp; brew install icu4c</span><br><span class="line">!brew unlink icu4c &amp;&amp; brew link icu4c --force</span><br><span class="line">!brew install node</span><br></pre></td></tr></table></figure>

<pre><code>Uninstalling node... (4,660 files, 60.3MB)
[31mError:[0m Refusing to uninstall /usr/local/Cellar/icu4c/66.1
because it is required by graphviz, harfbuzz and pango, which are currently installed.
You can override this and force removal with:
  brew uninstall --ignore-dependencies icu4c
Unlinking /usr/local/Cellar/icu4c/66.1... 0 symlinks removed
[33mWarning:[0m Refusing to link macOS provided/shadowed software: icu4c
If you need to have icu4c first in your PATH run:
  echo &apos;export PATH=&quot;/usr/local/opt/icu4c/bin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile
  echo &apos;export PATH=&quot;/usr/local/opt/icu4c/sbin:$PATH&quot;&apos; &gt;&gt; /Users/wglee/.bash_profile

For compilers to find icu4c you may need to set:
  export LDFLAGS=&quot;-L/usr/local/opt/icu4c/lib&quot;
  export CPPFLAGS=&quot;-I/usr/local/opt/icu4c/include&quot;
Updating Homebrew...
[34m==&gt;[0m [1mDownloading https://homebrew.bintray.com/bottles/node-14.3.0.catalina.bottle[0m
[34m==&gt;[0m [1mDownloading from https://akamai.bintray.com/e3/e34c4c25365bc0f5cc245d791dd93[0m
######################################################################## 100.0%
[34m==&gt;[0m [1mPouring node-14.3.0.catalina.bottle.tar.gz[0m
[34m==&gt;[0m [1mCaveats[0m
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d
[34m==&gt;[0m [1mSummary[0m
🍺  /usr/local/Cellar/node/14.3.0: 4,659 files, 60.8MB


** 필자가 갑자기 hexo가 작동되지 않아 적잖이 당황하던 중 해결법을 찾아서 이것도 공유해.
!brew uninstall --force node
!brew uninstall icu4c &amp;&amp; brew install icu4c
!brew unlink icu4c &amp;&amp; brew link icu4c --force
!brew install node

노드 언인스톨 후 재설치하면 문제 없이 작동한다.</code></pre></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-19T10:53:06.000Z" title="2020-05-19T10:53:06.000Z">2020-05-19</time><span class="level-item">3 minutes read (About 499 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/19/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-%EC%84%A4%EC%B9%98%EB%90%98%EC%96%B4%EC%9E%88%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%EB%B2%84%EC%A0%84-%EC%B2%B4%ED%81%AC%ED%95%98%EA%B8%B0/">파이썬에서 설치되어있는 라이브러리 버전 체크하기</a></h1><div class="content"><p>파이썬을 사용하다 보면 사용하고자 하는 라이브러리 버전에 따라 같은 라이브러리임에도 불구하고 명령어가 다른 경우가 있어. 그럴 경우 설치된 라이브러리의 버전을 알아야 거기에 맞게 진행을 할 수 있겠지?</p>
<p>이때, 설치한 라이브러리들의 버전이 무엇인지부터 먼저 확인해보자!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze</span><br></pre></td></tr></table></figure>

<pre><code>anaconda-client==1.7.2
anaconda-navigator==1.9.12
appnope==0.1.0
asn1crypto==1.3.0
attrs==19.3.0
autopep8==1.5.2
autopy==4.0.0
backcall==0.1.0
backports.functools-lru-cache==1.6.1
backports.tempfile==1.0
backports.weakref==1.0.post1
beautifulsoup4==4.6.0
bleach==3.1.0
boto==2.49.0
boto3==1.13.12
botocore==1.16.12
branca==0.4.0
bs4==0.0.1
cachetools==4.0.0
certifi==2020.4.5.1
cffi==1.14.0
chardet==3.0.4
chart-studio==1.0.0
click==7.1.1
cloudpickle==1.3.0
clyent==1.2.2
colorama==0.4.3
colorlover==0.3.0
conda==4.8.3
conda-build==3.18.9
conda-package-handling==1.6.0
conda-verify==3.4.2
configobj==5.0.6
cryptography==2.8
cufflinks==0.17.3
cvxpy==1.0.31
cycler==0.10.0
cytoolz==0.10.1
dask==2.14.0
decorator==4.4.2
defusedxml==0.6.0
dill==0.3.1.1
docutils==0.15.2
dpkt==1.9.2
ecos==2.0.7.post1
entrypoints==0.3
et-xmlfile==1.0.1
filelock==3.0.12
finance-datareader==0.9.6
folium==0.10.1
funcy==1.14
future==0.18.2
gensim==3.8.3
glob2==0.7
google-api-core==1.16.0
google-auth==1.12.0
google-auth-oauthlib==0.4.1
google-cloud-bigquery==1.24.0
google-cloud-core==1.3.0
google-resumable-media==0.5.0
googleapis-common-protos==1.51.0
idna==2.9
imageio==2.8.0
importlib-metadata==1.5.0
inflect==4.1.0
ipykernel==5.1.4
ipython==7.13.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
jaraco.itertools==5.0.0
jdcal==1.4.1
jedi==0.16.0
Jinja2==2.11.1
jmespath==0.10.0
joblib==0.14.1
JPype1==0.7.5
jsonschema==3.2.0
jupyter-client==6.1.2
jupyter-core==4.6.3
jupyterthemes==0.20.0
kiwisolver==1.2.0
konlpy==0.5.2
lesscpy==0.14.0
libarchive-c==2.8
lief==0.9.0
lxml==4.5.0
MarkupSafe==1.1.1
matplotlib==3.2.1
missingno==0.4.2
mistune==0.8.4
mkl-fft==1.0.15
mkl-random==1.1.0
mkl-service==2.3.0
more-itertools==8.2.0
mpmath==1.1.0
multiprocess==0.70.9
navigator-updater==0.2.1
nbconvert==5.6.1
nbformat==5.0.4
netifaces==0.10.9
networkx==2.4
nltk==3.5
notebook==6.0.3
numexpr==2.7.1
numpy==1.18.1
oauthlib==3.1.0
olefile==0.46
opencv-python==4.2.0.34
openpyxl==3.0.3
osqp==0.6.1
packaging==20.3
pandas==1.0.3
pandas-gbq==0.13.1
pandocfilters==1.4.2
parso==0.6.2
patsy==0.5.1
pexpect==4.8.0
pgmpy==0.1.10
picklable-itertools==0.1.1
pickleshare==0.7.5
Pillow==7.0.0
pkginfo==1.5.0.1
plotly==4.5.0
pluggy==0.13.1
ply==3.11
prometheus-client==0.7.1
prompt-toolkit==3.0.4
protobuf==3.11.3
psutil==5.7.0
ptyprocess==0.6.0
py==1.8.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycodestyle==2.5.0
pycosat==0.6.3
pycparser==2.20
pydata-google-auth==0.3.0
Pygments==2.6.1
pyLDAvis==2.1.2
PyMySQL==0.9.3
pyOpenSSL==19.1.0
pyparsing==2.4.7
pyrsistent==0.16.0
PySocks==1.7.1
pytest==5.4.2
python-dateutil==2.8.1
python-xlib==0.27
pytz==2019.3
PyWavelets==1.1.1
PyYAML==5.3.1
pyzmq==18.1.1
QtPy==1.9.0
regex==2020.5.14
requests==2.23.0
requests-file==1.4.3
requests-oauthlib==1.3.0
retrying==1.3.3
rsa==4.0
ruamel-yaml==0.15.87
s3transfer==0.3.3
schedule==0.6.0
scikit-image==0.16.2
scikit-learn==0.22.2.post1
scipy==1.4.1
scs==2.1.2
seaborn==0.10.0
selenium==3.141.0
Send2Trash==1.5.0
six==1.14.0
sklearn==0.0
smart-open==2.0.0
soupsieve==2.0
soynlp==0.0.493
SQLAlchemy==1.3.16
statsmodels==0.11.0
sympy==1.5.1
terminado==0.8.3
testpath==0.4.4
toolz==0.10.0
torch==1.5.0
tornado==6.0.4
tqdm==4.44.1
traitlets==4.3.3
tweepy==3.8.0
urllib3==1.25.8
wcwidth==0.1.9
webencodings==0.5.1
wget==3.2
widgetsnbextension==3.5.1
wordcloud==1.7.0
xlrd==1.2.0
xmltodict==0.12.0
zipp==2.2.0
Note: you may need to restart the kernel to use updated packages.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>어때? 쉽지? 물론 특정 라이브러리를 잡아서 하는 방법도 있지만, 여기서 소개하는 방법은 설치된 모든 라이브러리의 버전을 확인할 수 있는 방법이야. 파이썬 라이브러리 종류가 매우 많은데 이 명령어 하나로 모든 설치된 라이브러리의 버전을 확인할 수 있으니까 상당히 편리한 것 같네.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-15T08:54:06.000Z" title="2020-05-15T08:54:06.000Z">2020-05-15</time><span class="level-item">25 minutes read (About 3688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/15/Data-Analytics-on-Football/">Data Analytic on Football</a></h1><div class="content"><p>[숫자는 거짓말하지 않는다: 왜 축구 클럽들이 그렇게 애널리틱스를 중요하게 생각할까(The numbers don’t lie: why football clubs place such importance on analytics)]</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82031796-54047480-96d5-11ea-8271-0e0216738f5c.jpg" alt="WEST_HAM-main2-large_trans++01hEHXp48ZjlCwDoPtrE-G07QTvTgRHNz-gQ-gQnMTo"></p>
<p>웨스트 햄의 로리 캠벨은 빅 클럽에서 핵심 결정에 정보를 제공하는 떠오르는 축구 애널리스트들 중의 하나이다.</p>
<p>선수들이 떠난 한참 후의 웨스트 햄 유나이티드의 채드웰 헬스 훈련장의 조용한 구석에서 로리 캠벨은 컴퓨터 화면을 응시하고 있다. 이것이 보비 무어와 제프 허스트 경의 두 번째 집이었던 이래로 그 주변의 거주지들은 오직 피상적으로 변했을 뿐이나, 필드 밖의 준비의 복잡성은 완전히 바뀌고 있다.</p>
<p>캠벨은 웨스트 햄의 기술 스카우트이자 분석가이다. 옥스포드 졸업생이자, 알라스테어 캠벨의 아들들 중 하나인  그는 성공적인 포커 선수였으며, 약간의 선수 경험과 코칭 배경을 가졌다. 그의 초점은 축구에 관한 무한한 통계적이 데이터들을 이해하는 것이며, 그래서 클럽의 핵심적인 의사 결정자들에게 무엇이 정말 중요한지를 전달하는 것이다.</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82031862-6c748f00-96d5-11ea-934a-558cd2f91617.jpg" alt="WEST_HAM-epa-large_trans++ZgEkZX3M936N5BQK4Va8RWtT0gK_6EfZT336f62EI5U"></p>
<p>은골로 칸테는 올시즌의 영입이고 축구 애널리틱스의 승리이다.</p>
<p>더구나, 축구의 가장 효과적인 분석 작업들 중 몇몇과 단순히 구매력을 거의 반영하지 않고 있는 프리미어 리그 테이블 사이의 잠재적인 상관 관계는 분명하다. 레스터 시티와 웨스트 햄은 오늘 만나 경기를 갖지만, 예를 들어 어떻게 그들이, 만체스터 유나이티드가 마루앙 펠라이니, 안더 에레라 그리고 바스티안 슈바인슈타이거에 7천만 파운드를 쏟아부을 때, 은골로 칸테, 디미트리 파예 그리고 리야드 마레즈를 천 6백만 파운드에 발견해냈을까? 그리고 무엇이 토튼햄 핫스퍼가 델리 알리와 에릭 다이어로 이끌었는지 혹은 사우스햄튼을 사디오 마네와 버질 반 다이크로 이끌었을까? 왜 팀들이 전에 없이 적은 크로스를 하고 있을까? 무엇이 레스터의 독특한 특징일까? 왜 펩 과르디올라와 같은 감독들이 먼 거리에서의 슈팅을 장려하지 않을까? 그리고 모든 시즌들 중에서 가장 놀라운, 테이블이 거짓말을 하지 않는다는 것은 정말 사실일까?</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82031949-88783080-96d5-11ea-84a3-4c77c58be603.jpg" alt="WEST_HAM-epa-large_trans++ZgEkZX3M936N5BQK4Va8RWtT0gK_6EfZT336f62EI5U"></p>
<p>캠벨은 웨스트 햄의 기술 스카우트이자 애널리스트이다.</p>
<p>애널리틱스는 최소한 부분적인 답을 제공한다. 비록 캠벨이 분석의 가치가 여전히 의사결정의 기저를 구성하고 있는 경험, 직관, 본질적인 지식과 접촉들을 대체한다기 보다는 보조하는 것이라고 확신하지만 말이다. 그는 “비효율적인 어떤 시장도 기회입니다.”라고 말한다. “축구가 재능을 가치 평가하는 세트나 동의된 방식을 갖고 있지 않고 너무나 임의적이라는 사실은 기회입니다. 통계와 애널리틱스는 차이가 있습니다. 통계는 당신에게 일어난 사건들에 대해서 말해줍니다. 그들은 맥락없이는 아무 것도 의미하지 않습니다.</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82032034-a5acff00-96d5-11ea-8d65-04f8f4368e07.jpg" alt="analytics2-large_trans++qVzuuqpFlyLIwiB6NTmJwfSVWeZ_vEN7c6bHu2jJnT8"></p>
<p>레스터의 직접적인 스타일은 그들을 뚜렷한 프리미어 리그 아웃라이어로 만들었다.</p>
<p>“애널리틱스는 그러한 통계들을 미래의 퍼포먼스를 예측하기 위해 해석하는 것입니다. 당신은 모든 것을 측정할 수 있습니다. 어려운 것은 무엇이 중요한지를 알아내는 것입니다. 한가지 좋은 것은 축구는 꽤 단순하다는 것입니다. 모든 것은 결국에는 어떻게든 골과 연관되어 있습니다, 그것이 우리의 득점 기회를 늘려주거나 아니면 실점을 막아주거나요. 그것은 또한 감독이 팀이 어떻게 플레이하기를 원하는 틀 안에서 통합니다.”</p>
<p>보다 더 많은 통찰들이 사우스햄튼의 훈련시설에서 발견될 수 있다. 가장 놀라운 곳은 클럽의 34세의 스카우팅과 선수선발 이사인 로스 윌슨이 자리잡고 있는 방이다. 그의 바로 앞에는 15개의 일련의 화면 앞에서 젊은 스태프들의 팀이 정보들을 처리하고 있다. 몇몇은 축구 분석의 특정 분야에서의 학위를 보유한 인턴들이다. 윌슨의 오른쪽에는 보다 희끗희끗한 머리를 가진 관계자인 로드 루딕과 같은 사람이 있다. 그는 뉴포트의 필드에서 뛰던 8살의 가레스 베일을 발견한 스카우트이다. 윌슨의 왼쪽에는 “블랙 박스”라는 단어가 미스테리하게 걸려있는 문이 있다.</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82032099-bc535600-96d5-11ea-9434-60c71dc5d890.jpg" alt="1111"></p>
<p>폴 미첼은 사우스햄튼에서 토튼햄으로 이동해서 선수선발 및 분석 팀장이 되었다.</p>
<p>사우스햄튼은 이 미니-시네마에 쓰이는 맞춤형 소프트웨어를 꾸준히 수정하고 있다. 그 소프트웨어는 몇번의 클릭들 만으로 전세계에 있는 어떤 선수에 대한 것도 볼 수 있다. 다른 클럽들은 비슷한 기술을 개발 중이며, 수학적으로 재능들을 골라내는 사람들 사이에서, 이적 시장이 떠오르고 있다. 아스날은 올 해 벤 뤼글워스를 레스터로부터 빼냈고, 미국 기반의 분석 회사인 statDNA를 2백만 파운드에 사들였다. 마우리시오 포체티노와 함께, 토튼햄은 그들의 선수선발과 분석 팀장인 폴 미첼을 사우스햄튼에서 선발했다. 부상으로 27세에 선수 커리어를 끝냈던 미첼은 이렇게 말한다. “저는 한 번 좋은 경기를 가질 때 다른 80경기에서는 그렇게 좋지 않다는 간단한 이론을 발견했습니다.”</p>
<p>포커에서처럼, 캠벨은 선수 선발을 “당신의 베팅의 경제적인 리스크를 가용한 정보를 가지고 관리하는 것”이라고 부른다. 하지만 다른 점은 그의 산업의 다른 멤버들과의 미팅이 분명히 핵심적이라는 것을 강조한다. 과거의 축구에서의 혁신의 시도가 주저앉은 곳에는 종종 의사소통의 실수나 개인간의 충돌들에 기반한다.</p>
<p>클라이브 우드와드 경은 보통 생각되는 것보다 해리 레드납과 더 나은 관계를 가지고 있다. 하지만 거기에 루퍼트 로, 데이브 바셋, 데니스 와이즈와 사이먼 클리포드를 더하면 당신이 그것이 어디서 잘못되었는지 파악하기 위해서 에르큘 포와드의 추리력까지 필요하지도 않다. 애널리틱스가 차이를 만들어내는 곳에는 주로 문화가 정립된다. “당신이 플레이할 필요가 있는 클럽들 중에는 전통이 깊이 배어있는 클럽들이 있을 것입니다. 하지만 운이 좋게도 내가 있었던 클럽들은 사고방식이 매우 열려있었습니다.” 윌슨이 말한다.</p>
<p>캠벨은 이렇게 더한다: “애널리틱스 세계가 힘겨워하는 부분은 전통적인 축구 세계로의 다리를 놓는 것입니다. 정보를 더 침투시키기 위해서요. 수학적인 측면에서는 완벽히 논리적으로 말이되는 많은 정보들을 주고 수십년간 발전해 온 스포츠가 그것을 하루 아침에 받아들이기를 기대하는 것은 사실 꽤 건방진 것이다. 나는 이것이 가장 큰 도전으로 남아있다고 말하고 싶다. 정보를 전달할 수 있기 위해서는 당신은 역학관계와 당신과 함께 일하는 사람들의 성격들을 이해해야만 한다. 나는 그것이 왜 애널리틱스가 다른 비지니스에서 그랬던 것처럼 축구에 침투하지 못한 이유라고 생각한다.” </p>
<p><img src="https://user-images.githubusercontent.com/59719711/82032159-d12fe980-96d5-11ea-9c26-b10634bad37d.jpg" alt="45"></p>
<p>아스날 감독 아슨 벵거는 또한 애널리틱스를 수용해오고 있다.</p>
<p>하지만 이것은 변하고 있다. 캠벨은 그가 슬래븐 빌리치와 선수 선발 디렉터인 토니 헨리 아래에서 일할 수 있어서 매우 운이 좋았다고 말한다. 그리고 모두의 역할에 분명함이 있었다. 그들이 원하는 것은 간단히 그들의 결정을 도와줄 정보에 대한 믿을만한 평가였다. 나이 많은 감독들 역시 접근하고 있다. 클라우디오 라니에리는 한 예이다. 아슨 벵거는 올 시즌 공식자리에서 아스날의 “기대 골 값 (xG)”에 대해 언급하면서 충격을 촉발시켰다. 그것은 팀이 통계적으로 얼마나 자주 득점할 수 있는지에 대한 스포츠 베팅과 애널리틱스에서 핵심적인 측정값이다.</p>
<p>보루시아 도르트문트의 코치 토마스 투헬은 xG에 대해 더 배우기 위해 매튜 베넘을 찾아갔다. 베넘은 스포츠 베팅에서 수백만을 벌었고 그 이후로 브렌트포드와 FC밋츌란을 사들였다. 선수를 개인적으로 관찰하기 위해 폭넓게 움직이는 캠벨과 마찬가지로, 베넘은 축구처럼 낮은 득점을 가진 스포츠에서의 어떤 수학 모델의 뚜렷한 변동성을 보완하기 위한 “눈으로 하는 스카우팅”의 중요성을 강조한다. 포커처럼, 랜덤과 통제불가능한 사건들이 판단들을 형성하는데 서두르는 가운데에 종종 간과하는 부분적인 역할을 한다.</p>
<p>캠벨은 이렇게 말한다. “이것은 축구를 흥미롭게 만드는 것입니다만 예측불가능은 항상 심각한 비효율을 동반합니다. 운을 불평하는 프로 포커 선수들은 편협한 것입니다. 운이라는 것은 그들이 사는 것을 가능케하는 것입니다. 만약 내가 정말 나쁜 선수와 포커를 친다면, 그는 100번 중에 40번을 이길 것입니다. 만약 내가 개리 카스파로프와 체스를 둔다면 그가 100번을 이기겠죠. 체스 선수들은 베팅으로 돈을 벌지 않습니다. 왜냐하면 아무도 그들에게 돈을 걸지 않으니까요.”</p>
<p><img src="https://user-images.githubusercontent.com/59719711/82032241-e7d64080-96d5-11ea-8ff4-3abe17fb219d.jpg" alt="WEST_HAM-reuters-large_trans++oQmI6CCYeWXI2SDtPW01M2FxAL8YkdOPBcsmeEiEP3A"></p>
<p>레스터의 동화같은 시즌은 통계적인 모델들이 틀렸음을 입증해오고 있다.</p>
<p>애널리틱스는 점점 더 많은 의견들로 가득찬 현재 지형 안에서 목소리를 내기 위해 싸우고 있다. 캠벨은 이렇게 말한다. “아슨 벵거는 우리가 수직적에서 수평적인 사회로 움직이고 있다고 말했습니다. 수직적인 것은 꼭대기에 리더가 있고 모두가 따르는 것입니다. 수평적인 것은 정보와 의견들에 폭격을 당하는 리더를 가진 것입니다. 그것이 리더가 어떤 것이 중요하고 어떤 것이 노이즈인지를 구분하는 것이 정말 중요한 부분입니다.”</p>
<p>그러면 처음 질문으로 돌아가 보자. 칸테, 마네 그리고 파예는 궁극적으로 기민한 축구적인 결정들이었다. 하지만 애널리틱스 커뮤니티의 기저에 있는 퍼포먼스 지표들의 승리였다. 분명한 통계적인 증거는 크로스들이 스루 볼 보다 적은 확률의 전술이라는 것이고 먼 거리에서의 슈팅은 더 나은 포지션으로 패스하는 것 보다 적은 골을 생산한다는 것이다.</p>
<p>수학 교수이자 사커매틱스의 저자 데이빗 섬터에 따르면, 레스터와 리그의 다른 팀들과의 충격적인 차이는 어떻게 그들이 상대적으로 길고 직선적인 패스들로 볼을 전방으로 빠르게 움직이는지이다.</p>
<p>그러면 테이블은 절대 거짓말하지 않는다는 클리셰는 거짓일까? 글쎄, 아마도 그것은 진실 전부를 말하지는 않는다. 거의 모든 xG모델은 아스날이 사실 엄청난 기회를 놓쳤고 선두에 있었어야 한다고 말하고 있다. 대부분의 모델은 만약 이번 시즌이 무한한 경기 수를 가졌을 때 레스터가 4위에서 8위 사이에 놓고 있다. 분산과 운은, 38경기 프로그램에서 상당한 양으로 남아있다. 여전히 그 차이들은 좁고, 만약 지난 해가 어떤 것도 새로 증명하는 것이 아니라면, 열심히 일하는 것과 스마트함이 클럽의 은행 계좌의 사이즈 보다 더 중요할 수 있다.</p>
<p>출 처 : <a href="http://www.telegraph.co.uk/football/2016/04/16/the-numbers-dont-lie-why-football-clubs-place-such-importance-on/">http://www.telegraph.co.uk/football/2016/04/16/the-numbers-dont-lie-why-football-clubs-place-such-importance-on/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-14T11:01:27.000Z" title="2020-05-14T11:01:27.000Z">2020-05-14</time><span class="level-item">30 minutes read (About 4483 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/14/leverage-outliar-cooks-distance-anova/">leverage-outliar-cooks_distance_anova</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br></pre></td></tr></table></figure>

<h4 id="레버리지-leverage"><a href="#레버리지-leverage" class="headerlink" title="레버리지 (leverage)"></a>레버리지 (leverage)</h4><pre><code>독립변수의 전체 데이터가 아닌 개별적인 데이터 표본 하나하나가 회귀분석 결과에 미치는 영향력은 레버리지 분석이나 아웃라이어 분석을 통해 알 수 있다.
레버리지(leverage)는 실제 종속변수값  𝑦 가 예측치(predicted target)  𝑦̂  에 미치는 영향을 나타낸 값이다. self-influence, self-sensitivity 라고도 한다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">dfx = pd.DataFrame(boston.data, columns=boston.feature_names)</span><br><span class="line">dfy = pd.DataFrame(boston.target, columns=[<span class="string">"MEDV"</span>])</span><br><span class="line">df = pd.concat([dfx,dfy],axis=<span class="number">1</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 14 columns</p>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dfX = sm.add_constant(dfx)</span><br><span class="line">df0 = pd.concat([dfX, dfy],axis=<span class="number">1</span>)</span><br><span class="line">df0</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>1.0</td>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>1.0</td>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>1.0</td>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>1.0</td>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>1.0</td>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 15 columns</p>
</div>



<h5 id="statsmodels를-이용한-레버리지-계산"><a href="#statsmodels를-이용한-레버리지-계산" class="headerlink" title="statsmodels를 이용한 레버리지 계산"></a>statsmodels를 이용한 레버리지 계산</h5><pre><code>레버리지 값은 RegressionResults 클래스의 get_influence 메서드로 다음과 같이 구할 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 100개의 데이터 생성</span></span><br><span class="line">X0, y, coef = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">1</span>, noise=<span class="number">20</span>,</span><br><span class="line">                             coef=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 레버리지가 높은 가상의 데이터를 추가</span></span><br><span class="line">data_100 = (<span class="number">4</span>, <span class="number">300</span>)</span><br><span class="line">data_101 = (<span class="number">3</span>, <span class="number">150</span>)</span><br><span class="line">X0 = np.vstack([X0, np.array([data_100[:<span class="number">1</span>], data_101[:<span class="number">1</span>]])])</span><br><span class="line">X = sm.add_constant(X0) <span class="comment">#상수항 추가</span></span><br><span class="line">y = np.hstack([y, [data_100[<span class="number">1</span>], data_101[<span class="number">1</span>]]])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(X0, y, s=<span class="number">30</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">plt.title(<span class="string">"가상의 회귀분석용 데이터"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="849" alt="output_6_0" src="https://user-images.githubusercontent.com/59719711/81926194-91f19200-961c-11ea-9b3f-3acaa0daccab.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = sm.OLS(pd.DataFrame(y), pd.DataFrame(X))</span><br><span class="line">result = model.fit()</span><br><span class="line">print(result.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      0   R-squared:                       0.936
Model:                            OLS   Adj. R-squared:                  0.935
Method:                 Least Squares   F-statistic:                     1464.
Date:                Thu, 14 May 2020   Prob (F-statistic):           1.61e-61
Time:                        14:22:53   Log-Likelihood:                -452.71
No. Observations:                 102   AIC:                             909.4
Df Residuals:                     100   BIC:                             914.7
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
0              3.2565      2.065      1.577      0.118      -0.840       7.353
1             78.3379      2.048     38.260      0.000      74.276      82.400
==============================================================================
Omnibus:                       16.191   Durbin-Watson:                   1.885
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.807
Skew:                          -0.534   Prob(JB):                     1.02e-08
Kurtosis:                       5.742   Cond. No.                         1.14
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


선형회귀 결과에서 get_influence 메서드를 호출하면 영향도 정보 객체를 구할 수 있고, 이 객체는 hat_matrix_diag 속성으로 레버리지 벡터의 값을 가지고도 있어</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">influence = result.get_influence()</span><br><span class="line">hat = influence.hat_matrix_diag</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt. stem(hat)</span><br><span class="line">plt.axhline(<span class="number">0.02</span>, c = <span class="string">'g'</span>, ls = <span class="string">'--'</span>) <span class="comment"># c = color , ls = linestyle</span></span><br><span class="line">plt.title(<span class="string">'각 데이터의 레버리지 값'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True.
  &quot;&quot;&quot;</code></pre><img width="831" alt="output_9_1" src="https://user-images.githubusercontent.com/59719711/81926258-a6ce2580-961c-11ea-9442-38672fbc831a.png">


<pre><code>그래프를 그리는 코드에서 0.02의 값은 레버리지 평균값을 구하는 공식 독립변수의 갯수 / 데이터의 갯수 로 구하면 된다</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![스크린샷 <span class="number">2020</span><span class="number">-05</span><span class="number">-14</span> 오후 <span class="number">2</span> <span class="number">31</span> <span class="number">46</span>](https://user-images.githubusercontent.com/<span class="number">59719711</span>/<span class="number">81926278</span>-b2215100<span class="number">-961</span>c<span class="number">-11</span>ea<span class="number">-9613</span>-c5ba582d5de0.png)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">ax = plt.subplot()</span><br><span class="line">plt.scatter(X0, y,s=<span class="number">30</span>)</span><br><span class="line">sm.graphics.abline_plot(model_results=result, ax=ax)</span><br><span class="line"></span><br><span class="line">idx = hat &gt; <span class="number">0.05</span></span><br><span class="line">plt.scatter(X0[idx], y[idx], s=<span class="number">300</span>, c=<span class="string">"r"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">"회귀분석 결과와 레버리지 포인트"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="833" alt="output_12_0" src="https://user-images.githubusercontent.com/59719711/81926328-bc434f80-961c-11ea-89ef-927b80f666c7.png">


<pre><code>그래프를 토대로 해석을 하자면, 데이터가 혼자만 너무 작거나 너무 크게 단독으로 존재할수록 레버리지가 커짐을 알 수 있어. 이 말은 저런 데이터은 전체 회귀분석 결과값에 큰 영향을 미친다는 말이야</code></pre><h4 id="아웃라이어-outlier"><a href="#아웃라이어-outlier" class="headerlink" title="아웃라이어(outlier)"></a>아웃라이어(outlier)</h4><pre><code>데이터와 동떨어진 값을 가지는 데이터, 즉 잔차가 큰 데이터를 아웃라이어(outlier)라고 하는데, 잔차의 크기는 독립 변수의 영향을 받으므로 아웃라이어를 찾으려면 이 영향을 제거한 표준화된 잔차를 계산해야 한다고 해. 무슨말인지 잘 모르겠지만 그래

statsmodels를 이용한 표준화 잔차 계산

잔차는 RegressionResult 객체의 resid 속성에 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>, <span class="number">6</span>))</span><br><span class="line">plt.stem(result.resid)</span><br><span class="line">plt.title(<span class="string">"각 데이터의 잔차"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True.</code></pre><img width="826" alt="output_16_1" src="https://user-images.githubusercontent.com/59719711/81926368-c8c7a800-961c-11ea-9826-24478a19b3b4.png">


<pre><code>표준화 잔차는 resid_pearson 속성에 있고, 보통 표준화 잔차가 2~4보다 크면 아웃라이어로 보는게 일반적이야</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt. stem(result.resid_pearson)</span><br><span class="line">plt.axhline(<span class="number">3</span>, c=<span class="string">'g'</span>, ls=<span class="string">'--'</span>)</span><br><span class="line">plt.axhline(<span class="number">-3</span>, c=<span class="string">'g'</span>, ls=<span class="string">'--'</span>)</span><br><span class="line">plt.title(<span class="string">'각 데이터의 표준화 잔차'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &quot;use_line_collection&quot; keyword argument to True.</code></pre><img width="818" alt="output_18_1" src="https://user-images.githubusercontent.com/59719711/81926392-d41ad380-961c-11ea-852b-985bce5ea681.png">


<h4 id="Cook’s-Distance"><a href="#Cook’s-Distance" class="headerlink" title="Cook’s Distance"></a>Cook’s Distance</h4><pre><code>회귀 분석에는 레버리지 따로, 잔차의 크기가 큰 데이터가 아웃라이어가 되고 그것을 보는 따로따로의 기능도 있지만  이 두개를 동시에 보는 방법이 바로 Cook&apos;s Distance야. 아마도 Cook이라는 사람이 만들었을 가능성이..

넘어가자

동시에 보는 기준이라고 생각하면 되고, 둘중 하나만 커지더라도 이 Cook&apos;s distance 값은 커지게 돼

모든 데이터의 레버리지와 잔차를 동시에 보려면 plot_leverage_resid2 명령을 사용하는데, 이 명령은 x축으로 표준화 잔차의 제곱을 표시하고 y축으로 레버리지값을 표시한다. 

그리고 데이터 아이디가 표시된 데이터들이 레버리지가 큰 아웃라이어 야</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">sm.graphics.plot_leverage_resid2(result)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">sm.graphics.influence_plot(result)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<pre><code>&lt;Figure size 1008x432 with 0 Axes&gt;</code></pre><img width="401" alt="output_21_1" src="https://user-images.githubusercontent.com/59719711/81926442-e563e000-961c-11ea-9511-4128f90f70c5.png">




<img width="392" alt="output_21_2" src="https://user-images.githubusercontent.com/59719711/81926462-ed238480-961c-11ea-827c-5e2360d26f75.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> statsmodels.graphics <span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line">cooks_d2, pvals = influence.cooks_distance</span><br><span class="line">K = influence.k_vars</span><br><span class="line">fox_cr = <span class="number">4</span> / (len(y) - K - <span class="number">1</span>)</span><br><span class="line">idx = np.where(cooks_d2 &gt; fox_cr)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">ax = plt.subplot()</span><br><span class="line">plt.scatter(X0, y)</span><br><span class="line">plt.scatter(X0[idx], y[idx], s=<span class="number">300</span>, c=<span class="string">"r"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">utils.annotate_axes(range(len(idx)), idx,</span><br><span class="line">                    list(zip(X0[idx], y[idx])), [(<span class="number">-20</span>, <span class="number">15</span>)] * len(idx), size=<span class="string">"small"</span>, ax=ax)</span><br><span class="line">plt.title(<span class="string">"Fox Recommendaion으로 선택한 아웃라이어"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="387" alt="output_22_0" src="https://user-images.githubusercontent.com/59719711/81926491-fb71a080-961c-11ea-815d-7cab11d016fd.png">


<h5 id="보스턴-집값-예측-문제¶"><a href="#보스턴-집값-예측-문제¶" class="headerlink" title="보스턴 집값 예측 문제¶"></a>보스턴 집값 예측 문제¶</h5><pre><code>보스턴 집값 문제에 아웃라이어를 적용해 보자. MEDV가 50인 데이터는 상식적으로 생각해도 이상한 데이터이므로 아웃라이어라고 판단할 수 있다. 나머지 데이터 중에서 폭스 추천공식을 사용하여 아웃라이어를 제외한 결과는 다음과 같다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">dfx = pd.DataFrame(boston.data, columns=boston.feature_names)</span><br><span class="line">dfy = pd.DataFrame(boston.target, columns=[<span class="string">"MEDV"</span>])</span><br><span class="line">df = pd.concat([dfx,dfy],axis=<span class="number">1</span>)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line">dfX = sm.add_constant(dfx)</span><br><span class="line">df0 = pd.concat([dfX, dfy],axis=<span class="number">1</span>)</span><br><span class="line">df0</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pred = result_boston.predict(dfX)</span><br><span class="line"></span><br><span class="line">influence_boston = result_boston.get_influence()</span><br><span class="line">cooks_d2, pvals = influence_boston.cooks_distance</span><br><span class="line">K = influence.k_vars</span><br><span class="line">fox_cr = <span class="number">4</span> / (len(y) - K - <span class="number">1</span>)</span><br><span class="line">idx = np.where(cooks_d2 &gt; fox_cr)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># MEDV = 50 제거</span></span><br><span class="line">idx = np.hstack([idx, np.where(boston.target == <span class="number">50</span>)[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">ax = plt.subplot()</span><br><span class="line">plt.scatter(dfy, pred)</span><br><span class="line">plt.scatter(dfy.MEDV[idx], pred[idx], s=<span class="number">200</span>, c=<span class="string">"r"</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">utils.annotate_axes(range(len(idx)), idx,</span><br><span class="line">                    list(zip(dfy.MEDV[idx], pred[idx])), [(<span class="number">-20</span>, <span class="number">15</span>)] * len(idx), size=<span class="string">"small"</span>, ax=ax)</span><br><span class="line">plt.title(<span class="string">"보스턴 집값 데이터에서 아웃라이어"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="374" alt="output_25_0" src="https://user-images.githubusercontent.com/59719711/81926518-04fb0880-961d-11ea-8ec0-70154b06352f.png">


<pre><code>다음은 이렇게 아웃라이어를 제외한 후에 다시 회귀분석을 한 결과이다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">idx2 = list(set(range(len(dfX))).difference(idx))</span><br><span class="line">dfX = dfX.iloc[idx2, :].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">dfy = dfy.iloc[idx2, :].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">model_boston2 = sm.OLS(dfy, dfX)</span><br><span class="line">result_boston2 = model_boston2.fit()</span><br><span class="line">print(result_boston2.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.812
Model:                            OLS   Adj. R-squared:                  0.806
Method:                 Least Squares   F-statistic:                     156.1
Date:                Thu, 14 May 2020   Prob (F-statistic):          2.41e-161
Time:                        15:14:52   Log-Likelihood:                -1285.2
No. Observations:                 485   AIC:                             2598.
Df Residuals:                     471   BIC:                             2657.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         18.8999      4.107      4.602      0.000      10.830      26.969
CRIM          -0.0973      0.024     -4.025      0.000      -0.145      -0.050
ZN             0.0278      0.010      2.651      0.008       0.007       0.048
INDUS         -0.0274      0.046     -0.595      0.552      -0.118       0.063
CHAS           0.9228      0.697      1.324      0.186      -0.447       2.292
NOX           -9.4922      2.856     -3.323      0.001     -15.105      -3.879
RM             5.0921      0.371     13.735      0.000       4.364       5.821
AGE           -0.0305      0.010     -2.986      0.003      -0.051      -0.010
DIS           -1.0562      0.150     -7.057      0.000      -1.350      -0.762
RAD            0.1990      0.049      4.022      0.000       0.102       0.296
TAX           -0.0125      0.003     -4.511      0.000      -0.018      -0.007
PTRATIO       -0.7777      0.098     -7.955      0.000      -0.970      -0.586
B              0.0107      0.002      5.348      0.000       0.007       0.015
LSTAT         -0.2846      0.043     -6.639      0.000      -0.369      -0.200
==============================================================================
Omnibus:                       45.944   Durbin-Watson:                   1.184
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               65.791
Skew:                           0.679   Prob(JB):                     5.17e-15
Kurtosis:                       4.188   Cond. No.                     1.59e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.59e+04. This might indicate that there are
strong multicollinearity or other numerical problems.


R-squared 의 성능점수가 올라간 것을 볼 수 있어.

이렇게 어떤 특정 데이터를 가지고 회귀분석 모델링을 할 때에는 하기 전에 레버리지가 큰 데이터와 아웃라이어의 값을 이러한 절차에 의해 뽑아서 제거하고 모델링을 한다면 더욱 성능이 좋은 회귀분석 모델링을 할 수 있는거야</code></pre><h4 id="분산-분석"><a href="#분산-분석" class="headerlink" title="분산 분석"></a>분산 분석</h4><pre><code>선형회귀분석의 결과가 얼마나 좋은지는 단순히 잔차제곱합(RSS: Residula Sum of Square)으로 평가할 수 없다. 변수의 단위 즉, 스케일이 달라지면 회귀분석과 상관없이 잔차제곱합도 달라지기 때문이야 ( ex.1km와 1000m)

분산 분석(ANOVA: Analysis of Variance)은 종속변수의 분산과 독립변수의 분산간의 관계를 사용하여 선형회귀분석의 성능을 평가하고자 하는 방법이다. 분산 분석은 서로 다른 두 개의 선형회귀분석의 성능 비교에 응용할 수 있으며 독립변수가 카테고리 변수인 경우 각 카테고리 값에 따른 영향을 정량적으로 분석하는데도 사용할 수 있게 돼

여러 수식들이 존재하지만 내가 이해를 못하겠고 결론은 다음과 같아.

모형 예측치의 움직임의 크기(분산,ESS)은 종속변수의 움직임의 크기(분산,TSS)보다 클 수 없어 그리고 모형의 성능이 좋을수록 모형 예측치의 움직임의 크기는 종속변수의 움직임의 크기와 비슷해진다는 점이야</code></pre><h5 id="F-검정을-사용한-변수-중요도-비교"><a href="#F-검정을-사용한-변수-중요도-비교" class="headerlink" title="F 검정을 사용한 변수 중요도 비교"></a>F 검정을 사용한 변수 중요도 비교</h5><pre><code>F검정은 각 독립변수의 중요도를 비교하기 위해 사용할 수 있다. 방법은 전체 모형과 각 변수 하나만을 뺀 모형들의 성능을 비교하는 것인데, 이는 간접적으로 각 독립 변수의 영향력을 측정하는 것이라고 할 수 있다. 예를 들어 보스턴 집값 데이터에서 CRIM이란 변수를 뺀 모델과 전체 모델의 비교하는 검정을 하면 이 검정 결과는 CRIM변수의 중요도를 나타낸다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_full = sm.OLS.from_formula(</span><br><span class="line">    <span class="string">"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS"</span>, data=df0)</span><br><span class="line">model_reduced = sm.OLS.from_formula(</span><br><span class="line">    <span class="string">"MEDV ~ ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS"</span>, data=df0)</span><br><span class="line"></span><br><span class="line">sm.stats.anova_lm(model_reduced.fit(), model_full.fit())</span><br></pre></td></tr></table></figure>

<pre><code>/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater
  return (a &lt; x) &amp; (x &lt; b)
/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less
  return (a &lt; x) &amp; (x &lt; b)
/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal
  cond2 = cond0 &amp; (x &lt;= _a)</code></pre><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>df_resid</th>
      <th>ssr</th>
      <th>df_diff</th>
      <th>ss_diff</th>
      <th>F</th>
      <th>Pr(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>493.0</td>
      <td>11322.004277</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>492.0</td>
      <td>11078.784578</td>
      <td>1.0</td>
      <td>243.219699</td>
      <td>10.801193</td>
      <td>0.001087</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>anova_lm 명령에서는 typ 인수를 2로 지정하면 하나 하나의 변수를 뺀 축소 모형에서의 F 검정값을 한꺼번에 계산할 수 있다.</code></pre><h5 id="아노바-분석-F검정"><a href="#아노바-분석-F검정" class="headerlink" title="아노바 분석 - F검정"></a>아노바 분석 - F검정</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = sm.OLS.from_formula(</span><br><span class="line">    <span class="string">"MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS"</span>, data=df0)</span><br><span class="line">result = model.fit()</span><br><span class="line">sm.stats.anova_lm(result, typ=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sum_sq</th>
      <th>df</th>
      <th>F</th>
      <th>PR(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CRIM</th>
      <td>243.219699</td>
      <td>1.0</td>
      <td>10.801193</td>
      <td>1.086810e-03</td>
    </tr>
    <tr>
      <th>ZN</th>
      <td>257.492979</td>
      <td>1.0</td>
      <td>11.435058</td>
      <td>7.781097e-04</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>2.516668</td>
      <td>1.0</td>
      <td>0.111763</td>
      <td>7.382881e-01</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>487.155674</td>
      <td>1.0</td>
      <td>21.634196</td>
      <td>4.245644e-06</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>1871.324082</td>
      <td>1.0</td>
      <td>83.104012</td>
      <td>1.979441e-18</td>
    </tr>
    <tr>
      <th>AGE</th>
      <td>0.061834</td>
      <td>1.0</td>
      <td>0.002746</td>
      <td>9.582293e-01</td>
    </tr>
    <tr>
      <th>DIS</th>
      <td>1232.412493</td>
      <td>1.0</td>
      <td>54.730457</td>
      <td>6.013491e-13</td>
    </tr>
    <tr>
      <th>RAD</th>
      <td>479.153926</td>
      <td>1.0</td>
      <td>21.278844</td>
      <td>5.070529e-06</td>
    </tr>
    <tr>
      <th>TAX</th>
      <td>242.257440</td>
      <td>1.0</td>
      <td>10.758460</td>
      <td>1.111637e-03</td>
    </tr>
    <tr>
      <th>PTRATIO</th>
      <td>1194.233533</td>
      <td>1.0</td>
      <td>53.034960</td>
      <td>1.308835e-12</td>
    </tr>
    <tr>
      <th>B</th>
      <td>270.634230</td>
      <td>1.0</td>
      <td>12.018651</td>
      <td>5.728592e-04</td>
    </tr>
    <tr>
      <th>LSTAT</th>
      <td>2410.838689</td>
      <td>1.0</td>
      <td>107.063426</td>
      <td>7.776912e-23</td>
    </tr>
    <tr>
      <th>CHAS</th>
      <td>218.970357</td>
      <td>1.0</td>
      <td>9.724299</td>
      <td>1.925030e-03</td>
    </tr>
    <tr>
      <th>Residual</th>
      <td>11078.784578</td>
      <td>492.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>각각의 독립변수들의 전체와 비교했을 때 얼마만큼 중요도를 가지는데 정량적으로 나온 결과값이야. 여기서 주목해야할 부분은 PR&gt;(&gt;F)부분으로 summary에서도 나오는 p-value값을 디테일하게 풀어놓은 값이고 예를 들어 LSTAT, RM의 경우 10의 -23승, 10의 -18승으로 수치가 제일 낮은걸 알 수 있어. 그러면 이 2가지의 독립변수가 종속변수에 가장 큰 영향을 미쳤다고 해석하면 되는거야
표의 F값을 보고도 알 수 있지만 F값은 확률의 의미는 없기 때문에 단순 순위를 매기는거 라면 결정할 수 있지만 만약 귀무가설/대립가설을 accept 하냐 reject 하냐의 확률적 의미를 판단한다면 F값만으로는 불가능해</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="범주형을-사용한-비선형성"><a href="#범주형을-사용한-비선형성" class="headerlink" title="범주형을 사용한 비선형성"></a>범주형을 사용한 비선형성</h5><pre><code>독립변수의 비선형성을 포착하는 또 다른 방법 중 하나는 강제로 범주형 값으로 만드는 것이다. 범주형 값이 되면서 독립변수의 오차가 생기지만 이로 인한 오차보다 비선형성으로 얻을 수 있는 이익이 클 수도 있다.

보스턴 집값 데이터에서 종속변수와 RM 변수의 관계는 선형에 가깝지만 방의 갯수가 아주 작아지거나 아주 커지면 선형모형에서 벗어난다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">sns.scatterplot(x=<span class="string">"RM"</span>, y=<span class="string">"MEDV"</span>, data=df0, s=<span class="number">60</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="836" alt="output_39_0" src="https://user-images.githubusercontent.com/59719711/81926554-17754200-961d-11ea-9ea7-69b37349b510.png">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_rm = sm.OLS.from_formula(<span class="string">'MEDV ~ RM'</span>, data=df0)</span><br><span class="line">result_rm = model_rm.fit()</span><br><span class="line">print(result_rm.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.484
Model:                            OLS   Adj. R-squared:                  0.483
Method:                 Least Squares   F-statistic:                     471.8
Date:                Thu, 14 May 2020   Prob (F-statistic):           2.49e-74
Time:                        19:28:18   Log-Likelihood:                -1673.1
No. Observations:                 506   AIC:                             3350.
Df Residuals:                     504   BIC:                             3359.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -34.6706      2.650    -13.084      0.000     -39.877     -29.465
RM             9.1021      0.419     21.722      0.000       8.279       9.925
==============================================================================
Omnibus:                      102.585   Durbin-Watson:                   0.684
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              612.449
Skew:                           0.726   Prob(JB):                    1.02e-133
Kurtosis:                       8.190   Cond. No.                         58.4
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


이렇게 RM 데이터 전체를 놓고 보면 종속변수 y와 아주 큰 상관관계가 있는것으로 보이지만 위에 그래프에서 봤듯이, 방의 갯수가 아주 적거나, 많으면 선형성을 보이지 않는 구간에 대해 조금 더 디테일하게 상관관게를 보고 싶다면 RM 데이터를 강제로 범주화 시켜 RM 데이터가 가지는 비선형성을 잡을 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rooms = np.arange(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">labels = [str(r) <span class="keyword">for</span> r <span class="keyword">in</span> rooms[:<span class="number">-1</span>]]</span><br><span class="line">df0[<span class="string">'CAT_RM'</span>] = np.round(df[<span class="string">'RM'</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">sns.barplot(<span class="string">'CAT_RM'</span>, <span class="string">'MEDV'</span>, data=df0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="836" alt="output_42_0" src="https://user-images.githubusercontent.com/59719711/81926587-222fd700-961d-11ea-88fb-a5ff2aec6b90.png">


<pre><code>이렇게 하면 RM 변수으로 인한 종속변수의 변화를 비선형 상수항으로 모형화 할 수 있다. 선형모형보다 성능이 향상된 것을 볼 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_rm2 = sm.OLS.from_formula(<span class="string">"MEDV ~ C(np.round(RM))"</span>, data=df0)</span><br><span class="line">result_rm2 = model_rm2.fit()</span><br><span class="line">print(result_rm2.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.537
Model:                            OLS   Adj. R-squared:                  0.532
Method:                 Least Squares   F-statistic:                     115.8
Date:                Thu, 14 May 2020   Prob (F-statistic):           3.57e-81
Time:                        19:33:48   Log-Likelihood:                -1645.6
No. Observations:                 506   AIC:                             3303.
Df Residuals:                     500   BIC:                             3329.
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
==========================================================================================
                             coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------------------
Intercept                 17.0200      2.814      6.049      0.000      11.492      22.548
C(np.round(RM))[T.5.0]    -2.0741      2.998     -0.692      0.489      -7.964       3.816
C(np.round(RM))[T.6.0]     2.3460      2.836      0.827      0.409      -3.226       7.918
C(np.round(RM))[T.7.0]    11.0272      2.869      3.843      0.000       5.389      16.665
C(np.round(RM))[T.8.0]    28.5425      3.093      9.228      0.000      22.466      34.619
C(np.round(RM))[T.9.0]    23.6133      4.595      5.139      0.000      14.586      32.641
==============================================================================
Omnibus:                       81.744   Durbin-Watson:                   0.799
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              467.887
Skew:                           0.542   Prob(JB):                    2.51e-102
Kurtosis:                       7.584   Cond. No.                         31.1
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre><h5 id="시간-독립변수의-변형"><a href="#시간-독립변수의-변형" class="headerlink" title="시간 독립변수의 변형"></a>시간 독립변수의 변형</h5><pre><code>독립변수가 시간인 경우에는 특정 시점에서 경과된 시간값으로 변형해야 한다. 일간 전기 사용량 데이터를 예로 들어 설명한다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = sm.datasets.get_rdataset(<span class="string">"elecdaily"</span>, package=<span class="string">"fpp2"</span>)</span><br><span class="line"></span><br><span class="line">df_elec = data.data.drop(columns=[<span class="string">"WorkDay"</span>, <span class="string">"Temperature"</span>])</span><br><span class="line">df_elec[<span class="string">"Date"</span>] = pd.date_range(<span class="string">"2014-1-1"</span>, <span class="string">"2014-12-31"</span>)</span><br><span class="line">df_elec.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Demand</th>
      <th>Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>360</th>
      <td>173.727990</td>
      <td>2014-12-27</td>
    </tr>
    <tr>
      <th>361</th>
      <td>188.512817</td>
      <td>2014-12-28</td>
    </tr>
    <tr>
      <th>362</th>
      <td>191.273009</td>
      <td>2014-12-29</td>
    </tr>
    <tr>
      <th>363</th>
      <td>186.240144</td>
      <td>2014-12-30</td>
    </tr>
    <tr>
      <th>364</th>
      <td>186.370181</td>
      <td>2014-12-31</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>파이썬 datetime 자료형은 toordinal 명령으로 특정 시점으로부터 경과한 시간의 일단위 값을 구하거나 timestamp 메서드로 초단위 값을 구할 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"></span><br><span class="line">df_elec[<span class="string">"Ordinal"</span>] = df_elec.Date.map(dt.datetime.toordinal)</span><br><span class="line">df_elec[<span class="string">"Timestamp"</span>] = df_elec.Date.map(dt.datetime.timestamp)</span><br><span class="line">df_elec.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Demand</th>
      <th>Date</th>
      <th>Ordinal</th>
      <th>Timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>360</th>
      <td>173.727990</td>
      <td>2014-12-27</td>
      <td>735594</td>
      <td>1.419606e+09</td>
    </tr>
    <tr>
      <th>361</th>
      <td>188.512817</td>
      <td>2014-12-28</td>
      <td>735595</td>
      <td>1.419692e+09</td>
    </tr>
    <tr>
      <th>362</th>
      <td>191.273009</td>
      <td>2014-12-29</td>
      <td>735596</td>
      <td>1.419779e+09</td>
    </tr>
    <tr>
      <th>363</th>
      <td>186.240144</td>
      <td>2014-12-30</td>
      <td>735597</td>
      <td>1.419865e+09</td>
    </tr>
    <tr>
      <th>364</th>
      <td>186.370181</td>
      <td>2014-12-31</td>
      <td>735598</td>
      <td>1.419952e+09</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>여기에서는 일단위 시간 값을 사용하여 회귀분석을 한다. 시간 값의 경우 크기가 크므로 반드시 스케일링을 해 주어야 한다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model5 = sm.OLS.from_formula(<span class="string">"Demand ~ scale(Ordinal)"</span>, data=df_elec)</span><br><span class="line">result5 = model5.fit()</span><br><span class="line">print(result5.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 Demand   R-squared:                       0.031
Model:                            OLS   Adj. R-squared:                  0.028
Method:                 Least Squares   F-statistic:                     11.58
Date:                Thu, 14 May 2020   Prob (F-statistic):           0.000739
Time:                        19:35:40   Log-Likelihood:                -1709.7
No. Observations:                 365   AIC:                             3423.
Df Residuals:                     363   BIC:                             3431.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==================================================================================
                     coef    std err          t      P&gt;|t|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept        221.2775      1.374    160.997      0.000     218.575     223.980
scale(Ordinal)    -4.6779      1.374     -3.404      0.001      -7.381      -1.975
==============================================================================
Omnibus:                       43.105   Durbin-Watson:                   0.677
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               96.485
Skew:                           0.614   Prob(JB):                     1.12e-21
Kurtosis:                       5.199   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


하지만 시간 독립변수는 이 외에더 다양한 특징들을 숨기고 있다. 예들 들어 연도, 월, 일, 요일 데이터를 별도의 독립변수로 분리하거나 한 달 내에서 몇번째 날짜인지 월의 시작 또는 끝인지를 나타내는 값은 모두 특징값이 될 수 있다. 판다스에서는 dt 특수 연산자를 사용하여 이러한 값을 구할 수 있다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df_elec[<span class="string">"Year"</span>] = df_elec[<span class="string">'Date'</span>].dt.year</span><br><span class="line">df_elec[<span class="string">"Month"</span>] = df_elec.Date.dt.month</span><br><span class="line">df_elec[<span class="string">"DayOfYear"</span>] = df_elec.Date.dt.dayofyear</span><br><span class="line">df_elec[<span class="string">"DayOfMonth"</span>] = df_elec.Date.dt.daysinmonth</span><br><span class="line">df_elec[<span class="string">"DayOfWeek"</span>] = df_elec.Date.dt.dayofweek</span><br><span class="line">df_elec[<span class="string">"WeekOfYear"</span>] = df_elec.Date.dt.weekofyear</span><br><span class="line">df_elec[<span class="string">"Weekday"</span>] = df_elec.Date.dt.weekday</span><br><span class="line">df_elec[<span class="string">"IsMonthStart"</span>] = df_elec.Date.dt.is_month_start</span><br><span class="line">df_elec[<span class="string">"IsMonthEnd"</span>] = df_elec.Date.dt.is_month_end</span><br><span class="line">df_elec.tail()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Demand</th>
      <th>Date</th>
      <th>Ordinal</th>
      <th>Timestamp</th>
      <th>Year</th>
      <th>Month</th>
      <th>DayOfYear</th>
      <th>DayOfMonth</th>
      <th>DayOfWeek</th>
      <th>WeekOfYear</th>
      <th>Weekday</th>
      <th>IsMonthStart</th>
      <th>IsMonthEnd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>360</th>
      <td>173.727990</td>
      <td>2014-12-27</td>
      <td>735594</td>
      <td>1.419606e+09</td>
      <td>2014</td>
      <td>12</td>
      <td>361</td>
      <td>31</td>
      <td>5</td>
      <td>52</td>
      <td>5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>361</th>
      <td>188.512817</td>
      <td>2014-12-28</td>
      <td>735595</td>
      <td>1.419692e+09</td>
      <td>2014</td>
      <td>12</td>
      <td>362</td>
      <td>31</td>
      <td>6</td>
      <td>52</td>
      <td>6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>362</th>
      <td>191.273009</td>
      <td>2014-12-29</td>
      <td>735596</td>
      <td>1.419779e+09</td>
      <td>2014</td>
      <td>12</td>
      <td>363</td>
      <td>31</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>363</th>
      <td>186.240144</td>
      <td>2014-12-30</td>
      <td>735597</td>
      <td>1.419865e+09</td>
      <td>2014</td>
      <td>12</td>
      <td>364</td>
      <td>31</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>364</th>
      <td>186.370181</td>
      <td>2014-12-31</td>
      <td>735598</td>
      <td>1.419952e+09</td>
      <td>2014</td>
      <td>12</td>
      <td>365</td>
      <td>31</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>



<pre><code>이렇게 추가적인 특징값을 이용하여 구한 모형은 성능이 향상된다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">feature_names = df_elec.columns.tolist()</span><br><span class="line">feature_names.remove(<span class="string">"Demand"</span>)</span><br><span class="line">feature_names.remove(<span class="string">"Date"</span>)</span><br><span class="line"></span><br><span class="line">formula = <span class="string">"""</span></span><br><span class="line"><span class="string">Demand ~ scale(Ordinal) + C(Month) + DayOfYear + </span></span><br><span class="line"><span class="string">         C(DayOfMonth) + C(DayOfWeek) + C(Weekday) + C(IsMonthStart) + C(IsMonthEnd)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">model6 = sm.OLS.from_formula(formula, data=df_elec)</span><br><span class="line">result6 = model6.fit()</span><br><span class="line">print(result6.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 Demand   R-squared:                       0.537
Model:                            OLS   Adj. R-squared:                  0.511
Method:                 Least Squares   F-statistic:                     19.98
Date:                Thu, 14 May 2020   Prob (F-statistic):           4.74e-46
Time:                        19:37:49   Log-Likelihood:                -1574.8
No. Observations:                 365   AIC:                             3192.
Df Residuals:                     344   BIC:                             3273.
Df Model:                          20                                         
Covariance Type:            nonrobust                                         
===========================================================================================
                              coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------
Intercept                  58.6105      2.423     24.188      0.000      53.844      63.377
C(Month)[T.2]              14.5730      4.587      3.177      0.002       5.551      23.595
C(Month)[T.3]              -1.2369      8.663     -0.143      0.887     -18.276      15.802
C(Month)[T.4]             -29.1875     10.239     -2.851      0.005     -49.326      -9.049
C(Month)[T.5]              23.4037     15.493      1.511      0.132      -7.069      53.876
C(Month)[T.6]              11.3667      3.758      3.024      0.003       3.974      18.759
C(Month)[T.7]              64.8095     22.750      2.849      0.005      20.063     109.556
C(Month)[T.8]              66.5692     26.490      2.513      0.012      14.467     118.671
C(Month)[T.9]              22.7687      9.491      2.399      0.017       4.100      41.437
C(Month)[T.10]             59.0491     33.895      1.742      0.082      -7.619     125.717
C(Month)[T.11]             33.4276     16.778      1.992      0.047       0.427      66.429
C(Month)[T.12]             72.2523     41.334      1.748      0.081      -9.047     153.552
C(DayOfMonth)[T.30]        38.3755     13.530      2.836      0.005      11.763      64.988
C(DayOfMonth)[T.31]         5.6620      7.806      0.725      0.469      -9.691      21.015
C(DayOfWeek)[T.1]           3.4766      1.829      1.900      0.058      -0.121       7.075
C(DayOfWeek)[T.2]           1.5756      1.821      0.865      0.387      -2.006       5.157
C(DayOfWeek)[T.3]           2.8568      1.831      1.560      0.120      -0.745       6.459
C(DayOfWeek)[T.4]           0.8832      1.831      0.482      0.630      -2.719       4.485
C(DayOfWeek)[T.5]         -12.8982      1.831     -7.045      0.000     -16.499      -9.297
C(DayOfWeek)[T.6]         -16.4623      1.829     -8.999      0.000     -20.060     -12.864
C(Weekday)[T.1]             3.4766      1.829      1.900      0.058      -0.121       7.075
C(Weekday)[T.2]             1.5756      1.821      0.865      0.387      -2.006       5.157
C(Weekday)[T.3]             2.8568      1.831      1.560      0.120      -0.745       6.459
C(Weekday)[T.4]             0.8832      1.831      0.482      0.630      -2.719       4.485
C(Weekday)[T.5]           -12.8982      1.831     -7.045      0.000     -16.499      -9.297
C(Weekday)[T.6]           -16.4623      1.829     -8.999      0.000     -20.060     -12.864
C(IsMonthStart)[T.True]     1.2012      5.781      0.208      0.836     -10.169      12.571
C(IsMonthEnd)[T.True]       4.7608      5.781      0.824      0.411      -6.609      16.131
scale(Ordinal)           -101.7884      4.209    -24.182      0.000    -110.068     -93.509
DayOfYear                   0.6769      0.085      7.926      0.000       0.509       0.845
==============================================================================
Omnibus:                      150.460   Durbin-Watson:                   0.577
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1586.415
Skew:                           1.422   Prob(JB):                         0.00
Kurtosis:                      12.809   Cond. No.                     1.05e+18
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 1.49e-29. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-13T09:33:34.000Z" title="2020-05-13T09:33:34.000Z">2020-05-13</time><span class="level-item">19 minutes read (About 2792 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/13/Linear-Regression-with-scale-categorical-regression-and-partial-regression/">Linear Regression with scale, categorical regression, and partial regression</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> font_manager, rc</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    <span class="keyword">if</span> platform.system() == <span class="string">'windows'</span>:</span><br><span class="line">        <span class="comment"># windows의 경우</span></span><br><span class="line">        font_name = font_manager.FomntProperties(fname=<span class="string">"c:/Windows/Font"</span>)</span><br><span class="line">        rc(<span class="string">'font'</span>, family = font_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># mac의 경우</span></span><br><span class="line">        rc(<span class="string">'font'</span>, family = <span class="string">'AppleGothic'</span>)</span><br><span class="line"><span class="keyword">except</span> :</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h5 id="스케일링"><a href="#스케일링" class="headerlink" title="스케일링"></a>스케일링</h5><img width="542" alt="스크린샷 2020-05-13 오후 6 20 21" src="https://user-images.githubusercontent.com/59719711/81794987-8af85000-9546-11ea-80c2-df81105e3ee1.png">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>이 summary report는 어제 보스턴 집값 데이터를 활용하여 선형회귀분석을 한 결과값이야. 아랫부분의 Cond. No. 라고 쓰여져 있는 부분인데 조건수(conditional number)는 가장 큰 고유치와 가장 작은 고유치의 비율을 뜻해.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">dfx = pd.DataFrame(boston.data, columns=boston.feature_names)</span><br><span class="line">dfy = pd.DataFrame(boston.target, columns=[<span class="string">"MEDV"</span>])</span><br><span class="line">df = pd.concat([dfx,dfy],axis=<span class="number">1</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 14 columns</p>
</div>



<pre><code>이것은 일부러 TAX변수를 크게 만들어 조건수를 증폭시켜본 데이터야</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">dfX = sm.add_constant(dfx)</span><br><span class="line">dfX2 = dfX.copy()</span><br><span class="line">dfX2[<span class="string">"TAX"</span>] *= <span class="number">1e13</span></span><br><span class="line">df2 = pd.concat([dfX2, dfy], axis=<span class="number">1</span>)</span><br><span class="line">model2 = sm.OLS.from_formula(<span class="string">"MEDV ~ "</span> + <span class="string">"+"</span>.join(boston.feature_names), data=df2)</span><br><span class="line">result2 = model2.fit()</span><br><span class="line">print(result2.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.333
Model:                            OLS   Adj. R-squared:                  0.329
Method:                 Least Squares   F-statistic:                     83.39
Date:                Wed, 13 May 2020   Prob (F-statistic):           8.62e-44
Time:                        16:03:11   Log-Likelihood:                -1737.9
No. Observations:                 506   AIC:                             3484.
Df Residuals:                     502   BIC:                             3501.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0038      0.000     -8.543      0.000      -0.005      -0.003
CRIM          -0.1567      0.046     -3.376      0.001      -0.248      -0.066
ZN             0.1273      0.016      7.752      0.000       0.095       0.160
INDUS         -0.1971      0.019    -10.433      0.000      -0.234      -0.160
CHAS           0.0034      0.000     12.430      0.000       0.003       0.004
NOX           -0.0023      0.000     -9.285      0.000      -0.003      -0.002
RM             0.0267      0.002     14.132      0.000       0.023       0.030
AGE            0.1410      0.017      8.443      0.000       0.108       0.174
DIS           -0.0286      0.004     -7.531      0.000      -0.036      -0.021
RAD            0.1094      0.018      6.163      0.000       0.075       0.144
TAX         1.077e-15   2.66e-16      4.051      0.000    5.55e-16     1.6e-15
PTRATIO       -0.1124      0.011    -10.390      0.000      -0.134      -0.091
B              0.0516      0.003     19.916      0.000       0.046       0.057
LSTAT         -0.6569      0.056    -11.790      0.000      -0.766      -0.547
==============================================================================
Omnibus:                       39.447   Durbin-Watson:                   0.863
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               46.611
Skew:                           0.704   Prob(JB):                     7.56e-11
Kurtosis:                       3.479   Cond. No.                     1.19e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.19e+17. This might indicate that there are
strong multicollinearity or other numerical problems.


조건수(Conditional No.)가 1000조 수준으로 증가한 것을 볼 수 있지? 오른쪽 제일 상단에 보이는 R-squared라는 값으로 표시되는 성능지표도 크게 감소한것을 볼 수 있어. R-squared 는 이 모델 성능에 대해 몇점인지를 알려주는 기능이라고 보면 되(0.333으로 나왔으니 100점 만점에 33.3점이라는 소리야)

statsmodels에서는  scale() 이라는 명령을 사용하여 스케일링을 할 수 있는데, 이 방식으로 스케일을 하면 스케일링에 사용된 평균과 표준편차를 저장하였다가 나중에 predict() 라는 명령을 사용할 때도 같은 스케일을 사용하기 때문에 편리한 것을 알 수 있어. 다만! 스케일링을 할 때에는 카테고리 변수, 즉 범주형 데이터는 스케일링을 하지 않는다는 것에 주의 해주면 되.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">feature_names = list(boston.feature_names)</span><br><span class="line">feature_names.remove(<span class="string">"CHAS"</span>) </span><br><span class="line">feature_names = [<span class="string">'scale(&#123;&#125;)'</span>.format(name) <span class="keyword">for</span> name <span class="keyword">in</span> feature_names] + [<span class="string">'CHAS'</span>]</span><br><span class="line">model3 = sm.OLS.from_formula(<span class="string">"MEDV ~ "</span> + <span class="string">"+"</span>.join(feature_names), data=df2)</span><br><span class="line">result3 = model3.fit()</span><br><span class="line">print(result3.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.741
Model:                            OLS   Adj. R-squared:                  0.734
Method:                 Least Squares   F-statistic:                     108.1
Date:                Wed, 13 May 2020   Prob (F-statistic):          6.72e-135
Time:                        16:11:05   Log-Likelihood:                -1498.8
No. Observations:                 506   AIC:                             3026.
Df Residuals:                     492   BIC:                             3085.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==================================================================================
                     coef    std err          t      P&gt;|t|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept         22.3470      0.219    101.943      0.000      21.916      22.778
scale(CRIM)       -0.9281      0.282     -3.287      0.001      -1.483      -0.373
scale(ZN)          1.0816      0.320      3.382      0.001       0.453       1.710
scale(INDUS)       0.1409      0.421      0.334      0.738      -0.687       0.969
scale(NOX)        -2.0567      0.442     -4.651      0.000      -2.926      -1.188
scale(RM)          2.6742      0.293      9.116      0.000       2.098       3.251
scale(AGE)         0.0195      0.371      0.052      0.958      -0.710       0.749
scale(DIS)        -3.1040      0.420     -7.398      0.000      -3.928      -2.280
scale(RAD)         2.6622      0.577      4.613      0.000       1.528       3.796
scale(TAX)        -2.0768      0.633     -3.280      0.001      -3.321      -0.833
scale(PTRATIO)    -2.0606      0.283     -7.283      0.000      -2.617      -1.505
scale(B)           0.8493      0.245      3.467      0.001       0.368       1.331
scale(LSTAT)      -3.7436      0.362    -10.347      0.000      -4.454      -3.033
CHAS               2.6867      0.862      3.118      0.002       0.994       4.380
==============================================================================
Omnibus:                      178.041   Durbin-Watson:                   1.078
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
Skew:                           1.521   Prob(JB):                    8.84e-171
Kurtosis:                       8.281   Cond. No.                         10.6
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


독립변수 데이터를 스케일링한것만으로 조건수의 수치가 확 내려간 것을 볼 수 있어. 어때? 쉽지?

조건수를 될 수 있으면 낮춰주는게 각 독립변수의 오차범위를 줄여줄 수 있다고 해. 그래서 큰 값의 데이터들은 스케일링을 통해 사이즈를 줄여주는 거지.</code></pre><h5 id="범주형-독립변수의-회귀분석"><a href="#범주형-독립변수의-회귀분석" class="headerlink" title="범주형 독립변수의 회귀분석"></a>범주형 독립변수의 회귀분석</h5><pre><code>이번에는 연속형 데이터가 아닌 범주형 데이터의 회귀분석을 하는 방법에 대해 알아보자! 범주형 데이터는 측정 결과가 몇 개의 범주 또는 향목의 형태로 나타나는 자료를 말하는데 그것을 숫자로 표현한 것이라고 할 수 있어. 예를 들면 남자는 1 여자는 0 이런식이지!

아무튼..

여기서 다룰 학습은 그러한 범주형 독립변수(데이터)의 회귀분석 모델링 시 에는 앞서 배운 더미변수화가 필수라는 거야. 풀랭크(full-rank) 방식과 축소랭크(reduced-rank) 방식이 있는데 풀랭크방식에서는 더미변수의 값을 원핫인코딩(one-hot-encoding) 방식으로 지정을 하는거야

예를 들어서..

남자는 1 이고 여자는 0 이면 
남자 : d1=1, d2=0
여자 : d1=0, d2=1 이런식으로 쓴다는 거지

축소랭크 방식에서는 특정한 하나의 범주값을 기준값(reference, baseline)으로 하고 기준값에 대응하는 더미변수의 가중치는 항상 1으로 놓아 계산 하는 방법이지

무슨 말인지 어렵지? 그럼 실 데이터로 예를 들어보도록 할게.

아래의 데이터는 1920년부터 1939년까지 영국 노팅험 지역의 기온을 나타낸 데이터야. 이 데이터에서 독립 변수는 월(monath)이며 범주값으로 처리를 할거야 그리고 value로 표기된 값이 종속변수인 해당 월의 평균 기온이라고 할 수 있지. 분석의 목적은 독립변수인 월 값을 이용하여 종속변수인 월 평균 기온을 예측하는 것이야.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> calendar <span class="keyword">import</span> isleap</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_partial_year</span><span class="params">(number)</span>:</span></span><br><span class="line">   <span class="comment">#연 단위 숫자에서 날짜를 계산하는 코드</span></span><br><span class="line">    year = int(number)</span><br><span class="line">    d = datetime.timedelta(days=(number - year) * (<span class="number">365</span> + isleap(year)))</span><br><span class="line">    day_one = datetime.datetime(year, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    date = d + day_one</span><br><span class="line">    <span class="keyword">return</span> date</span><br><span class="line"></span><br><span class="line">df_nottem = sm.datasets.get_rdataset(<span class="string">"nottem"</span>).data</span><br><span class="line"></span><br><span class="line">df_nottem[<span class="string">"date0"</span>] = df_nottem[[<span class="string">"time"</span>]].applymap(convert_partial_year)</span><br><span class="line">df_nottem[<span class="string">"date"</span>] = pd.DatetimeIndex(df_nottem[<span class="string">"date0"</span>]).round(<span class="string">'60min'</span>) + datetime.timedelta(seconds=<span class="number">3600</span>*<span class="number">24</span>)</span><br><span class="line">df_nottem[<span class="string">"month"</span>] = df_nottem[<span class="string">"date"</span>].dt.strftime(<span class="string">"%m"</span>).astype(<span class="string">'category'</span>)</span><br><span class="line"><span class="keyword">del</span> df_nottem[<span class="string">"date0"</span>], df_nottem[<span class="string">"date"</span>]</span><br><span class="line">df_nottem</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time</th>
      <th>value</th>
      <th>month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1920.000000</td>
      <td>40.6</td>
      <td>01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1920.083333</td>
      <td>40.8</td>
      <td>02</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1920.166667</td>
      <td>44.4</td>
      <td>03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1920.250000</td>
      <td>46.7</td>
      <td>04</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1920.333333</td>
      <td>54.1</td>
      <td>05</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>235</th>
      <td>1939.583333</td>
      <td>61.8</td>
      <td>08</td>
    </tr>
    <tr>
      <th>236</th>
      <td>1939.666667</td>
      <td>58.2</td>
      <td>09</td>
    </tr>
    <tr>
      <th>237</th>
      <td>1939.750000</td>
      <td>46.7</td>
      <td>10</td>
    </tr>
    <tr>
      <th>238</th>
      <td>1939.833333</td>
      <td>46.6</td>
      <td>11</td>
    </tr>
    <tr>
      <th>239</th>
      <td>1939.916667</td>
      <td>37.8</td>
      <td>12</td>
    </tr>
  </tbody>
</table>
<p>240 rows × 3 columns</p>
</div>



<pre><code>박스플롯을 통해 해당 월에 종속변수데이터가 어디에 주로 모여있는지 확인을 해보는거야</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_nottem.boxplot(<span class="string">"value"</span>, <span class="string">"month"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="382" alt="output_14_0" src="https://user-images.githubusercontent.com/59719711/81795405-0a861f00-9547-11ea-8d19-a3c30dfdf966.png">


<pre><code>%% 범주형 독립변수의 경우 앞서 시행한 회귀분석에서 추가해준 상수항(add_constant)을 추가하지 않아.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 풀랭크 방식</span></span><br><span class="line">model = sm.OLS.from_formula(<span class="string">"value ~ C(month) + 0"</span>, df_nottem)</span><br><span class="line">result = model.fit()</span><br><span class="line">print(result.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># C()로 카테고리로 처리</span></span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  value   R-squared:                       0.930
Model:                            OLS   Adj. R-squared:                  0.927
Method:                 Least Squares   F-statistic:                     277.3
Date:                Wed, 13 May 2020   Prob (F-statistic):          2.96e-125
Time:                        16:28:22   Log-Likelihood:                -535.82
No. Observations:                 240   AIC:                             1096.
Df Residuals:                     228   BIC:                             1137.
Df Model:                          11                                         
Covariance Type:            nonrobust                                         
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
C(month)[01]    39.6950      0.518     76.691      0.000      38.675      40.715
C(month)[02]    39.1900      0.518     75.716      0.000      38.170      40.210
C(month)[03]    42.1950      0.518     81.521      0.000      41.175      43.215
C(month)[04]    46.2900      0.518     89.433      0.000      45.270      47.310
C(month)[05]    52.5600      0.518    101.547      0.000      51.540      53.580
C(month)[06]    58.0400      0.518    112.134      0.000      57.020      59.060
C(month)[07]    61.9000      0.518    119.592      0.000      60.880      62.920
C(month)[08]    60.5200      0.518    116.926      0.000      59.500      61.540
C(month)[09]    56.4800      0.518    109.120      0.000      55.460      57.500
C(month)[10]    49.4950      0.518     95.625      0.000      48.475      50.515
C(month)[11]    42.5800      0.518     82.265      0.000      41.560      43.600
C(month)[12]    39.5300      0.518     76.373      0.000      38.510      40.550
==============================================================================
Omnibus:                        5.430   Durbin-Watson:                   1.529
Prob(Omnibus):                  0.066   Jarque-Bera (JB):                5.299
Skew:                          -0.281   Prob(JB):                       0.0707
Kurtosis:                       3.463   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = sm.OLS.from_formula(<span class="string">"value ~ C(month)"</span>, df_nottem)</span><br><span class="line">result = model.fit()</span><br><span class="line">print(result.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  value   R-squared:                       0.930
Model:                            OLS   Adj. R-squared:                  0.927
Method:                 Least Squares   F-statistic:                     277.3
Date:                Wed, 13 May 2020   Prob (F-statistic):          2.96e-125
Time:                        16:32:20   Log-Likelihood:                -535.82
No. Observations:                 240   AIC:                             1096.
Df Residuals:                     228   BIC:                             1137.
Df Model:                          11                                         
Covariance Type:            nonrobust                                         
==================================================================================
                     coef    std err          t      P&gt;|t|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept         39.6950      0.518     76.691      0.000      38.675      40.715
C(month)[T.02]    -0.5050      0.732     -0.690      0.491      -1.947       0.937
C(month)[T.03]     2.5000      0.732      3.415      0.001       1.058       3.942
C(month)[T.04]     6.5950      0.732      9.010      0.000       5.153       8.037
C(month)[T.05]    12.8650      0.732     17.575      0.000      11.423      14.307
C(month)[T.06]    18.3450      0.732     25.062      0.000      16.903      19.787
C(month)[T.07]    22.2050      0.732     30.335      0.000      20.763      23.647
C(month)[T.08]    20.8250      0.732     28.450      0.000      19.383      22.267
C(month)[T.09]    16.7850      0.732     22.931      0.000      15.343      18.227
C(month)[T.10]     9.8000      0.732     13.388      0.000       8.358      11.242
C(month)[T.11]     2.8850      0.732      3.941      0.000       1.443       4.327
C(month)[T.12]    -0.1650      0.732     -0.225      0.822      -1.607       1.277
==============================================================================
Omnibus:                        5.430   Durbin-Watson:                   1.529
Prob(Omnibus):                  0.066   Jarque-Bera (JB):                5.299
Skew:                          -0.281   Prob(JB):                       0.0707
Kurtosis:                       3.463   Cond. No.                         12.9
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


축소랭크 방식 ( +0 제거) - &apos;기준이 되는 데이터 값에서  다른 범주형 데이터 값이 얼마나 다르냐&apos; 의 의미로 보면 됨, 즉 1월을 기준으로 2월에는 차이가 얼마나 있느냐 를 나타내는거야. 풀랭크 방식과 축소랭크 방식의 차이를 조금은 알 수 있게된거같아.

이 이상은 내가 이해를 못했기 때문에 넘어가도록 할게.</code></pre><h5 id="부분회귀"><a href="#부분회귀" class="headerlink" title="부분회귀"></a>부분회귀</h5><pre><code>만약 회귀분석을 한 후에 새로운 독립변수를 추가하여 다시 회귀분석을 한다면 그 전에 회귀분석으로 구했던 가중치의 값은 변할까 변하지 않을까? 예를 들어  𝑥1 이라는 독립변수만으로 회귀분석한 결과가 다음과 같다고 하자.</code></pre><img width="202" alt="스크린샷 2020-05-13 오후 4 49 48" src="https://user-images.githubusercontent.com/59719711/81795180-cdba2800-9546-11ea-9243-aeeca5cda269.png">

<pre><code>이 때 새로운 독립변수  𝑥2 를 추가하여 회귀분석을 하게 되면 이 때 나오는  𝑥1 에 대한 가중치  𝑤′1 가 원래의  𝑤1 과 같을까 다를까?</code></pre><img width="307" alt="스크린샷 2020-05-13 오후 4 49 52" src="https://user-images.githubusercontent.com/59719711/81795240-de6a9e00-9546-11ea-9ef1-5bd349d3ec8a.png">

<pre><code>답부터 말하자면

일반적으로  𝑤′1 의 값은 원래의  𝑤1 의 값과 다르다.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line">dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)</span><br><span class="line">dfX = sm.add_constant(dfX0)</span><br><span class="line">dfy = pd.DataFrame(boston.target, columns=[<span class="string">"MEDV"</span>])</span><br><span class="line">df = pd.concat([dfX, dfy], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model_boston = sm.OLS(dfy, dfX)</span><br><span class="line">result_boston = model_boston.fit()</span><br><span class="line">print(result_boston.summary())</span><br></pre></td></tr></table></figure>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.741
Model:                            OLS   Adj. R-squared:                  0.734
Method:                 Least Squares   F-statistic:                     108.1
Date:                Wed, 13 May 2020   Prob (F-statistic):          6.72e-135
Time:                        18:01:08   Log-Likelihood:                -1498.8
No. Observations:                 506   AIC:                             3026.
Df Residuals:                     492   BIC:                             3085.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         36.4595      5.103      7.144      0.000      26.432      46.487
CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
ZN             0.0464      0.014      3.382      0.001       0.019       0.073
INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
RM             3.8099      0.418      9.116      0.000       2.989       4.631
AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
RAD            0.3060      0.066      4.613      0.000       0.176       0.436
TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
B              0.0093      0.003      3.467      0.001       0.004       0.015
LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
==============================================================================
Omnibus:                      178.041   Durbin-Watson:                   1.078
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
Skew:                           1.521   Prob(JB):                    8.84e-171
Kurtosis:                       8.281   Cond. No.                     1.51e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.51e+04. This might indicate that there are
strong multicollinearity or other numerical problems.


이렇게 보면 AGE는 집값을 결정하는데 음의 상관관계를 가진다고 볼 수 있다고 할 수 있어</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.regplot(x=<span class="string">"AGE"</span>, y=<span class="string">"MEDV"</span>, data=df)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="389" alt="output_25_0" src="https://user-images.githubusercontent.com/59719711/81795459-1c67c200-9547-11ea-9580-72d65bfab466.png">


<pre><code>plot_partregress(endog, exog_i, exog_others, data=None, obs_labels=True, ret_coords=False)

endog: 종속변수 문자열
exog_i: 분석 대상이 되는 독립변수 문자열
exog_others: 나머지 독립변수 문자열의 리스트
data: 모든 데이터가 있는 데이터프레임
obs_labels: 데이터 라벨링 여부
ret_coords: 잔차 데이터 반환 여부

하지만! 다른 독립변수에 영향을 받은 AGE가 집값에 영향을 미쳤는지에 대한 부분을 확인해보면 그래프는 다음과 같아.
AGE 데이터에 대한 부분회귀인셈이지.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">others = list(set(df.columns).difference(set([<span class="string">"MEDV"</span>, <span class="string">"AGE"</span>])))</span><br><span class="line">p, resids = sm.graphics.plot_partregress(</span><br><span class="line">    <span class="string">"MEDV"</span>, <span class="string">"AGE"</span>, others, data=df, obs_labels=<span class="literal">False</span>, ret_coords=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 크게 상관이 없다는 거로 나오게 되</span></span><br></pre></td></tr></table></figure>

<img width="395" alt="output_29_0" src="https://user-images.githubusercontent.com/59719711/81795521-2be70b00-9547-11ea-9d31-c7d0f66eedab.png">


<pre><code>sm.graphics.plot_partregress_grid 명령을 쓰면 전체 데이터에 대해 한번에 부분회귀 플롯을 그릴 수 있어.

plot_partregress_grid(result, fig)

result: 회귀분석 결과 객체
fig: plt.figure 객체</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">20</span>))</span><br><span class="line">sm.graphics.plot_partregress_grid(result_boston, fig=fig)</span><br><span class="line">fig.suptitle(<span class="string">""</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img width="563" alt="output_32_0" src="https://user-images.githubusercontent.com/59719711/81795567-399c9080-9547-11ea-8128-fc56699f56a0.png">


<pre><code>CCPR 플롯
CCPR(Component-Component plus Residual) 플롯도 부분회귀 플롯과 마찬가지로 특정한 하나의 변수의 영향을 살펴보기 위한 것이야

부분회귀분석과 비슷하지만 다른점이 하나 있는데 그것은 위에서 언급한 다른 변수에 영향을 받은 AGE가 아니라 AGE 데이터 그 자체와 집값의 상관관계를 보기위한 방법이라고 보면 되</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sm.graphics.plot_ccpr(result_boston, <span class="string">"AGE"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img width="395" alt="output_34_0" src="https://user-images.githubusercontent.com/59719711/81795631-49b47000-9547-11ea-9b2d-70d08594dd10.png">


<pre><code>CCPR 플롯에서는 부분회귀 플롯과 달리 독립변수가 원래의 값 그대로 나타난다는 점을 다시 한번 상기 시켜줄게</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">15</span>))</span><br><span class="line">sm.graphics.plot_ccpr_grid(result_boston, fig=fig)</span><br><span class="line">fig.suptitle(<span class="string">""</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img width="564" alt="output_36_0" src="https://user-images.githubusercontent.com/59719711/81795667-533dd800-9547-11ea-8b7d-5ca07cf8a5ed.png">


<pre><code>plot_regress_exog(result, exog_idx)

result: 회귀분석 결과 객체
exog_idx: 분석 대상이 되는 독립변수 문자열</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = sm.graphics.plot_regress_exog(result_boston, <span class="string">"AGE"</span>)</span><br><span class="line">plt.tight_layout(pad=<span class="number">4</span>, h_pad=<span class="number">0.5</span>, w_pad=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<img width="356" alt="output_38_0" src="https://user-images.githubusercontent.com/59719711/81795721-62248a80-9547-11ea-9e1f-12d645504968.png">
</div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://user-images.githubusercontent.com/59719711/85970198-ab9c3c80-ba04-11ea-8990-4bdec6914e3c.jpeg" alt="wglee87"></figure><p class="title is-size-4 is-block line-height-inherit">wglee87</p><p class="is-size-6 is-block">Data has a better idea</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hwasung, KR</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">31</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">3</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wglee87" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wglee87"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://Instagram.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-12T12:06:35.000Z">2020-08-12</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/12/%E1%84%80%E1%85%AE%E1%84%80%E1%85%B3%E1%86%AF-%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A9%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B5%E1%86%BC-API-%E1%84%8F%E1%85%B5-%E1%84%87%E1%85%A1%E1%86%AF%E1%84%80%E1%85%B3%E1%86%B8-%E1%84%87%E1%85%A1%E1%86%AE%E1%84%82%E1%85%B3%E1%86%AB-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-How-to-be-issued-the-Geocoding-API-key-from-Google/">구글 지오코딩 API 키 발급 받는 방법 (How to be issued the Geocoding API key from Google)</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-07-24T05:54:01.000Z">2020-07-24</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/24/MySQL-Ubuntu%EC%97%90%EC%84%9C-MySQL-%EC%99%84%EC%A0%84-%EC%82%AD%EC%A0%9C%ED%95%98%EA%B8%B0/">[MySQL] Ubuntu에서 MySQL 완전 삭제하기</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-07-23T10:24:00.000Z">2020-07-23</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/23/MySQL-workbench%E1%84%8B%E1%85%A6%E1%84%89%E1%85%A5-ERD%E1%84%90%E1%85%AE%E1%86%AF-%E1%84%89%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%84%80%E1%85%B5-Database-Modeling/">[mysql] workbench에서 ERD툴 사용하기 (Database Modeling)</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-07-22T13:58:24.000Z">2020-07-22</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/22/MySQL-Storage-Engine-InnoDB-vs-MyISAM/">[MySQL] Storage Engine (InnoDB vs MyISAM)</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-07-19T09:25:05.000Z">2020-07-19</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/19/Python-SQLAlchemy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/">[Python] SQLAlchemy 사용하기</a></p><p class="is-uppercase"></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/05/"><span class="level-start"><span class="level-item">May 2020</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">April 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/fastcampus/"><span class="tag">fastcampus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/" alt="Geony&#039;s Tech Blog" height="28"></a><p class="size-small"><span>&copy; 2020 WGLee87</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wglee87.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>